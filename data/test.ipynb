{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-22T02:59:01.017096Z",
     "start_time": "2023-08-22T02:58:56.883237Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# img = cv2.imread('0.jpg', cv2.IMREAD_GRAYSCALE).astype(np.float32) #读取为灰度图片\n",
    "img = cv2.imread('1.png', cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "# img = cv2.imread('2.png', cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "# img = cv2.imread('3.png', cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "# img = cv2.imread('4.png', cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "# img = cv2.imread('5.png', cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "# img = cv2.imread('6.png', cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "print(img.shape)\n",
    "print(img[:10, :10])\n",
    "plt.imshow(img, 'gray')\n",
    "plt.show()\n",
    "# cv2.imshow('test', img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_dct = cv2.dct(img)\n",
    "# img_dct_f = np.log(np.abs(img_dct))\n",
    "img_dct_f = np.abs(img_dct)\n",
    "print(img_dct.shape)\n",
    "print(img_dct_f[:10, :10])\n",
    "plt.imshow(img_dct_f)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lf = (img_dct_f >= 0) * (img_dct_f < 10)\n",
    "lf = img_dct_f > 20\n",
    "mf = (img_dct_f >= 10) * (img_dct_f < 20)\n",
    "hf = img_dct_f < 10\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('low freq')\n",
    "plt.imshow(lf)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('mid freq')\n",
    "plt.imshow(mf)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('high freq')\n",
    "plt.imshow(hf)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_dct_low = cv2.idct(img_dct * lf)\n",
    "img_dct_mid = cv2.idct(img_dct * mf)\n",
    "img_dct_high = cv2.idct(img_dct * hf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(50,50))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.title('original')\n",
    "plt.imshow(img, 'gray')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title('low')\n",
    "plt.imshow(img_dct_low, 'gray')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title('mid')\n",
    "plt.imshow(img_dct_mid, 'gray')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title('high')\n",
    "plt.imshow(img_dct_high, 'gray')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(np.min(img_dct_low), np.max(img_dct_low))\n",
    "print(np.min(img_dct_mid), np.max(img_dct_mid))\n",
    "print(np.min(img_dct_high), np.max(img_dct_high))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = cv2.idct(img_dct)\n",
    "plt.imshow(a, 'gray')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(img_dct_low.shape)\n",
    "print(img_dct_low[:,:,None].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def filter_bank_preprocess(img_path: str, k: int=10):\n",
    "    img = cv2.imread(img_path, flags=cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "    img_dct = cv2.dct(img)\n",
    "    img_dct_f = np.abs(img_dct)\n",
    "\n",
    "    low_freq = img_dct_f > 2*k\n",
    "    mid_freq = (img_dct_f >= k) * (img_dct_f < 2*k)\n",
    "    high_freq = img_dct_f < k\n",
    "\n",
    "    img_dct_low = cv2.idct(img_dct * low_freq)[:, :, None]\n",
    "    img_dct_mid = cv2.idct(img_dct * mid_freq)[:, :, None]\n",
    "    img_dct_high = cv2.idct(img_dct * high_freq)[:, :, None]\n",
    "    return np.concatenate((img_dct_low, img_dct_mid, img_dct_high), axis=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res = filter_bank_preprocess('1.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(res.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = res[:,:,0]\n",
    "plt.imshow(t, 'gray')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img = cv2.imread('0.jpg').astype(np.float32) #读取为灰度图片\n",
    "print(img.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img = cv2.imread('0.jpg')\n",
    "print(isinstance(torch.from_numpy(img), torch.ByteTensor))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.transforms import DCT\n",
    "from data.dataset import RecaptureDataset\n",
    "from torchvision.transforms import transforms as T\n",
    "from config import opt\n",
    "\n",
    "img = Image.open('0.jpg')\n",
    "transform = T.Compose([\n",
    "    T.Resize(opt.img_size),\n",
    "    DCT()\n",
    "])\n",
    "\n",
    "img = transform(img)\n",
    "print(type(img), img.shape)\n",
    "print(torch.max(img))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v3_small\n",
    "\n",
    "model = mobilenet_v3_small(weights=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model.features[-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tristan/anaconda3/envs/torch2/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "model = resnet50(weights=False)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T06:44:38.354846Z",
     "start_time": "2023-08-21T06:44:37.798393Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 56, 56])\n",
      "torch.Size([1, 1024, 14, 14])\n",
      "torch.Size([1, 2048, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,3,224,224)\n",
    "x = model.conv1(x)\n",
    "x = model.bn1(x)\n",
    "x = model.relu(x)\n",
    "x = model.maxpool(x)\n",
    "l1 = model.layer1(x)\n",
    "print(l1.shape)\n",
    "x = model.layer2(l1)\n",
    "l3 = model.layer3(x)\n",
    "print(l3.shape)\n",
    "l4 = model.layer4(l3)\n",
    "print(l4.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T03:54:16.021610Z",
     "start_time": "2023-08-21T03:54:15.793758Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models.models import ResNet50Branch\n",
    "model = ResNet50Branch()\n",
    "print(model(torch.randn(1,3,224,224))[2].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12, 5])\n",
      "torch.Size([2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3,4,5)\n",
    "print(a.flatten(1,2).shape)\n",
    "print(a.reshape(2,3,4,5).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T03:38:15.597491Z",
     "start_time": "2023-08-21T03:38:15.587655Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "a = torch.randn(1,2,3)\n",
    "b = torch.randn(1,2,3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T03:07:51.690362Z",
     "start_time": "2023-08-21T03:07:51.643154Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "c = a @ b.transpose(-2, -1)\n",
    "print(c.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T03:08:07.058023Z",
     "start_time": "2023-08-21T03:08:07.011979Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,3,224,224)\n",
    "a = nn.functional.interpolate(input=a, size=(7,7), mode='bilinear')\n",
    "print(a.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T03:49:17.985305Z",
     "start_time": "2023-08-21T03:49:17.976763Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace=True)\n",
      "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (denseblock1): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition(\n",
      "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition(\n",
      "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition3): _Transition(\n",
      "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock4): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import densenet121\n",
    "model = densenet121(weights=None)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T05:59:54.571033Z",
     "start_time": "2023-08-21T05:59:54.362133Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0306,  0.2107]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from models.models import TBNet\n",
    "model = TBNet()\n",
    "a = torch.randn(1,3,224,224)\n",
    "a = model(a)\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T06:39:17.658172Z",
     "start_time": "2023-08-21T06:39:15.132045Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 3.5939e-01,  1.9074e-01, -2.6771e-01,  3.0885e-02,  5.9954e-01,\n          7.0140e-01, -2.4401e-01, -1.7202e-01,  4.4703e-02,  1.0456e-01,\n         -5.9610e-01, -3.6353e-02, -8.4576e-01,  1.7502e-01, -1.4140e-01,\n          5.4820e-01, -2.8961e-01, -2.9041e-01,  9.7538e-01, -7.6583e-01,\n         -3.9553e-01,  2.3112e-01, -1.5042e+00,  9.5894e-03,  1.0959e-01,\n          7.0200e-01, -1.5800e-02, -5.7178e-01,  5.0056e-01,  6.3486e-01,\n         -6.9760e-01, -3.5327e-01,  2.9515e-02,  7.6578e-01,  3.5963e-01,\n         -8.7728e-02,  5.6729e-01,  3.0701e-01, -1.1256e+00,  4.1008e-01,\n          4.6911e-01, -1.4084e+00, -1.9215e-01,  7.8370e-01,  1.8526e-01,\n          1.1930e-02, -1.2467e-01, -4.1979e-01,  1.7588e-01,  7.7406e-01,\n         -1.0249e+00,  3.4002e-01,  6.1109e-01, -6.8165e-02,  1.4725e-01,\n         -1.1190e+00,  9.5786e-01,  1.6915e-02,  2.5695e-01,  4.1932e-01,\n         -2.8678e-01, -5.6024e-01,  2.9855e-01, -8.4647e-01, -8.7467e-02,\n         -8.7404e-01, -1.5808e-01, -1.2848e-01, -4.9689e-01, -3.0223e-01,\n         -4.7726e-01,  1.6124e-02,  6.0201e-02, -2.9933e-01, -9.5461e-01,\n         -7.3522e-01, -1.1496e+00,  3.7245e-01,  4.9767e-01,  4.5468e-01,\n         -4.6310e-01,  3.8140e-02,  2.8659e-01, -4.7718e-01,  7.6632e-01,\n          1.7452e-01,  8.5091e-01,  1.1836e+00,  3.2874e-01,  2.4508e-01,\n         -3.0702e-01, -3.4275e-01,  1.3394e+00, -2.0909e-01, -3.3004e-01,\n         -3.6576e-01, -6.6061e-01, -4.0338e-02,  5.0762e-01,  1.1228e-01,\n          1.6957e-01, -2.0165e-01,  5.5565e-01, -8.2195e-01, -4.9646e-02,\n         -1.9334e-01, -1.2335e-01, -4.3935e-01,  4.9785e-01,  9.0524e-01,\n         -2.1945e-01, -5.1913e-01, -1.0077e+00,  4.9929e-01,  7.2113e-01,\n          3.3140e-01,  8.9493e-02,  3.8405e-01, -1.8448e+00,  3.6043e-01,\n          9.2143e-01, -7.0842e-01,  1.5272e-01, -3.5166e-01,  4.4163e-01,\n         -1.1576e-01, -9.6062e-01,  8.9145e-01, -3.6059e-01,  1.3376e+00,\n          5.4195e-02, -9.7626e-02,  8.4493e-01, -1.0861e-01, -3.2184e-01,\n         -7.1234e-02, -5.8434e-01,  7.6470e-01,  8.1323e-01, -8.1327e-01,\n          9.3436e-02, -6.0851e-01,  2.3070e-01, -3.5725e-01, -5.1528e-02,\n          1.4173e-01, -1.3833e-01, -3.1497e-01,  8.6696e-01, -4.1947e-01,\n          3.0663e-01, -1.3010e+00, -9.7507e-02,  6.8871e-01, -1.9853e-01,\n         -2.4080e-01,  4.3585e-01, -1.9273e-01,  3.6652e-01,  8.5685e-01,\n         -1.0950e-01, -9.8607e-01,  1.5672e+00,  1.3896e-01, -9.3370e-01,\n         -1.0482e+00, -4.6723e-01, -2.0339e-01,  3.7788e-01, -1.6270e+00,\n          1.0160e+00, -1.9334e-01, -4.5058e-01, -1.8933e-01,  2.2493e-01,\n          5.4013e-01, -2.6056e-01,  9.4603e-02, -1.8826e-01, -1.4835e-02,\n          5.2196e-01, -4.1193e-01,  2.1596e-01,  1.0482e+00,  6.1357e-01,\n          5.7034e-01,  6.6414e-02,  9.3747e-03,  1.1649e-01,  1.0759e-01,\n          4.8416e-02,  1.1313e-01,  7.2702e-01, -2.2376e-01, -1.9111e-01,\n         -3.3920e-01,  6.7733e-01,  2.0174e-01, -1.0499e+00, -8.3261e-01,\n         -1.1466e+00,  7.8420e-01, -2.1313e-01, -3.7985e-01,  5.3582e-01,\n          1.5599e-01, -3.4239e-01,  8.4401e-01, -2.1258e-01,  2.9062e-01,\n          6.6539e-01, -1.2215e-01, -4.2059e-01, -6.1746e-01,  1.2370e-01,\n         -1.9862e-02,  1.3743e-01, -1.7073e+00,  5.5345e-01,  1.0273e+00,\n         -2.0155e-01, -6.1496e-02,  5.4080e-02, -3.9670e-01, -3.2675e-01,\n          1.0790e+00, -7.7397e-01,  4.8586e-01, -4.0445e-01,  1.2942e+00,\n          7.4886e-01, -5.8429e-01,  5.3912e-02,  4.9031e-01, -2.4822e-01,\n          1.5924e-01, -4.9130e-01, -3.7385e-02,  6.6503e-01, -7.2373e-01,\n          5.8827e-01, -1.6390e-01, -4.5406e-01, -8.9496e-01, -5.0499e-01,\n          3.0133e-01,  5.9568e-01,  2.3148e-01,  2.2434e-01, -4.9201e-01,\n          3.4563e-01,  1.6153e-01,  3.2602e-01, -9.6121e-02,  3.2709e-01,\n          3.4531e-01, -7.0206e-01,  5.1848e-01,  6.3778e-01, -7.3387e-01,\n          1.2268e+00, -5.8308e-01, -5.8858e-01, -3.7113e-01, -1.1965e+00,\n         -1.1049e-01,  1.0496e-01,  7.2923e-01,  1.6762e+00,  1.7328e-01,\n         -8.5056e-01,  6.7207e-01, -1.6988e+00, -2.8500e-01,  9.0035e-01,\n         -3.2701e-01,  5.9243e-02, -3.0779e-01,  2.5110e-01,  7.8872e-01,\n          3.5346e-01,  7.5154e-01, -2.3890e-01, -2.5508e-01,  9.0110e-02,\n         -2.6079e-01,  1.2598e+00, -6.9169e-03,  3.8764e-01,  1.1920e-01,\n          3.0664e-01,  5.1501e-01, -6.4210e-02,  4.0861e-01,  1.7878e-01,\n         -7.1928e-01, -2.2025e-01, -6.2583e-02, -2.0567e-01, -3.0704e-01,\n          1.1813e-01, -1.2030e+00, -4.1168e-03,  4.5378e-02,  6.1864e-02,\n         -3.7400e-01, -6.1627e-01, -8.0202e-01,  1.7800e-01, -9.2671e-01,\n         -6.8673e-01,  6.7659e-01, -5.3424e-01, -6.0687e-01, -2.8722e-01,\n          4.8323e-01,  2.5689e-01, -3.9347e-01, -1.5364e-01, -9.6819e-01,\n          3.9636e-01,  3.4634e-01,  2.2751e-01, -5.1119e-01, -5.7336e-01,\n         -4.9270e-01, -4.2723e-02,  8.8130e-01,  4.4545e-01,  4.4873e-01,\n         -2.8957e-01, -9.0316e-02, -5.5573e-01,  3.2010e-01, -6.2041e-01,\n         -4.1830e-01, -1.2645e-01,  9.5508e-01,  2.0058e-01, -1.0752e-01,\n          6.1074e-01,  5.9610e-01, -2.0810e-01,  2.0035e-01, -3.7031e-01,\n          9.0876e-01, -2.8176e-01,  6.0957e-01,  5.7223e-01, -4.6037e-01,\n         -1.2738e+00,  6.9324e-02, -2.2068e-01,  5.5754e-01, -7.1691e-02,\n          9.9160e-01, -1.4292e-01,  3.8664e-01,  1.3834e-01,  1.3609e-01,\n          9.0439e-02,  1.2058e+00,  8.0802e-01, -1.9620e-01, -4.2707e-01,\n         -4.5065e-01,  4.3929e-01,  1.4756e-01,  3.1928e-01, -4.5257e-02,\n          7.9482e-01, -8.2181e-01,  1.2683e-01,  7.8055e-02, -7.5785e-02,\n         -8.5219e-01,  1.2424e+00, -3.7569e-01, -6.6971e-02, -1.2095e+00,\n          3.6544e-01, -6.5786e-01, -9.1166e-01, -3.7401e-01, -1.2156e+00,\n         -3.4540e-01, -9.3870e-01, -6.2226e-01,  2.4204e-01,  1.2645e+00,\n          2.7823e-01,  2.7651e-02, -7.7411e-01,  2.0804e-01, -4.9926e-01,\n         -3.6580e-01, -4.7016e-01,  1.0046e-03,  9.2105e-01, -3.6339e-01,\n          2.1690e-01, -4.5717e-01, -1.7010e-01,  3.6716e-01, -1.1163e+00,\n         -1.0480e-01,  2.2454e-01,  9.5114e-01, -6.1223e-02, -1.0983e+00,\n         -1.6320e-01, -2.3070e-01,  6.2290e-01, -3.1016e-01,  3.0638e-01,\n          5.9866e-01, -7.1569e-01, -3.3831e-01,  1.2068e-01, -9.5950e-01,\n          1.4703e+00, -4.3329e-01, -9.6211e-02,  4.8329e-01,  7.7323e-01,\n          9.0117e-02, -7.5723e-02, -9.8500e-02, -1.3541e-01,  9.7506e-01,\n          4.6798e-01, -3.2514e-01,  8.9367e-01,  2.6482e-01, -6.1636e-01,\n         -1.5255e-01,  7.6291e-01, -8.0428e-01,  1.0458e+00,  4.6602e-01,\n         -5.9039e-01, -8.7422e-01,  1.8979e-02, -1.1288e-02,  1.9157e-02,\n         -2.9725e-01,  6.3540e-01, -5.1935e-01, -4.6377e-01, -3.8705e-01,\n          1.1649e-01,  1.0152e+00,  3.4484e-01,  7.9345e-02,  7.2441e-01,\n         -5.5932e-01,  2.8154e-02,  1.0989e+00,  4.9868e-01, -8.5524e-02,\n         -2.1032e-01,  7.2102e-01,  1.3381e-01,  3.9357e-01,  3.8261e-02,\n         -9.3662e-03,  3.0712e-01,  9.4957e-01,  1.2332e-01,  1.1615e+00,\n          7.9801e-01, -4.6938e-02, -1.9296e-01, -1.0302e+00,  5.1765e-01,\n          1.0010e+00, -2.8737e-01, -7.3490e-01, -2.5145e-01, -1.2212e+00,\n          2.2173e-01,  8.0810e-01,  2.0277e-02, -6.9003e-02,  3.8459e-01,\n          1.7383e+00,  7.5948e-01, -7.5028e-01, -1.4824e-02,  4.2252e-01,\n         -1.8215e-01,  3.4507e-01,  5.8704e-02,  5.5180e-01,  6.8935e-02,\n          3.9556e-01, -3.7777e-02,  5.5953e-01,  3.8301e-01, -6.3164e-01,\n         -2.8863e-01, -9.2467e-02, -2.7123e-01,  1.8700e-01, -2.8544e-01,\n         -2.4945e-01, -4.8080e-01, -6.2173e-01, -1.1131e+00,  2.1847e-01,\n         -1.4433e+00, -1.4207e+00, -1.2074e+00, -6.1384e-02,  1.7199e-01,\n         -1.2350e-01,  5.0792e-02,  3.0186e-02,  7.7794e-01, -7.0715e-01,\n          4.8544e-01, -3.5280e-02,  5.6500e-01,  2.4721e-01,  1.0772e+00,\n         -2.7227e-01, -7.3185e-02, -1.4142e-01,  7.9845e-01,  6.8468e-01,\n         -5.1324e-01, -2.7575e-01, -1.5465e+00,  7.0748e-01,  1.9165e-01,\n         -6.6160e-02, -2.8313e-01, -8.2440e-01,  5.6080e-02,  9.1256e-01,\n         -8.2768e-01,  1.1538e+00, -1.7454e-01, -1.3118e-01,  5.9438e-02,\n         -2.0769e-01, -2.5989e-01, -1.7583e-01, -1.6555e-01, -3.1383e-01,\n         -9.4872e-01,  2.6215e-01,  6.2619e-01, -6.6498e-01, -4.0307e-02,\n          2.4992e-02,  4.9736e-01,  1.1562e+00, -1.0693e-01, -1.1047e-02,\n          6.7944e-01,  6.2027e-02,  5.0421e-01,  2.3653e-01, -1.0391e+00,\n         -2.4919e-02, -2.5606e-01, -2.1033e-01,  6.2936e-01, -2.4040e-01,\n         -9.8765e-01,  4.6433e-01,  5.2194e-01,  1.1964e+00,  7.4641e-01,\n          3.7664e-01, -8.8801e-01, -1.5397e-01, -1.1169e+00, -2.3492e-01,\n         -3.2065e-01, -7.8526e-01,  7.9725e-01,  2.6526e-02, -4.2169e-01,\n          7.8298e-01,  7.7031e-02, -1.4875e-01,  5.0859e-01, -3.6010e-01,\n          1.4801e-01,  5.6800e-01, -2.0473e-01, -3.0891e-01, -1.1555e-01,\n          3.8406e-01,  8.8541e-01, -4.8920e-01, -2.2593e-01,  2.2452e-01,\n         -1.5784e-01, -1.0240e-01,  1.4090e-01,  2.1392e-01,  2.3168e-01,\n          3.2666e-02, -8.3709e-02,  2.8547e-01,  6.9245e-03,  4.4607e-01,\n          2.5879e-01, -7.3663e-01, -1.5796e-01, -4.3940e-01,  1.1717e+00,\n         -1.6573e+00, -5.6561e-01, -5.1216e-01, -1.3474e-01, -3.5810e-01,\n         -9.0807e-02, -1.5326e-01, -4.2064e-01,  3.6282e-01, -1.1576e+00,\n          1.1844e+00,  4.0921e-01,  9.2977e-02, -9.8515e-01, -1.9461e-01,\n         -5.8427e-01,  6.0172e-01,  1.1793e+00,  1.8882e-01,  2.6702e-01,\n          2.5154e-01, -5.1716e-01, -1.0235e+00,  4.3506e-01, -2.8167e-01,\n          1.0018e-01,  7.2171e-01, -2.9084e-02, -1.4197e-01,  5.5226e-01,\n         -5.5687e-01,  8.5155e-01, -2.7681e-01, -4.1686e-01, -3.1531e-01,\n         -3.9924e-01,  1.1607e+00, -1.5754e-01,  8.1041e-01,  2.0791e-01,\n          9.4569e-02,  7.7134e-01,  1.3024e-01, -5.3040e-01, -1.4149e-01,\n         -3.6657e-01,  1.2084e-01, -9.7749e-01,  5.4670e-01,  5.0176e-01,\n          1.7543e-01,  7.3026e-01,  3.6452e-01, -1.1546e-01,  8.3710e-01,\n          1.0410e+00,  2.9193e-01,  6.6529e-01,  1.0079e+00, -7.9474e-01,\n         -7.3059e-01, -4.3373e-01,  2.9593e-01, -8.8055e-01,  2.2743e-01,\n         -1.4568e-01, -3.2593e-01,  3.7859e-02, -2.1292e-01, -3.8620e-01,\n         -3.2732e-01,  7.4400e-01, -6.8337e-02,  2.6844e-01,  1.5337e-01,\n          1.2386e-01,  4.5234e-01,  3.4875e-01,  1.3522e+00, -7.5968e-01,\n          7.9732e-01,  3.1556e-01,  5.3358e-01, -3.9676e-01, -3.6684e-01,\n          4.6913e-01,  3.4717e-01,  6.2908e-01,  6.4972e-01, -1.9526e-01,\n          3.4955e-03, -4.3906e-01,  3.3199e-01, -1.7163e+00, -3.3515e-01,\n         -9.1095e-02, -7.9908e-01,  2.7138e-01,  6.7128e-01, -6.8959e-03,\n          3.7071e-01, -8.6551e-01,  9.4528e-02,  6.9998e-01, -4.5049e-01,\n         -2.9101e-01,  3.7664e-01, -4.4690e-01, -1.4004e-03,  6.4460e-01,\n         -1.4778e+00, -2.1847e-01,  4.6984e-01, -5.8383e-01,  3.2126e-01,\n         -9.8908e-02,  9.2857e-02,  3.3611e-01,  6.0593e-01, -4.6622e-02,\n          6.4931e-01,  5.0375e-01, -2.9720e-01, -1.5448e-01,  4.8187e-01,\n         -2.2176e-01, -4.1157e-01, -1.9694e-02, -1.0878e+00,  1.1552e-01,\n         -6.2642e-01, -3.0483e-01,  5.0381e-01,  1.2908e+00, -9.8930e-01,\n         -1.3122e-03, -6.4429e-02, -4.4385e-01,  2.9900e-01, -1.4305e-01,\n         -7.0391e-01, -7.7439e-01,  2.3458e-01, -4.0510e-01, -1.1626e-01,\n         -9.6276e-01, -1.4477e-01, -5.5304e-02,  9.5465e-01, -4.0316e-02,\n         -1.9667e-01,  9.5061e-01, -1.0306e+00,  3.6293e-01,  1.3505e-01,\n         -6.6567e-01,  1.2627e-01,  1.5637e-01, -9.1740e-01,  3.9599e-01,\n         -3.9682e-02, -4.9905e-01, -8.9958e-01, -5.1827e-01, -3.3807e-01,\n          8.9582e-02, -2.4238e-01,  1.5478e-01,  4.7668e-01, -3.6524e-01,\n          7.8474e-01,  8.8465e-01,  5.7266e-01, -3.0026e-02,  2.0615e-01,\n          4.9902e-01,  1.2887e+00, -1.6580e-01, -4.5740e-01, -1.6126e+00,\n          2.7822e-02,  2.8140e-01, -1.6096e-01,  2.6398e-02, -5.6187e-01,\n         -1.1744e-01, -2.5909e-01, -1.3387e-02,  3.7001e-01,  9.4863e-01,\n         -5.4048e-01, -1.7203e+00, -4.5748e-01, -3.3717e-01, -3.3423e-01,\n         -2.7843e-01, -1.7661e-01,  3.4573e-01,  4.8724e-02,  7.1933e-03,\n         -1.4947e-01,  6.7782e-01, -2.3142e-01,  3.4604e-01, -3.5820e-01,\n          1.0215e-01, -1.2615e+00, -4.0924e-01,  8.4441e-01, -5.9669e-01,\n         -9.7411e-01,  4.4464e-01, -6.4751e-01, -6.1792e-02,  7.5403e-01,\n         -9.3005e-02,  5.2896e-01,  7.6908e-01,  5.9656e-01, -3.8857e-01,\n         -3.4059e-01,  4.6556e-01,  5.2330e-02, -1.5948e+00, -6.2415e-01,\n         -5.9466e-02,  4.0051e-01,  4.3513e-01, -6.9974e-01, -6.7625e-01,\n          1.5498e-01, -1.5268e-01, -4.9612e-01,  4.6557e-01, -5.7385e-01,\n          1.3507e-01,  1.7209e-01,  2.2392e-01, -7.5631e-01,  1.7173e-01,\n          1.0374e-01, -9.7002e-01, -5.6500e-01, -1.3221e+00, -2.6588e-02,\n         -4.1888e-01,  6.0616e-01, -6.4163e-01, -3.5673e-01,  4.4920e-01,\n         -1.1090e+00,  9.5444e-01, -7.4536e-01,  5.8539e-02, -6.7456e-02,\n          2.3336e-01, -7.3197e-01,  5.8307e-01, -1.9771e-01,  1.9936e-01,\n          4.9243e-01, -1.4257e-01, -4.7371e-01, -3.9872e-01, -4.4555e-01,\n          1.2651e-01, -1.6218e-01,  2.0760e-01, -4.8244e-01,  7.8044e-01,\n          5.9627e-01, -3.9398e-01, -2.1658e-01,  5.4277e-01,  1.3875e+00,\n         -8.5285e-01, -4.5677e-01,  1.0885e+00, -4.0301e-01, -3.1038e-01,\n         -8.6474e-01,  2.5379e-01,  9.4597e-01, -1.0128e+00, -6.3385e-01,\n          3.6645e-01,  4.4168e-01,  8.1224e-01, -3.3201e-01, -3.2714e-01,\n         -6.3941e-01, -4.3923e-02, -6.1053e-01, -1.0257e-01,  1.7413e-01,\n          4.6202e-01,  5.7745e-02, -4.5185e-01,  4.0446e-01,  1.1242e+00,\n         -2.4482e-01, -2.9486e-01,  3.9666e-01, -1.8372e-01, -2.1882e-01,\n         -6.0027e-01,  6.9581e-01,  6.0219e-01,  1.9056e-01, -5.5855e-01,\n          5.4785e-03,  1.6718e-01, -5.5656e-01,  1.1430e-02, -3.9192e-02,\n          1.4620e+00, -3.0348e-01,  6.1252e-01, -5.3722e-01,  4.4317e-01,\n         -2.8994e-01,  1.0999e+00,  1.1371e+00,  1.1831e+00,  4.4491e-01,\n          1.8419e-01, -3.0692e-01, -1.8359e-01,  6.7689e-01, -5.6638e-01,\n          7.3604e-01,  1.7382e+00, -8.7347e-02, -8.3285e-03, -4.9746e-01,\n         -6.8958e-01,  8.6942e-02,  4.7175e-01, -5.3637e-01, -5.3659e-01,\n          4.3034e-01,  1.5921e-01, -2.7128e-02, -1.8795e-01,  8.4269e-01,\n          2.8264e-01,  1.7651e-02,  2.9821e-01,  6.4457e-02,  4.9134e-01,\n         -7.4926e-01,  1.8127e+00,  2.6518e-01,  1.5111e-01, -4.8106e-01,\n          1.8856e-01,  2.2885e-01, -1.1647e-01, -4.0511e-01, -3.5340e-01,\n         -5.8303e-01,  1.5668e-01, -3.6940e-01, -6.9542e-01,  9.5828e-01,\n          7.6455e-02,  2.5183e-01,  2.4749e-01,  1.0498e-02, -3.2126e-01,\n          8.4900e-01, -2.0951e-01, -1.0402e+00,  3.5290e-01,  3.8861e-01,\n         -1.5258e-01,  2.6122e-01, -9.9997e-01,  3.8169e-01,  3.3616e-01,\n          8.3635e-01, -1.0730e-01,  6.5006e-01, -8.7520e-01,  1.1010e+00]],\n       grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1,3,224,224))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T06:44:46.156884Z",
     "start_time": "2023-08-21T06:44:45.881926Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x7f79190767c0>]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "net = models.resnet18(pretrained=False)\n",
    "\n",
    "\n",
    "max_epoch=150 # 一共50 epoch\n",
    "iters=200    # 每个epoch 有 200 个 bach\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler =  torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer = optimizer,T_0 = 10, T_mult=2) #  * iters\n",
    "\n",
    "\n",
    "lr = []\n",
    "for epoch in range(max_epoch):\n",
    "    for batch in range(iters):\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    lr.append(scheduler.get_lr()[0])\n",
    "    scheduler.step() # 注意 每个epoch 结束， 更新learning rate\n",
    "\n",
    "plt.plot(np.arange(max_epoch), lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T08:08:52.770304Z",
     "start_time": "2023-08-21T08:08:50.593936Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from bisect import bisect_right\n",
    "class WarmupMultiStepLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        milestones: List[int],\n",
    "        gamma: float = 0.1,\n",
    "        warmup_factor: float = 0.001,\n",
    "        warmup_iters: int = 1000,\n",
    "        warmup_method: str = \"linear\",\n",
    "        last_epoch: int = -1,\n",
    "    ):\n",
    "        if not list(milestones) == sorted(milestones):\n",
    "            raise ValueError(\n",
    "                \"Milestones should be a list of\" \" increasing integers. Got {}\", milestones\n",
    "            )\n",
    "        self.milestones = milestones\n",
    "        self.gamma = gamma\n",
    "        self.warmup_factor = warmup_factor\n",
    "        self.warmup_iters = warmup_iters\n",
    "        self.warmup_method = warmup_method\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self) -> List[float]:\n",
    "        warmup_factor = _get_warmup_factor_at_iter(\n",
    "            self.warmup_method, self.last_epoch, self.warmup_iters, self.warmup_factor\n",
    "        )\n",
    "        return [\n",
    "            base_lr * warmup_factor * self.gamma ** bisect_right(self.milestones, self.last_epoch)\n",
    "            for base_lr in self.base_lrs\n",
    "##################################################\n",
    "## self.base_lrs 【0.001，.... 0.001】 len = 84\n",
    "#################################################\n",
    "        ]\n",
    "\n",
    "    def _compute_values(self) -> List[float]:\n",
    "        # The new interface\n",
    "        return self.get_lr()\n",
    "\n",
    "\n",
    "\n",
    "def _get_warmup_factor_at_iter(method: str, iter: int, warmup_iters: int, warmup_factor: float) -> float:\n",
    "    \"\"\"\n",
    "    Return the learning rate warmup factor at a specific iteration.\n",
    "    See https://arxiv.org/abs/1706.02677 for more details.\n",
    "\n",
    "    Args:\n",
    "        method (str): warmup method; either \"constant\" or \"linear\".\n",
    "        iter (int): iteration at which to calculate the warmup factor.\n",
    "        warmup_iters (int): the number of warmup iterations.\n",
    "        warmup_factor (float): the base warmup factor (the meaning changes according\n",
    "            to the method used).\n",
    "\n",
    "    Returns:\n",
    "        float: the effective warmup factor at the given iteration.\n",
    "    \"\"\"\n",
    "    if iter >= warmup_iters:\n",
    "        return 1.0\n",
    "\n",
    "    if method == \"constant\":\n",
    "        return warmup_factor\n",
    "    elif method == \"linear\":\n",
    "        alpha = iter / warmup_iters\n",
    "        return warmup_factor * (1 - alpha) + alpha\n",
    "    else:\n",
    "        raise ValueError(\"Unknown warmup method: {}\".format(method))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T08:54:36.559408Z",
     "start_time": "2023-08-21T08:54:36.541117Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x7f79a6c41b20>]"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "from utils.schedulers import CosineAnnealingWarmupLR\n",
    "import utils\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from config import cfg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = resnet18(weights=None)\t# 加载模型\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.1)\t# base_lr = 0.1\n",
    "\n",
    "# 设置warm up的轮次为100次\n",
    "warm_up_iter = 10\n",
    "T_max = 150\t# 周期\n",
    "iters = 200\n",
    "lr_max = 0.1\t# 最大值\n",
    "lr_min = 1e-5\t# 最小值\n",
    "\n",
    "# 为param_groups[0] (即model.layer2) 设置学习率调整规则 - Warm up + Cosine Anneal, lr_lambda计算的是 lr 的乘法因子\n",
    "# lambda0 = lambda cur_iter: (cur_iter / warm_up_iter) if  cur_iter < warm_up_iter else \\\n",
    "#     ((lr_min +\n",
    "#       0.5*(lr_max-lr_min)\n",
    "#       *(1.0+math.cos( (cur_iter-warm_up_iter)\n",
    "#                       /(T_max)*math.pi)))/0.1)\n",
    "\n",
    "# LambdaLR\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda0])\n",
    "# scheduler = getattr(utils.schedulers, 'CosineAnnealingWarmupLR')(optimizer, **opt.CosineAnnealingWarmupLR)\n",
    "scheduler = getattr(optim.lr_scheduler, 'OneCycleLR')(optimizer, **cfg.OneCycleLR)\n",
    "# print(utils.schedulers.__dict__)\n",
    "\n",
    "lr = []\n",
    "for epoch in range(160):\n",
    "    for batch in range(iters):\n",
    "        optimizer.step()\n",
    "        lr.append(scheduler.get_lr()[0])\n",
    "        lr.append(list(optimizer.param_groups)[0]['lr'])\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "# for epoch in range(max_epoch):\n",
    "#     for batch in range(iters):\n",
    "#         optimizer.step()\n",
    "#\n",
    "#\n",
    "#         lr.append(scheduler.get_lr()[0])\n",
    "#         scheduler.step() # 注意 每个epoch 结束， 更新learning rate\n",
    "\n",
    "plt.plot(np.arange(len(lr)), lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T06:41:27.900211Z",
     "start_time": "2023-08-22T06:41:22.307970Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'warmup_iters' is an invalid keyword argument for print()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mopt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCosineAnnealingWarmupLR\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'warmup_iters' is an invalid keyword argument for print()"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T10:13:45.168081Z",
     "start_time": "2023-08-21T10:13:44.134127Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "optimizer.__setattr__('lr', 1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T08:32:01.889544Z",
     "start_time": "2023-08-21T08:32:01.878330Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T08:35:38.942626Z",
     "start_time": "2023-08-21T08:35:38.913982Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "setattr(optimizer, 'lr', 1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T08:34:10.773246Z",
     "start_time": "2023-08-21T08:34:10.761001Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "{'defaults': {'lr': 0.01,\n  'momentum': 0.9,\n  'dampening': 0,\n  'weight_decay': 0,\n  'nesterov': False,\n  'maximize': False,\n  'foreach': None,\n  'differentiable': False},\n '_optimizer_step_pre_hooks': OrderedDict(),\n '_optimizer_step_post_hooks': OrderedDict(),\n '_zero_grad_profile_name': 'Optimizer.zero_grad#SGD.zero_grad',\n 'state': defaultdict(dict, {}),\n 'param_groups': [{'params': [Parameter containing:\n    tensor([[[[-7.1404e-03, -7.4733e-03,  8.7343e-03,  ..., -1.0164e-02,\n               -8.9273e-03,  4.8680e-02],\n              [-4.1810e-02, -1.6961e-02,  1.3489e-02,  ...,  2.1115e-02,\n               -7.1084e-03, -3.5479e-02],\n              [ 1.5529e-02, -2.8140e-03,  6.1207e-02,  ...,  3.7086e-02,\n                6.7508e-03,  3.6157e-02],\n              ...,\n              [-4.2176e-02,  2.7259e-02,  2.3060e-02,  ...,  3.0351e-02,\n               -1.3796e-02, -2.2765e-02],\n              [-2.8254e-02,  1.5029e-02,  4.5346e-02,  ...,  2.4494e-02,\n               -8.8141e-03,  7.4116e-03],\n              [-5.7725e-04, -1.7068e-02, -1.7888e-02,  ..., -1.1643e-02,\n               -1.1455e-02,  2.5079e-02]],\n    \n             [[-6.0099e-03,  2.6280e-02,  1.8836e-02,  ..., -3.7755e-03,\n                4.4867e-02, -5.6969e-02],\n              [ 2.5139e-03,  2.0768e-02, -1.3158e-02,  ..., -2.6689e-02,\n               -4.6827e-02, -1.5056e-02],\n              [-5.7230e-03, -2.3970e-02, -5.2654e-03,  ...,  1.0922e-02,\n               -3.1296e-03, -6.2718e-03],\n              ...,\n              [-2.7346e-02, -1.2489e-02,  1.7945e-04,  ..., -1.6491e-02,\n                4.9465e-02, -8.8486e-03],\n              [ 3.7561e-02, -2.8490e-02,  3.7271e-02,  ..., -3.1561e-03,\n                7.6993e-03, -5.2203e-04],\n              [ 2.3320e-04,  3.9909e-02, -4.6784e-03,  ...,  1.7533e-02,\n               -1.6315e-02,  3.6609e-02]],\n    \n             [[-1.1169e-02,  3.3225e-02, -5.3413e-02,  ...,  6.4199e-03,\n               -7.6204e-04, -2.9799e-02],\n              [-1.9201e-02, -2.4581e-02, -1.2386e-02,  ...,  2.2140e-02,\n               -1.2910e-02,  1.8810e-02],\n              [-9.5672e-03, -7.7919e-03, -2.1912e-02,  ...,  5.6868e-04,\n                6.8331e-03, -2.9353e-02],\n              ...,\n              [ 1.5757e-02, -1.9784e-02,  7.7431e-04,  ..., -9.2923e-03,\n                1.8837e-03,  2.0007e-02],\n              [-1.1657e-02, -1.2967e-02, -9.0579e-03,  ...,  1.0891e-02,\n               -3.5740e-02, -1.5165e-02],\n              [ 8.6293e-03, -5.6948e-03,  2.2729e-02,  ...,  1.8414e-02,\n                2.6295e-02, -3.5042e-03]]],\n    \n    \n            [[[ 8.8417e-03, -5.4604e-03, -1.3415e-03,  ..., -4.1196e-02,\n                1.3626e-02,  4.8149e-03],\n              [ 3.7111e-02,  5.2432e-03,  4.5896e-03,  ..., -1.8592e-02,\n                2.7547e-02,  1.1344e-02],\n              [ 3.1319e-02,  2.1385e-02, -1.3173e-02,  ...,  2.1463e-02,\n               -1.4458e-03,  1.0129e-02],\n              ...,\n              [-5.2314e-03, -5.6322e-04,  1.6404e-02,  ..., -4.2407e-02,\n                7.1322e-02,  2.9941e-02],\n              [-1.2828e-02,  2.5389e-02, -3.8248e-03,  ..., -3.5330e-02,\n               -7.2721e-03, -2.5494e-03],\n              [ 3.2300e-03,  1.5138e-02,  2.4379e-02,  ...,  1.4856e-02,\n               -3.0790e-03, -1.9642e-02]],\n    \n             [[-5.3895e-03, -2.7325e-02, -3.0403e-02,  ..., -4.0289e-02,\n                1.8690e-02,  1.5439e-02],\n              [ 8.5971e-04,  3.0002e-03,  3.3367e-02,  ...,  3.8728e-02,\n               -1.8712e-02, -5.6994e-04],\n              [ 5.4640e-06,  6.9418e-03,  2.1550e-02,  ..., -1.9201e-02,\n               -2.3628e-02, -4.5070e-02],\n              ...,\n              [ 1.4499e-02,  3.7911e-02,  3.4790e-02,  ..., -2.6948e-02,\n               -2.3815e-02,  6.4854e-03],\n              [-3.9044e-02, -4.3352e-02, -9.5662e-03,  ...,  5.3710e-04,\n               -1.9348e-02, -2.1791e-02],\n              [ 2.4939e-02, -1.5215e-02, -3.4046e-03,  ...,  1.6904e-02,\n               -6.7378e-03,  1.7460e-02]],\n    \n             [[-1.7268e-03,  4.7036e-03, -1.9391e-03,  ..., -2.2017e-02,\n               -2.8657e-02, -8.6292e-03],\n              [-2.9552e-02,  1.7794e-02, -1.0008e-02,  ...,  4.2806e-02,\n               -1.9035e-02, -2.7639e-02],\n              [-1.0113e-02, -3.7960e-02, -3.6353e-02,  ...,  1.1964e-02,\n               -1.3844e-02, -1.4290e-03],\n              ...,\n              [-7.3460e-03, -2.3144e-03, -4.6316e-03,  ..., -7.0099e-03,\n               -3.4592e-02, -4.6743e-02],\n              [-5.1903e-03, -3.8004e-02, -2.2289e-02,  ..., -1.2386e-03,\n               -1.6033e-02,  4.2493e-03],\n              [ 3.6659e-02,  2.1696e-02, -1.9676e-02,  ..., -1.4439e-02,\n               -2.8179e-02,  5.5242e-02]]],\n    \n    \n            [[[ 3.1248e-02, -3.0067e-02, -2.7740e-02,  ..., -6.1310e-03,\n               -2.1524e-02,  3.7181e-03],\n              [ 1.1013e-02, -1.3849e-02,  4.3947e-02,  ..., -7.6159e-03,\n               -1.5880e-02, -1.2301e-02],\n              [ 1.2478e-02,  3.8817e-03,  1.6454e-02,  ..., -4.3332e-02,\n               -8.3096e-03,  4.1784e-02],\n              ...,\n              [ 2.0022e-03,  1.7294e-02, -3.3324e-02,  ..., -3.9306e-02,\n               -8.7672e-03,  2.0614e-02],\n              [ 9.6176e-03, -2.5064e-03, -2.2459e-02,  ..., -2.9462e-02,\n                1.3141e-02,  1.4362e-02],\n              [-1.8060e-02, -4.2450e-02,  6.0439e-03,  ...,  1.9249e-02,\n               -1.6661e-02, -8.9243e-03]],\n    \n             [[-6.2778e-02,  1.3390e-02, -2.2811e-02,  ...,  5.4948e-04,\n               -9.1444e-03, -2.8657e-02],\n              [-1.3766e-02,  2.6998e-02, -1.6456e-02,  ...,  8.4684e-03,\n               -1.6804e-04, -2.2008e-02],\n              [-6.1722e-02, -5.7015e-02, -5.9509e-03,  ..., -4.4795e-02,\n               -2.4048e-02,  2.0593e-02],\n              ...,\n              [-1.9590e-02,  2.5830e-02, -2.3061e-02,  ...,  1.1281e-02,\n               -3.4468e-02, -4.2363e-02],\n              [-7.1121e-03, -1.3002e-02,  9.7743e-03,  ..., -4.3075e-02,\n                3.1686e-03,  9.5774e-03],\n              [ 3.7398e-03, -6.8414e-03,  3.4841e-02,  ...,  1.3261e-02,\n                3.3293e-02,  2.1376e-02]],\n    \n             [[-1.9856e-02, -4.0286e-02,  2.9246e-02,  ...,  1.8037e-02,\n                2.0940e-02, -3.5930e-02],\n              [-3.3833e-02,  2.0548e-02, -3.4658e-02,  ..., -5.8168e-02,\n               -3.2811e-02,  1.9011e-02],\n              [-2.6109e-02, -3.8032e-03,  6.1372e-02,  ..., -5.1748e-04,\n                7.2188e-03,  2.9532e-02],\n              ...,\n              [-1.3174e-02, -7.3150e-03, -2.2365e-04,  ...,  8.4006e-03,\n               -1.7630e-02,  5.7517e-03],\n              [ 1.8402e-02, -1.2257e-02,  3.6851e-03,  ..., -4.5955e-02,\n                5.4689e-03,  2.3583e-02],\n              [ 8.6045e-03, -3.4991e-02,  2.9343e-02,  ...,  2.0045e-03,\n                1.4059e-02, -1.9955e-02]]],\n    \n    \n            ...,\n    \n    \n            [[[-8.0101e-03, -4.7368e-02, -2.7672e-02,  ...,  9.3854e-05,\n                1.2508e-02, -6.5810e-03],\n              [-1.4996e-02, -7.6764e-03,  1.3966e-02,  ...,  2.6644e-02,\n               -4.0562e-02, -1.9665e-02],\n              [ 6.1438e-02, -2.3704e-02, -2.9991e-02,  ..., -4.4266e-02,\n                2.4239e-02,  2.7210e-02],\n              ...,\n              [ 2.3025e-02, -1.4620e-03, -3.6267e-02,  ..., -3.5307e-02,\n               -2.4846e-02, -3.9038e-02],\n              [-4.2533e-02,  2.5143e-02, -5.1347e-03,  ...,  1.0907e-02,\n                3.4176e-02, -5.3710e-03],\n              [-1.8852e-02, -4.1163e-02, -1.3490e-02,  ..., -1.9786e-02,\n                2.3909e-02,  1.0370e-02]],\n    \n             [[-3.6106e-03,  2.5734e-02,  1.7094e-02,  ...,  4.2368e-02,\n               -1.9125e-02,  1.2151e-03],\n              [ 2.1739e-02,  7.4142e-03, -8.9568e-03,  ..., -2.3368e-02,\n                1.2934e-02,  2.0302e-02],\n              [-5.1560e-03,  8.8712e-03, -2.3797e-02,  ..., -2.9202e-02,\n               -1.9714e-03,  2.3073e-02],\n              ...,\n              [-7.3101e-03,  1.6157e-02,  9.3400e-03,  ...,  1.5084e-02,\n                2.1066e-03, -7.3647e-03],\n              [-1.7553e-03,  2.2558e-02,  2.5583e-02,  ...,  1.9530e-02,\n                3.0982e-02, -1.7273e-02],\n              [-1.8099e-02,  4.2158e-02, -5.5355e-03,  ...,  1.8881e-03,\n               -6.9533e-02,  3.6745e-02]],\n    \n             [[ 7.3383e-03, -3.0046e-03,  3.1521e-02,  ...,  3.5160e-03,\n                8.7556e-03,  1.2824e-02],\n              [ 3.2551e-02, -1.5327e-02, -2.2221e-02,  ...,  4.7517e-02,\n               -3.4004e-02, -1.2374e-02],\n              [-3.0785e-02,  2.8643e-02,  1.0061e-02,  ...,  7.6123e-04,\n                2.6055e-02,  1.2460e-04],\n              ...,\n              [ 2.1653e-03, -5.7366e-03,  1.0244e-02,  ...,  1.6465e-02,\n                2.4126e-02,  1.0591e-02],\n              [ 4.6345e-02, -4.2562e-02, -3.3019e-02,  ..., -1.9263e-02,\n                1.3435e-02,  8.1851e-03],\n              [ 2.4420e-02, -1.9435e-02,  7.0969e-03,  ...,  5.4547e-02,\n                6.5464e-03, -3.8848e-02]]],\n    \n    \n            [[[ 3.4396e-02, -1.3398e-02, -9.7031e-03,  ..., -4.2165e-03,\n                1.7995e-02, -6.9804e-02],\n              [-3.6606e-02, -9.4933e-03,  3.1037e-02,  ..., -9.7645e-03,\n               -6.2449e-03,  2.6611e-02],\n              [ 1.9683e-02,  7.7826e-03, -5.1592e-02,  ...,  3.3320e-02,\n                4.4986e-04, -2.5888e-02],\n              ...,\n              [-2.8722e-02, -4.2707e-03,  8.4396e-03,  ...,  1.6652e-02,\n               -2.9622e-02,  1.9604e-02],\n              [-1.1247e-02,  1.0270e-02, -2.2386e-02,  ...,  2.0576e-02,\n               -1.0433e-02,  9.2402e-03],\n              [-8.3044e-03, -4.1027e-03,  8.5376e-03,  ..., -8.8421e-04,\n               -3.1265e-02, -1.2603e-03]],\n    \n             [[ 7.6493e-03,  1.0594e-02, -3.2038e-02,  ...,  4.7604e-02,\n               -3.1300e-02, -4.4290e-02],\n              [-4.2076e-02, -1.1125e-02, -7.0594e-02,  ...,  1.6086e-02,\n                3.8311e-03, -3.6211e-02],\n              [ 4.0544e-02, -1.8319e-02,  3.0052e-02,  ...,  2.1508e-02,\n               -1.6668e-02,  5.4622e-03],\n              ...,\n              [ 1.6895e-02, -1.8255e-02,  2.3499e-03,  ..., -1.3411e-02,\n               -4.8680e-02,  2.9214e-02],\n              [ 1.9856e-03,  8.0592e-03,  2.8095e-03,  ...,  2.6491e-04,\n                8.7624e-03,  1.8670e-02],\n              [ 3.4992e-02, -1.4596e-02,  7.8350e-03,  ..., -1.0999e-02,\n               -1.3193e-02,  3.4129e-02]],\n    \n             [[-9.1849e-03,  2.0878e-02, -1.9334e-02,  ..., -4.2967e-02,\n               -7.5825e-03,  2.1667e-02],\n              [ 1.0779e-02, -6.6622e-03, -3.0944e-02,  ...,  4.1621e-02,\n                4.7622e-03,  5.1376e-02],\n              [-1.0733e-02, -1.2248e-02,  3.0020e-03,  ..., -6.0709e-02,\n                1.0248e-02,  2.4787e-02],\n              ...,\n              [-1.2453e-03, -3.6515e-02, -3.8735e-02,  ...,  6.7886e-03,\n                1.7251e-02,  2.9521e-02],\n              [-2.5069e-02,  4.1098e-02,  2.2299e-02,  ...,  2.3725e-03,\n               -5.0067e-03, -1.5223e-02],\n              [ 7.2575e-03,  5.8264e-03,  2.3509e-02,  ..., -1.4419e-02,\n                4.6584e-03,  2.0153e-02]]],\n    \n    \n            [[[ 1.3643e-02,  3.4294e-02,  2.4654e-02,  ..., -3.4619e-02,\n                2.4184e-02,  3.5540e-02],\n              [-2.7514e-02,  3.5889e-03,  3.7293e-03,  ...,  4.9935e-03,\n                1.1816e-02,  1.2974e-02],\n              [-4.0706e-02, -7.3796e-03,  1.0349e-02,  ...,  2.7751e-02,\n                7.0618e-04, -1.0965e-02],\n              ...,\n              [-1.9999e-02,  1.1642e-02, -1.3448e-02,  ..., -4.6629e-02,\n                3.7250e-02,  9.5574e-02],\n              [-2.5918e-02, -3.2236e-02,  3.5534e-02,  ..., -1.3701e-02,\n                1.5506e-02, -1.5267e-02],\n              [-1.9172e-03, -1.3537e-02,  1.1842e-02,  ...,  6.9489e-03,\n                1.0914e-02,  1.3352e-02]],\n    \n             [[-5.1476e-02,  1.6628e-02, -4.9674e-02,  ...,  8.6832e-03,\n                4.0670e-03, -1.1754e-02],\n              [ 2.3883e-02,  1.0897e-02,  1.3622e-02,  ..., -2.7664e-02,\n               -9.5039e-03, -3.8200e-04],\n              [-2.6624e-02,  4.1311e-02, -3.0163e-02,  ...,  2.3975e-02,\n                2.5665e-02,  4.5823e-03],\n              ...,\n              [-1.3320e-02, -4.2313e-03,  3.5819e-02,  ...,  2.5189e-02,\n               -4.7757e-02, -1.7329e-02],\n              [-2.2253e-02,  7.3803e-03, -4.5496e-04,  ...,  2.1770e-02,\n                4.2885e-03,  4.6440e-02],\n              [ 4.4231e-02,  2.4666e-02, -2.2541e-02,  ..., -3.8044e-02,\n                2.8740e-02,  1.0499e-03]],\n    \n             [[-4.6687e-03, -6.2704e-02, -3.2327e-02,  ...,  3.0747e-02,\n                5.9401e-02, -6.0839e-03],\n              [-3.1864e-02,  1.8410e-03, -3.9936e-02,  ...,  1.8405e-02,\n                1.2629e-02, -6.8607e-03],\n              [ 2.3213e-02,  2.1234e-03, -2.9355e-02,  ..., -6.1530e-03,\n                2.1512e-02, -2.1078e-02],\n              ...,\n              [ 3.8108e-02,  4.0705e-02,  1.1940e-03,  ..., -2.9376e-03,\n                1.3160e-02, -2.3248e-02],\n              [ 1.9385e-02, -2.2407e-02, -3.1158e-02,  ...,  8.4535e-03,\n               -1.1433e-02,  2.2084e-03],\n              [-3.0171e-02,  8.9181e-03,  4.0469e-02,  ...,  2.9125e-02,\n                1.8823e-02, -3.5987e-02]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n           requires_grad=True),\n    Parameter containing:\n    tensor([[[[ 0.0291, -0.0731, -0.0005],\n              [ 0.0548,  0.1114, -0.0368],\n              [-0.0458, -0.0388,  0.0660]],\n    \n             [[-0.0432,  0.0388,  0.0924],\n              [ 0.0648, -0.0346, -0.0430],\n              [-0.0508,  0.1049,  0.0171]],\n    \n             [[ 0.0493, -0.0638,  0.0435],\n              [ 0.0279,  0.0041,  0.0809],\n              [ 0.0570,  0.0866, -0.0346]],\n    \n             ...,\n    \n             [[-0.0391,  0.1029,  0.0161],\n              [-0.0548, -0.0544, -0.0094],\n              [-0.0716, -0.0462, -0.1192]],\n    \n             [[-0.0524, -0.1034,  0.0010],\n              [-0.0625, -0.0454,  0.0077],\n              [-0.0041,  0.0752, -0.0053]],\n    \n             [[-0.0905,  0.0015, -0.0497],\n              [ 0.0160,  0.0649,  0.0240],\n              [-0.0218, -0.0174, -0.0552]]],\n    \n    \n            [[[-0.0663,  0.0176, -0.0647],\n              [-0.0400,  0.0028,  0.0377],\n              [-0.0051, -0.0064, -0.0158]],\n    \n             [[-0.0409,  0.0983, -0.0157],\n              [-0.0228,  0.0195,  0.0393],\n              [-0.0792, -0.0095,  0.0694]],\n    \n             [[-0.0752, -0.0581, -0.0559],\n              [-0.0089,  0.0033, -0.0819],\n              [ 0.0396,  0.0388,  0.0489]],\n    \n             ...,\n    \n             [[ 0.0027,  0.0998,  0.1164],\n              [-0.0752,  0.0420, -0.0329],\n              [ 0.0261,  0.0276, -0.0176]],\n    \n             [[-0.1149, -0.0303,  0.0411],\n              [-0.0635, -0.0460,  0.0546],\n              [-0.0478,  0.0018, -0.0343]],\n    \n             [[-0.0245,  0.0062,  0.0883],\n              [ 0.0096, -0.0109, -0.1103],\n              [ 0.0489, -0.0331, -0.0369]]],\n    \n    \n            [[[ 0.0222,  0.0114,  0.0147],\n              [-0.0202,  0.0065,  0.1331],\n              [-0.0817, -0.0429,  0.0127]],\n    \n             [[ 0.0556, -0.0897,  0.0980],\n              [ 0.0789, -0.0197, -0.0037],\n              [ 0.1051, -0.0080, -0.0686]],\n    \n             [[-0.0304, -0.0106,  0.0033],\n              [-0.0555, -0.0048,  0.0757],\n              [ 0.0213, -0.0038,  0.0318]],\n    \n             ...,\n    \n             [[ 0.0498, -0.0419, -0.0254],\n              [-0.1599,  0.0649,  0.0007],\n              [ 0.0713, -0.0308, -0.0264]],\n    \n             [[ 0.0317, -0.0501, -0.0789],\n              [ 0.0297, -0.0045,  0.0782],\n              [ 0.0500, -0.0031, -0.0057]],\n    \n             [[-0.1131,  0.0601,  0.0719],\n              [ 0.1621,  0.0427,  0.0292],\n              [ 0.0937,  0.0993, -0.0580]]],\n    \n    \n            ...,\n    \n    \n            [[[-0.0473,  0.1064,  0.0325],\n              [ 0.0245,  0.0543, -0.0063],\n              [-0.1154,  0.0347,  0.0208]],\n    \n             [[-0.0739, -0.0066,  0.1071],\n              [-0.0126, -0.0458, -0.0674],\n              [ 0.1295, -0.0096,  0.0966]],\n    \n             [[-0.0149, -0.0135, -0.0164],\n              [ 0.0153, -0.0216,  0.1009],\n              [-0.0246,  0.0337,  0.0033]],\n    \n             ...,\n    \n             [[ 0.0091, -0.0460, -0.1372],\n              [-0.0248, -0.0294,  0.0565],\n              [-0.1193,  0.0782, -0.0238]],\n    \n             [[ 0.0478,  0.0105,  0.0799],\n              [-0.0564, -0.0639,  0.0285],\n              [-0.1022, -0.0738, -0.0612]],\n    \n             [[ 0.0084,  0.0146, -0.0170],\n              [-0.0649, -0.0499, -0.0050],\n              [-0.0337,  0.0009,  0.0220]]],\n    \n    \n            [[[-0.0936,  0.0350,  0.0245],\n              [-0.0126, -0.1277, -0.0358],\n              [-0.0396, -0.0381, -0.0091]],\n    \n             [[-0.0559,  0.0511, -0.0648],\n              [-0.0108, -0.0404, -0.0166],\n              [-0.0116, -0.1357, -0.0355]],\n    \n             [[-0.1112,  0.0348,  0.0902],\n              [-0.0987,  0.0510, -0.0388],\n              [-0.0562,  0.1073, -0.0058]],\n    \n             ...,\n    \n             [[-0.0081,  0.0063, -0.0526],\n              [-0.1222, -0.0831,  0.0513],\n              [ 0.0610,  0.0228, -0.1516]],\n    \n             [[-0.0598, -0.0205,  0.0221],\n              [ 0.0417, -0.0261,  0.0912],\n              [-0.0493, -0.0886, -0.0642]],\n    \n             [[ 0.0213,  0.0823, -0.0429],\n              [ 0.0896, -0.0204,  0.0163],\n              [-0.0021, -0.0188, -0.0617]]],\n    \n    \n            [[[ 0.0558, -0.0486, -0.0690],\n              [ 0.1228, -0.0066, -0.0357],\n              [ 0.0501, -0.0637,  0.0296]],\n    \n             [[ 0.0234,  0.0208, -0.0275],\n              [ 0.0009,  0.0394, -0.0146],\n              [-0.0696, -0.0389, -0.1175]],\n    \n             [[ 0.2013,  0.0571, -0.0776],\n              [-0.0619, -0.0032, -0.0077],\n              [ 0.0522, -0.0158, -0.0579]],\n    \n             ...,\n    \n             [[-0.0603,  0.0525, -0.0942],\n              [-0.0065, -0.1034,  0.0118],\n              [ 0.0013,  0.0549, -0.0788]],\n    \n             [[-0.0304,  0.0440, -0.0158],\n              [-0.0670,  0.0095,  0.0374],\n              [ 0.0733, -0.1254,  0.0229]],\n    \n             [[-0.0578, -0.0936,  0.0620],\n              [ 0.0777, -0.0833, -0.1388],\n              [ 0.0364, -0.0552, -0.0248]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n           requires_grad=True),\n    Parameter containing:\n    tensor([[[[-0.0052,  0.0241, -0.0595],\n              [-0.0310, -0.0003, -0.0441],\n              [-0.1002,  0.0709,  0.0589]],\n    \n             [[ 0.0432, -0.0135, -0.0553],\n              [ 0.0594,  0.0084,  0.0077],\n              [-0.1012,  0.0395, -0.1053]],\n    \n             [[-0.0233,  0.0217, -0.0108],\n              [ 0.0297, -0.1044, -0.0865],\n              [-0.0046, -0.0317, -0.0151]],\n    \n             ...,\n    \n             [[-0.0793, -0.0003,  0.0896],\n              [-0.0242, -0.0343, -0.0447],\n              [-0.0441,  0.0321,  0.0288]],\n    \n             [[ 0.0544, -0.0863, -0.0730],\n              [ 0.0053,  0.0141,  0.0256],\n              [-0.0482, -0.0033,  0.0275]],\n    \n             [[-0.0482,  0.0142,  0.0310],\n              [ 0.0058,  0.0249, -0.0388],\n              [-0.0021,  0.0768,  0.0790]]],\n    \n    \n            [[[ 0.0400,  0.0447, -0.0645],\n              [-0.0763, -0.0232,  0.0374],\n              [-0.0059, -0.0470,  0.0746]],\n    \n             [[ 0.0077, -0.1207, -0.0859],\n              [ 0.0424,  0.0617, -0.0616],\n              [ 0.0301,  0.0523, -0.0294]],\n    \n             [[-0.1333,  0.0336, -0.1020],\n              [-0.0551,  0.0301,  0.0038],\n              [-0.0262,  0.0458,  0.0719]],\n    \n             ...,\n    \n             [[-0.0193,  0.0484,  0.0506],\n              [ 0.0540, -0.0385, -0.1179],\n              [ 0.0468, -0.0315,  0.1064]],\n    \n             [[-0.1000,  0.1354,  0.0183],\n              [-0.0002, -0.0017, -0.1070],\n              [ 0.0096,  0.1197,  0.0120]],\n    \n             [[ 0.0536, -0.0229, -0.0928],\n              [-0.0140,  0.0180, -0.0853],\n              [ 0.0034, -0.0970,  0.0811]]],\n    \n    \n            [[[ 0.0658, -0.0955,  0.0704],\n              [-0.0627, -0.0166, -0.0653],\n              [ 0.0469,  0.1172,  0.0580]],\n    \n             [[-0.0131,  0.0678, -0.0681],\n              [ 0.0222,  0.0219,  0.0068],\n              [-0.0474, -0.0161, -0.0393]],\n    \n             [[-0.0781,  0.0552, -0.0533],\n              [ 0.0907, -0.0523, -0.0343],\n              [ 0.0888, -0.0359,  0.0159]],\n    \n             ...,\n    \n             [[ 0.0146, -0.0529, -0.0151],\n              [-0.1300,  0.0065, -0.0188],\n              [-0.0126,  0.1054, -0.0204]],\n    \n             [[-0.0321,  0.0057,  0.0025],\n              [ 0.0323, -0.0086,  0.0034],\n              [ 0.0063, -0.0387,  0.0283]],\n    \n             [[ 0.0101, -0.0470,  0.1088],\n              [ 0.0220, -0.0270,  0.0258],\n              [-0.0378,  0.0773,  0.0523]]],\n    \n    \n            ...,\n    \n    \n            [[[-0.0053, -0.0044, -0.0564],\n              [-0.0205, -0.0457, -0.0412],\n              [ 0.0262, -0.0806, -0.0498]],\n    \n             [[-0.0715, -0.1297, -0.0051],\n              [-0.0501, -0.0114,  0.0140],\n              [-0.0406, -0.0587,  0.0257]],\n    \n             [[-0.0006, -0.0381,  0.0095],\n              [ 0.0531,  0.0158,  0.0534],\n              [ 0.0087,  0.0317,  0.0635]],\n    \n             ...,\n    \n             [[-0.0221, -0.0090, -0.0128],\n              [ 0.0004, -0.0187, -0.0317],\n              [-0.0118,  0.1022, -0.0358]],\n    \n             [[-0.0531,  0.0578,  0.0190],\n              [ 0.0915, -0.0116, -0.0034],\n              [-0.0587,  0.0033, -0.0373]],\n    \n             [[ 0.0896, -0.0168, -0.1157],\n              [ 0.0152, -0.0203,  0.0538],\n              [-0.0210,  0.0313,  0.0642]]],\n    \n    \n            [[[-0.0319, -0.0808,  0.0901],\n              [ 0.0506, -0.0814, -0.0548],\n              [ 0.0634,  0.0982, -0.0060]],\n    \n             [[-0.0177,  0.0655, -0.0351],\n              [-0.0071,  0.0236, -0.0321],\n              [ 0.1462,  0.0616,  0.0904]],\n    \n             [[-0.0803, -0.1380,  0.0465],\n              [-0.0305,  0.0449, -0.0264],\n              [-0.0573,  0.0892, -0.0822]],\n    \n             ...,\n    \n             [[-0.0013, -0.0154, -0.0275],\n              [ 0.0410,  0.0613,  0.0215],\n              [-0.0333, -0.0392, -0.0249]],\n    \n             [[ 0.0977,  0.0045,  0.0290],\n              [ 0.0050, -0.0982, -0.0179],\n              [-0.0168,  0.0214,  0.0061]],\n    \n             [[ 0.0163,  0.0229, -0.0994],\n              [-0.0241,  0.0053,  0.0201],\n              [ 0.0831, -0.0012, -0.0207]]],\n    \n    \n            [[[-0.0170,  0.0356,  0.0341],\n              [-0.1283,  0.0720, -0.0325],\n              [-0.0658,  0.0076,  0.0611]],\n    \n             [[ 0.0391,  0.0901, -0.0121],\n              [-0.0761, -0.0403,  0.0386],\n              [-0.1640, -0.0263, -0.0791]],\n    \n             [[-0.0265, -0.0654,  0.0486],\n              [-0.0111,  0.0212, -0.0439],\n              [ 0.1332, -0.0357,  0.0357]],\n    \n             ...,\n    \n             [[ 0.0292, -0.0369, -0.0027],\n              [ 0.0409,  0.0059,  0.0858],\n              [ 0.0336, -0.0340, -0.0877]],\n    \n             [[ 0.0409,  0.0757,  0.1452],\n              [-0.0060, -0.0694, -0.0114],\n              [-0.0904, -0.0276, -0.0539]],\n    \n             [[ 0.0557,  0.0108, -0.0292],\n              [-0.0756, -0.0155,  0.1088],\n              [ 0.0272,  0.0894,  0.0541]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n           requires_grad=True),\n    Parameter containing:\n    tensor([[[[-0.0155,  0.0148,  0.0410],\n              [-0.0340, -0.0667, -0.0134],\n              [ 0.0747, -0.0328,  0.0062]],\n    \n             [[ 0.0240,  0.0722,  0.0673],\n              [-0.1046, -0.0158,  0.0007],\n              [-0.0690,  0.0140, -0.0313]],\n    \n             [[ 0.0309, -0.0468,  0.0279],\n              [ 0.0135,  0.0439, -0.0340],\n              [-0.0574, -0.0045, -0.0233]],\n    \n             ...,\n    \n             [[ 0.0138,  0.0158,  0.0093],\n              [-0.0284,  0.0424, -0.0730],\n              [-0.0930,  0.0228,  0.0447]],\n    \n             [[ 0.0005,  0.0362, -0.0906],\n              [ 0.0645,  0.0169,  0.0115],\n              [ 0.0614, -0.0719,  0.0673]],\n    \n             [[-0.0656,  0.0993, -0.0610],\n              [-0.0388,  0.0135, -0.0921],\n              [-0.0114, -0.0701, -0.0115]]],\n    \n    \n            [[[ 0.0418,  0.0013,  0.0082],\n              [-0.0648,  0.0130,  0.0108],\n              [ 0.1205,  0.0844,  0.0104]],\n    \n             [[-0.0423,  0.0649, -0.0297],\n              [ 0.0166,  0.0501,  0.0094],\n              [-0.0122,  0.0470,  0.0420]],\n    \n             [[-0.0650,  0.0728,  0.1500],\n              [-0.0503,  0.0206,  0.1199],\n              [ 0.0108,  0.0330,  0.0227]],\n    \n             ...,\n    \n             [[-0.0468, -0.2060, -0.0059],\n              [-0.0497, -0.0373,  0.0141],\n              [-0.0021, -0.0434,  0.0174]],\n    \n             [[-0.0089, -0.0214,  0.0322],\n              [ 0.0365, -0.0928,  0.0494],\n              [ 0.0244,  0.0568, -0.0533]],\n    \n             [[ 0.0675,  0.0433, -0.0346],\n              [ 0.0405,  0.0073,  0.0223],\n              [-0.0863,  0.0473, -0.0353]]],\n    \n    \n            [[[ 0.0245, -0.0063, -0.1158],\n              [-0.0018, -0.0515, -0.0345],\n              [ 0.0639,  0.0508, -0.0753]],\n    \n             [[ 0.0028, -0.0071,  0.0483],\n              [ 0.0323, -0.0665,  0.1049],\n              [-0.0632, -0.0393, -0.0626]],\n    \n             [[ 0.0012,  0.0049, -0.0042],\n              [ 0.0614, -0.1262,  0.0438],\n              [-0.0750,  0.0171,  0.0094]],\n    \n             ...,\n    \n             [[-0.0257,  0.0039,  0.0946],\n              [ 0.0617,  0.0242, -0.0470],\n              [-0.1150,  0.0191, -0.0537]],\n    \n             [[-0.0218,  0.0648,  0.0157],\n              [ 0.0490,  0.0409, -0.0346],\n              [ 0.0165, -0.0292, -0.0681]],\n    \n             [[-0.0669, -0.0425, -0.0010],\n              [ 0.0074, -0.0130, -0.0700],\n              [ 0.0774, -0.0124, -0.0219]]],\n    \n    \n            ...,\n    \n    \n            [[[-0.0211, -0.1239,  0.0651],\n              [ 0.0514, -0.0513,  0.1646],\n              [ 0.0138,  0.0735,  0.0364]],\n    \n             [[ 0.0138, -0.0513,  0.0700],\n              [-0.0662,  0.0278,  0.0649],\n              [-0.0099,  0.0217, -0.1264]],\n    \n             [[ 0.0481, -0.0401,  0.0215],\n              [-0.0277,  0.0437,  0.0414],\n              [ 0.0281, -0.0176, -0.0034]],\n    \n             ...,\n    \n             [[ 0.0074,  0.0433, -0.0585],\n              [-0.0175, -0.0181, -0.0255],\n              [ 0.0179, -0.0201,  0.1220]],\n    \n             [[ 0.0498, -0.0276, -0.1441],\n              [ 0.0608, -0.0082, -0.0032],\n              [ 0.0742,  0.0222,  0.0119]],\n    \n             [[-0.0432, -0.0024,  0.1424],\n              [-0.0604,  0.0489,  0.0841],\n              [-0.0772, -0.0558, -0.1051]]],\n    \n    \n            [[[-0.0241,  0.0168,  0.0509],\n              [ 0.0710,  0.0161,  0.0416],\n              [-0.0122, -0.0229,  0.0807]],\n    \n             [[ 0.0110,  0.0679,  0.0136],\n              [-0.0332, -0.0562,  0.0445],\n              [ 0.0373,  0.0662,  0.0838]],\n    \n             [[ 0.0009, -0.0565,  0.0657],\n              [-0.0513, -0.0674,  0.0946],\n              [-0.0314,  0.0765,  0.0168]],\n    \n             ...,\n    \n             [[-0.0120,  0.0501,  0.0390],\n              [-0.0047,  0.1024,  0.0102],\n              [-0.0196,  0.0652, -0.0769]],\n    \n             [[ 0.1297,  0.0091,  0.0628],\n              [-0.0034,  0.0478,  0.0953],\n              [-0.0395,  0.0027,  0.0389]],\n    \n             [[-0.0657, -0.0190, -0.0505],\n              [-0.1045, -0.0031, -0.0269],\n              [-0.0386, -0.1133,  0.0056]]],\n    \n    \n            [[[-0.0340, -0.0863,  0.0466],\n              [-0.0687,  0.0626,  0.0099],\n              [ 0.1716,  0.0317,  0.0320]],\n    \n             [[-0.0615,  0.0579, -0.0270],\n              [-0.0186,  0.0238,  0.0322],\n              [ 0.0151,  0.0002,  0.0514]],\n    \n             [[-0.0653, -0.0078, -0.0742],\n              [-0.0102, -0.0215, -0.0303],\n              [ 0.0169, -0.0753, -0.0716]],\n    \n             ...,\n    \n             [[ 0.0209, -0.0726, -0.0422],\n              [-0.0210,  0.0188, -0.0500],\n              [-0.0176, -0.0324, -0.0529]],\n    \n             [[-0.0324,  0.0253, -0.0020],\n              [ 0.0676,  0.0334,  0.0157],\n              [-0.0247, -0.0172, -0.0323]],\n    \n             [[ 0.0280,  0.0280,  0.0223],\n              [ 0.1035,  0.0442,  0.0147],\n              [-0.0625,  0.0647, -0.0924]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n           requires_grad=True),\n    Parameter containing:\n    tensor([[[[ 1.1117e-01, -2.9095e-02,  6.2351e-02],\n              [ 4.5505e-02, -4.3320e-02,  9.6972e-02],\n              [-1.0099e-02,  2.4612e-02,  1.9735e-02]],\n    \n             [[ 3.9901e-02,  5.3589e-02,  5.5181e-02],\n              [-3.2049e-02,  4.4902e-02,  8.3180e-02],\n              [-3.5597e-02, -4.9842e-02,  5.4306e-02]],\n    \n             [[ 4.9917e-02,  5.6091e-02,  5.7939e-02],\n              [ 9.1379e-02, -2.9504e-02, -1.8471e-02],\n              [ 1.4255e-02,  1.5089e-02,  5.1303e-02]],\n    \n             ...,\n    \n             [[-7.5602e-02, -4.1674e-02,  2.4497e-02],\n              [-4.6787e-02, -2.6641e-02,  6.6398e-02],\n              [-3.9820e-02,  4.7777e-02,  1.1117e-01]],\n    \n             [[-2.9657e-02, -1.1342e-02, -6.9376e-03],\n              [ 8.6026e-02,  1.2342e-01,  2.8104e-02],\n              [ 4.1824e-02,  1.1131e-01,  3.1292e-02]],\n    \n             [[ 2.8271e-02,  9.8473e-02,  7.2533e-02],\n              [ 5.1912e-02,  3.1571e-02,  2.0590e-02],\n              [ 1.4871e-02, -3.5717e-02,  2.4491e-02]]],\n    \n    \n            [[[-9.0706e-02, -4.0365e-02, -2.2201e-02],\n              [ 6.0545e-02, -2.9430e-02, -5.5483e-02],\n              [ 4.6119e-02, -6.2201e-02,  7.8676e-03]],\n    \n             [[-2.2247e-02, -1.9701e-02, -2.9673e-02],\n              [ 6.8651e-02, -3.5964e-02, -1.9974e-02],\n              [ 1.1072e-02,  2.4048e-02, -3.5415e-02]],\n    \n             [[-2.9958e-02, -1.7139e-02, -4.2087e-02],\n              [ 1.6262e-02,  2.6910e-02, -9.5637e-02],\n              [-4.6163e-02, -7.9748e-03,  5.8888e-02]],\n    \n             ...,\n    \n             [[-1.1924e-02, -3.3736e-02,  6.6722e-03],\n              [ 3.6215e-02,  1.1637e-01,  6.9653e-02],\n              [ 4.6201e-02,  8.8573e-02,  1.7321e-02]],\n    \n             [[-5.6263e-02,  7.9984e-02,  4.6702e-02],\n              [ 1.7338e-02,  5.2572e-02, -7.4043e-02],\n              [ 2.0283e-02, -3.5292e-02, -1.5342e-01]],\n    \n             [[-4.1082e-02,  8.4005e-02,  6.0189e-02],\n              [ 5.2282e-03,  7.2193e-02, -7.1259e-02],\n              [-1.5338e-02,  6.8373e-02,  3.0734e-02]]],\n    \n    \n            [[[ 5.8555e-02,  4.9840e-02, -8.1606e-03],\n              [ 5.1663e-02,  9.7127e-02,  6.0984e-02],\n              [ 1.1420e-02, -4.4598e-02, -4.5469e-02]],\n    \n             [[-2.9808e-02,  5.4549e-02, -1.9879e-02],\n              [ 5.6796e-02,  3.6126e-02,  1.5797e-02],\n              [ 3.0681e-02, -5.0942e-02, -5.3234e-02]],\n    \n             [[ 1.3922e-02,  1.3971e-01,  6.8423e-02],\n              [-8.8235e-03, -2.4182e-02, -2.3721e-02],\n              [ 5.1082e-02,  5.7232e-02,  2.6594e-02]],\n    \n             ...,\n    \n             [[ 4.7715e-02, -1.3884e-02, -7.4601e-02],\n              [ 1.5931e-01,  1.6225e-02, -1.5851e-01],\n              [-4.4029e-02,  4.3811e-02,  1.5098e-02]],\n    \n             [[ 1.9101e-02,  8.5560e-03,  2.9918e-03],\n              [-3.5806e-02, -1.2388e-02,  8.5016e-02],\n              [ 2.1537e-02, -4.2989e-02, -5.9063e-02]],\n    \n             [[ 1.0627e-01, -1.0029e-01,  7.0548e-02],\n              [ 1.3122e-01, -3.8284e-02,  4.9331e-02],\n              [ 2.0666e-02, -7.8969e-03, -7.8043e-02]]],\n    \n    \n            ...,\n    \n    \n            [[[-7.3404e-02,  1.0395e-01, -1.0564e-01],\n              [-8.6638e-02, -1.3331e-02,  5.8380e-02],\n              [ 9.6669e-02, -6.4393e-02, -2.6950e-02]],\n    \n             [[ 4.2169e-02, -3.5770e-02,  2.7937e-02],\n              [-3.6940e-02,  9.7184e-02,  7.7518e-02],\n              [-4.7211e-02, -4.9220e-02,  1.5032e-02]],\n    \n             [[-9.6998e-03, -3.4866e-03,  1.1197e-02],\n              [ 1.0703e-01, -8.9120e-02, -7.7425e-02],\n              [-5.1220e-03,  5.2214e-02,  1.0386e-03]],\n    \n             ...,\n    \n             [[ 7.3904e-02, -2.0579e-02,  5.3743e-02],\n              [ 6.4658e-02, -1.6979e-02,  4.6628e-02],\n              [-1.0712e-01,  5.3177e-02, -6.5074e-02]],\n    \n             [[-1.2869e-01, -6.8221e-02, -3.5271e-02],\n              [ 1.0368e-02,  7.7134e-02, -1.6178e-02],\n              [ 1.1905e-03,  6.1234e-02,  1.9344e-02]],\n    \n             [[ 1.6331e-01, -1.7726e-02,  2.2983e-02],\n              [ 4.9459e-02,  2.3697e-02,  2.3612e-02],\n              [ 4.8791e-02,  8.3491e-02, -1.6329e-02]]],\n    \n    \n            [[[-2.4798e-02,  5.4632e-02, -4.3357e-03],\n              [ 4.1360e-02,  8.8548e-02, -2.9969e-02],\n              [-7.1657e-03, -3.1028e-03,  1.1612e-02]],\n    \n             [[-1.0608e-01, -8.4292e-02,  8.0686e-02],\n              [-4.5012e-02, -8.9466e-02,  1.5836e-02],\n              [-4.0448e-02,  3.0319e-03,  8.5973e-03]],\n    \n             [[-4.8067e-02,  3.8327e-02,  5.4546e-03],\n              [ 4.0764e-02, -4.1688e-02, -2.0536e-02],\n              [ 2.6727e-02, -1.2958e-02,  8.8835e-04]],\n    \n             ...,\n    \n             [[ 5.6679e-02, -6.6460e-02, -1.4601e-01],\n              [-5.3729e-02,  2.1689e-02, -4.8634e-03],\n              [ 6.0694e-03, -2.2590e-02, -1.0623e-01]],\n    \n             [[-5.8056e-03, -4.3194e-02,  1.1075e-02],\n              [ 2.2644e-02, -4.1696e-02,  4.0362e-02],\n              [ 5.1533e-02,  1.4794e-02,  9.1213e-02]],\n    \n             [[ 1.6283e-02,  1.5179e-01,  5.1099e-02],\n              [-7.9721e-02, -5.9985e-03, -1.4712e-01],\n              [-2.7363e-02,  3.0541e-03, -7.4720e-02]]],\n    \n    \n            [[[-3.0849e-02,  3.1053e-02, -9.9097e-02],\n              [-1.5681e-02,  1.9421e-02,  5.4210e-02],\n              [ 6.1172e-02,  1.2152e-03,  5.6097e-02]],\n    \n             [[ 2.0473e-02,  4.3987e-02,  5.2103e-02],\n              [ 4.4138e-03,  1.0351e-01,  3.6172e-02],\n              [ 9.5577e-02,  7.5368e-02,  2.3372e-02]],\n    \n             [[-2.6699e-03, -7.6463e-02,  1.2800e-02],\n              [-2.6513e-02,  3.2613e-02, -5.5420e-02],\n              [-1.5776e-02, -1.1828e-01,  4.0880e-02]],\n    \n             ...,\n    \n             [[-3.8276e-03, -1.3912e-02,  9.3381e-04],\n              [ 7.4328e-02,  3.0974e-02, -3.8526e-02],\n              [ 2.6967e-02, -1.9196e-02,  2.0900e-02]],\n    \n             [[ 3.0646e-02, -4.3295e-02, -2.2476e-02],\n              [-5.2158e-02,  2.2017e-02, -1.7852e-02],\n              [-2.3442e-02, -2.8545e-02, -1.1269e-04]],\n    \n             [[-7.6638e-02, -9.0088e-02, -9.8108e-02],\n              [-7.2121e-02, -4.8652e-02,  4.3184e-02],\n              [ 7.3495e-02,  3.4100e-02, -4.3698e-02]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n           requires_grad=True),\n    Parameter containing:\n    tensor([[[[ 4.0709e-02, -6.2678e-02,  5.2150e-02],\n              [-3.4498e-02, -2.0331e-02,  8.8591e-03],\n              [-3.3158e-02, -5.2324e-02, -8.5716e-02]],\n    \n             [[ 5.6935e-02, -4.2908e-02,  3.2072e-02],\n              [ 9.4390e-02,  6.4215e-02, -2.0484e-02],\n              [-4.3170e-02,  3.6924e-02, -1.9030e-02]],\n    \n             [[-2.3678e-02,  2.2585e-03, -9.4823e-02],\n              [-9.2126e-03,  5.4281e-02, -3.7122e-02],\n              [ 2.2675e-02,  3.3190e-03, -9.1563e-03]],\n    \n             ...,\n    \n             [[ 3.3055e-03,  6.3868e-03,  3.2608e-02],\n              [-5.0498e-02, -1.7660e-02,  1.6688e-02],\n              [-8.9370e-03,  1.8002e-03, -1.6350e-02]],\n    \n             [[-3.3263e-03, -4.2373e-03,  5.4166e-02],\n              [-1.1797e-02,  2.9909e-03, -3.1301e-02],\n              [-2.6974e-02,  8.5289e-04,  5.7301e-02]],\n    \n             [[ 3.8594e-02, -4.0294e-02,  1.1868e-02],\n              [-2.6001e-02,  3.9564e-02, -7.7106e-02],\n              [ 1.8437e-02, -1.6760e-02,  6.0273e-02]]],\n    \n    \n            [[[ 1.1575e-02, -3.8338e-02, -2.1302e-02],\n              [ 6.0687e-02, -2.2629e-02, -1.7661e-02],\n              [-2.7905e-03, -6.4348e-03, -2.1730e-02]],\n    \n             [[-7.5024e-03,  3.8549e-02, -4.7755e-02],\n              [ 5.1501e-02,  1.0550e-02,  7.9881e-03],\n              [-5.8324e-02, -7.1682e-02, -1.2610e-04]],\n    \n             [[-5.7628e-02, -4.8407e-03, -2.4064e-02],\n              [-2.8970e-02, -9.0138e-04, -2.1083e-02],\n              [-2.7304e-02, -3.8489e-02,  1.3306e-02]],\n    \n             ...,\n    \n             [[ 1.5826e-03,  3.7835e-02, -2.1143e-02],\n              [-2.9350e-02, -8.0042e-02, -2.8358e-02],\n              [-3.9254e-02, -1.1300e-02,  4.0144e-02]],\n    \n             [[ 2.6206e-02, -2.7688e-02, -2.9583e-03],\n              [-1.2960e-02,  1.8375e-02,  9.0927e-04],\n              [ 1.5551e-02, -1.5242e-01,  9.6321e-03]],\n    \n             [[-2.9378e-02, -4.5752e-02, -5.4963e-03],\n              [ 1.7068e-02,  1.2849e-02, -3.2465e-02],\n              [ 4.5886e-02,  4.6032e-02, -5.3898e-02]]],\n    \n    \n            [[[ 2.1513e-02,  3.4143e-02,  1.8607e-02],\n              [ 5.3731e-03, -4.1503e-03,  4.3466e-02],\n              [ 3.3112e-02, -4.6641e-02, -4.5731e-02]],\n    \n             [[-6.9144e-02, -3.5617e-04,  4.7752e-02],\n              [ 1.9147e-02, -6.3748e-02, -1.8839e-03],\n              [-3.1623e-02, -9.1459e-03,  1.3552e-02]],\n    \n             [[ 3.4078e-02,  9.3781e-02,  4.6667e-02],\n              [ 2.5926e-02, -8.5965e-03,  1.1945e-02],\n              [-6.4790e-03, -8.6322e-02, -4.6849e-03]],\n    \n             ...,\n    \n             [[ 2.2752e-02,  2.7890e-02,  1.2382e-02],\n              [-4.2203e-02,  2.1488e-02, -2.0504e-02],\n              [ 3.3248e-02,  6.8575e-02, -4.2248e-02]],\n    \n             [[ 4.1048e-02,  4.0187e-02, -4.7380e-03],\n              [ 4.2552e-02,  7.2782e-02,  1.3840e-02],\n              [-8.4229e-02,  6.4733e-02,  6.3957e-03]],\n    \n             [[-1.8521e-02, -2.8295e-02, -1.9612e-02],\n              [ 8.0802e-02, -8.1567e-02,  2.1731e-02],\n              [-5.9611e-03, -2.4883e-02,  2.2972e-02]]],\n    \n    \n            ...,\n    \n    \n            [[[ 2.6924e-02,  7.0502e-02,  3.3779e-02],\n              [-2.7416e-02, -2.2192e-02,  3.0675e-02],\n              [ 7.2928e-02,  4.8921e-03,  1.7101e-02]],\n    \n             [[-9.3361e-04, -5.3008e-02,  4.2736e-02],\n              [-5.5568e-02,  4.2622e-02,  3.0436e-03],\n              [-2.8251e-03,  4.4316e-02,  1.0210e-02]],\n    \n             [[-2.9905e-02,  1.1630e-02, -3.0718e-02],\n              [ 5.8457e-02,  3.3939e-02, -5.5900e-02],\n              [-2.9532e-02,  4.0931e-02,  1.9541e-02]],\n    \n             ...,\n    \n             [[ 3.0120e-02, -6.9849e-02,  4.9213e-02],\n              [ 5.0998e-02, -5.1639e-02,  3.5889e-03],\n              [-1.0530e-02, -1.1613e-02, -4.8905e-02]],\n    \n             [[ 2.2312e-02, -7.7891e-03, -4.1356e-02],\n              [-3.5505e-02, -7.4658e-02, -1.2298e-02],\n              [-9.6567e-02,  7.5407e-02, -4.7295e-02]],\n    \n             [[ 3.0573e-02,  8.6520e-02, -1.6655e-02],\n              [ 5.5007e-02,  3.5274e-02, -3.0438e-02],\n              [ 1.9736e-02, -9.2764e-03,  6.2362e-02]]],\n    \n    \n            [[[-3.1866e-02,  8.3491e-03,  2.5784e-02],\n              [-1.8311e-02, -8.6786e-02, -2.7448e-02],\n              [ 5.4313e-02,  7.0033e-02, -3.4277e-02]],\n    \n             [[ 1.9879e-02,  2.5569e-03, -9.7566e-02],\n              [ 5.9236e-02,  2.5567e-03,  5.2346e-02],\n              [ 3.5483e-02, -1.9786e-02, -5.3448e-02]],\n    \n             [[ 6.5636e-02, -1.4248e-02,  3.7166e-02],\n              [ 2.0957e-02, -1.0031e-01,  3.6480e-02],\n              [ 2.5658e-02,  4.4141e-02,  3.8043e-02]],\n    \n             ...,\n    \n             [[-3.6866e-03,  6.1575e-03, -4.4138e-02],\n              [ 5.0180e-02, -8.2152e-03, -5.9231e-02],\n              [ 1.0478e-01, -3.0379e-02, -1.6644e-03]],\n    \n             [[-1.5936e-03,  8.0126e-02, -6.9477e-04],\n              [ 1.3205e-03, -1.7071e-02, -3.5630e-02],\n              [-5.8570e-02, -4.6741e-03, -2.2651e-02]],\n    \n             [[-1.1194e-02, -5.3266e-03, -3.4789e-02],\n              [-3.5533e-02,  4.1307e-02, -1.0617e-01],\n              [ 6.3124e-03,  6.8195e-03,  4.1212e-02]]],\n    \n    \n            [[[ 5.5647e-02, -6.3328e-02, -3.4944e-03],\n              [ 2.5386e-03,  1.8028e-02, -6.7057e-02],\n              [ 7.5539e-02, -5.1301e-02,  7.8864e-03]],\n    \n             [[ 8.3624e-02, -6.3891e-03, -9.0289e-03],\n              [ 1.3875e-02, -2.4674e-02,  4.0498e-02],\n              [-1.5384e-02, -5.6525e-02, -4.5109e-03]],\n    \n             [[ 4.9060e-03,  1.7072e-02,  5.2063e-02],\n              [-5.7485e-02,  2.2219e-02,  7.0763e-02],\n              [-5.0291e-02,  3.6860e-02, -1.8068e-02]],\n    \n             ...,\n    \n             [[-2.6310e-02,  6.3276e-02, -1.0305e-02],\n              [-8.2297e-02, -6.8374e-02, -1.6467e-02],\n              [ 4.1887e-02,  4.1115e-02, -2.5149e-02]],\n    \n             [[-3.6856e-02, -4.9408e-02, -7.8911e-02],\n              [-1.7292e-02,  2.7492e-02,  1.8111e-02],\n              [-6.2022e-03,  1.3993e-02,  5.2860e-02]],\n    \n             [[ 3.6874e-02,  1.2024e-02, -7.4141e-02],\n              [-4.1032e-02, -5.7596e-02,  1.2237e-03],\n              [ 3.9359e-02, -1.1682e-02,  2.7273e-02]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n    Parameter containing:\n    tensor([[[[-5.5132e-02,  6.1581e-03,  6.2155e-02],\n              [-3.9588e-02,  2.1351e-02, -8.5307e-03],\n              [ 3.3137e-03, -4.7785e-02,  2.2671e-02]],\n    \n             [[ 1.0252e-02,  4.9284e-04,  4.9006e-02],\n              [ 3.9250e-02, -1.9205e-02,  4.0689e-02],\n              [ 3.3908e-02, -5.5157e-03,  1.1382e-04]],\n    \n             [[-8.0724e-02, -6.6952e-02,  1.1967e-01],\n              [ 1.5912e-02, -4.0614e-02, -3.6660e-04],\n              [-3.0199e-03, -7.3329e-02, -1.8926e-02]],\n    \n             ...,\n    \n             [[ 1.4091e-02,  5.3040e-02, -5.1714e-02],\n              [ 5.2940e-02,  1.0801e-02, -8.0268e-02],\n              [-3.0345e-03, -3.0500e-02,  3.0718e-03]],\n    \n             [[-6.8103e-02, -1.6239e-02, -4.0471e-02],\n              [-2.4963e-03, -4.4566e-02,  3.3209e-03],\n              [-1.4027e-02, -2.0748e-02,  5.2107e-02]],\n    \n             [[ 1.5229e-02,  4.2778e-02, -6.8885e-02],\n              [ 4.1060e-02,  7.9826e-03,  4.0354e-02],\n              [-2.7851e-02,  1.4252e-02,  4.9374e-02]]],\n    \n    \n            [[[ 5.8551e-02, -1.4222e-02,  2.3086e-02],\n              [-3.3897e-02,  2.3701e-02,  1.6385e-02],\n              [-1.3154e-02,  2.3847e-02,  3.2779e-03]],\n    \n             [[ 1.1425e-02,  1.7411e-02,  3.0053e-03],\n              [ 6.3932e-02,  2.1539e-03,  1.9246e-02],\n              [ 1.3903e-02,  1.1038e-02,  3.0293e-02]],\n    \n             [[-7.8046e-02, -2.4012e-02, -1.6148e-03],\n              [-5.9441e-02, -7.9619e-02,  1.7563e-02],\n              [-1.6352e-02,  2.6260e-02, -1.6353e-02]],\n    \n             ...,\n    \n             [[ 3.2416e-02,  3.0322e-02, -4.3227e-02],\n              [-7.1312e-04, -7.0147e-02, -2.8298e-02],\n              [ 1.3697e-02, -3.2669e-02, -1.9760e-02]],\n    \n             [[ 3.0338e-02, -4.4869e-02, -4.3644e-02],\n              [ 3.7513e-02, -5.0508e-02,  2.9708e-02],\n              [ 1.0861e-02,  1.4295e-02, -5.0877e-02]],\n    \n             [[ 1.7638e-02,  3.5764e-02, -2.9033e-02],\n              [ 5.8480e-02,  1.7246e-03, -3.1016e-02],\n              [-1.2116e-02,  4.0844e-03,  2.5637e-02]]],\n    \n    \n            [[[ 7.1538e-02, -5.9762e-02,  6.4303e-02],\n              [ 3.7188e-02,  8.1027e-02, -6.0617e-03],\n              [ 7.8999e-03, -3.9217e-02,  6.4078e-02]],\n    \n             [[ 4.5048e-02,  4.9938e-02, -1.1003e-02],\n              [-1.5555e-02,  2.5820e-02, -5.3349e-02],\n              [-1.4551e-02, -1.7487e-02, -3.1760e-02]],\n    \n             [[-1.3106e-02, -4.3381e-02,  7.2223e-02],\n              [-4.3040e-03,  1.5294e-02, -1.4622e-02],\n              [-1.3212e-03,  5.3914e-02, -7.0336e-02]],\n    \n             ...,\n    \n             [[-3.6397e-02, -3.0814e-02,  3.8736e-02],\n              [ 1.3169e-02,  6.9540e-02,  6.1916e-02],\n              [ 8.3321e-02, -1.4446e-02,  3.3511e-02]],\n    \n             [[-4.1125e-02,  2.9394e-02,  2.2708e-02],\n              [-1.5860e-02, -1.2410e-02,  6.8116e-02],\n              [-1.5952e-02,  3.1628e-02,  1.7851e-02]],\n    \n             [[-5.7896e-03, -2.8078e-02,  3.6424e-02],\n              [ 7.9604e-03,  1.8035e-02,  2.8705e-02],\n              [-2.1666e-02, -4.6044e-02, -1.3612e-02]]],\n    \n    \n            ...,\n    \n    \n            [[[-2.5350e-02, -8.0007e-02,  2.3251e-02],\n              [-2.9572e-02, -1.5612e-02, -3.0355e-02],\n              [ 7.7593e-03, -6.4445e-04, -7.6144e-02]],\n    \n             [[ 9.1698e-02,  2.9712e-02, -4.5819e-02],\n              [ 6.8393e-02, -5.3000e-02, -1.7357e-03],\n              [ 3.6118e-03,  2.3138e-02,  4.8860e-02]],\n    \n             [[-3.0788e-02,  3.3899e-02,  5.6864e-02],\n              [ 5.9842e-02,  1.1341e-02,  2.7271e-02],\n              [ 7.5435e-02,  3.0943e-03,  8.6027e-02]],\n    \n             ...,\n    \n             [[-6.4952e-04,  1.3534e-02, -5.2719e-02],\n              [ 3.8420e-03, -1.3917e-03,  4.8201e-02],\n              [ 1.4682e-02,  6.2768e-02, -4.3282e-03]],\n    \n             [[ 5.5483e-02,  2.7869e-02, -4.4269e-03],\n              [-1.3244e-02, -1.9095e-02, -7.3391e-02],\n              [-1.4976e-02, -6.6795e-02,  7.1049e-02]],\n    \n             [[-1.1357e-01, -1.6638e-02, -2.5256e-02],\n              [-6.9313e-02, -1.4186e-02, -5.6660e-02],\n              [-1.6836e-02, -2.0438e-02, -4.6411e-02]]],\n    \n    \n            [[[ 2.0160e-03, -4.3484e-03,  5.5491e-03],\n              [-5.1170e-02,  3.5762e-03, -4.0828e-02],\n              [-3.6502e-02, -2.9368e-03, -4.0809e-03]],\n    \n             [[ 8.7791e-03,  3.5724e-04, -4.3560e-02],\n              [-1.3235e-02, -1.5638e-02,  1.3841e-02],\n              [ 3.4437e-02,  6.0284e-02,  3.1675e-02]],\n    \n             [[ 6.5982e-02,  1.6979e-02, -1.5486e-02],\n              [-3.4057e-02,  6.1261e-03, -2.1180e-02],\n              [-2.7400e-03,  9.2941e-03, -1.3075e-02]],\n    \n             ...,\n    \n             [[ 3.3516e-02, -2.8972e-02,  1.8746e-02],\n              [-1.1273e-01, -5.8152e-02,  2.2279e-02],\n              [-1.4907e-02,  3.8506e-02,  5.8382e-02]],\n    \n             [[ 1.5498e-02,  1.3391e-03, -4.0825e-02],\n              [ 5.4882e-02, -4.4000e-03,  3.4205e-02],\n              [ 6.2278e-02,  6.7408e-03,  3.6943e-03]],\n    \n             [[ 2.6520e-02, -1.0136e-02, -1.7161e-02],\n              [-2.4361e-02, -1.6154e-02, -1.2498e-02],\n              [ 3.2515e-02,  1.9940e-02, -6.8158e-02]]],\n    \n    \n            [[[ 1.9788e-02, -1.9544e-02,  1.6722e-02],\n              [-6.9213e-03,  9.9418e-03, -5.5190e-02],\n              [-6.7213e-03, -1.8039e-02,  1.5267e-02]],\n    \n             [[ 3.4658e-03,  1.2383e-02,  2.2611e-02],\n              [-1.1115e-02,  1.5204e-02, -4.0966e-02],\n              [ 1.6756e-02, -9.9951e-02, -2.4163e-02]],\n    \n             [[-2.2572e-02,  5.0634e-03, -5.8334e-04],\n              [-9.2095e-03, -3.0775e-02,  9.0906e-03],\n              [-6.5781e-02,  5.1061e-02,  4.1510e-02]],\n    \n             ...,\n    \n             [[ 2.8715e-03, -2.6396e-02,  3.9264e-02],\n              [-8.7225e-02, -5.6797e-02,  8.4563e-03],\n              [-2.2378e-02, -9.9180e-03, -2.0158e-02]],\n    \n             [[ 2.4298e-02,  5.1600e-02, -4.1342e-02],\n              [ 8.8668e-04,  6.4274e-02,  7.4735e-02],\n              [-7.7784e-02, -2.9163e-02, -2.0755e-02]],\n    \n             [[ 3.4149e-02,  5.1939e-03,  3.9159e-02],\n              [ 8.7105e-03,  8.7687e-02, -2.1579e-03],\n              [ 3.7153e-03, -4.2947e-02,  1.0657e-02]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n    Parameter containing:\n    tensor([[[[-0.1546]],\n    \n             [[-0.3110]],\n    \n             [[-0.0673]],\n    \n             ...,\n    \n             [[-0.0580]],\n    \n             [[-0.0044]],\n    \n             [[-0.2127]]],\n    \n    \n            [[[-0.1002]],\n    \n             [[-0.0839]],\n    \n             [[ 0.0432]],\n    \n             ...,\n    \n             [[ 0.2273]],\n    \n             [[ 0.1345]],\n    \n             [[-0.2129]]],\n    \n    \n            [[[ 0.0309]],\n    \n             [[ 0.1934]],\n    \n             [[ 0.1573]],\n    \n             ...,\n    \n             [[-0.1719]],\n    \n             [[ 0.0115]],\n    \n             [[-0.0970]]],\n    \n    \n            ...,\n    \n    \n            [[[-0.1925]],\n    \n             [[-0.0344]],\n    \n             [[ 0.0473]],\n    \n             ...,\n    \n             [[-0.0204]],\n    \n             [[ 0.0989]],\n    \n             [[ 0.2493]]],\n    \n    \n            [[[-0.0347]],\n    \n             [[-0.0518]],\n    \n             [[ 0.0708]],\n    \n             ...,\n    \n             [[ 0.1118]],\n    \n             [[-0.0680]],\n    \n             [[ 0.1156]]],\n    \n    \n            [[[-0.0803]],\n    \n             [[-0.1228]],\n    \n             [[ 0.0354]],\n    \n             ...,\n    \n             [[ 0.0310]],\n    \n             [[ 0.0990]],\n    \n             [[-0.2581]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n    Parameter containing:\n    tensor([[[[ 0.0394, -0.0474,  0.0663],\n              [ 0.0650,  0.0074,  0.0361],\n              [-0.0211,  0.0024,  0.0080]],\n    \n             [[-0.0008, -0.0415, -0.0571],\n              [ 0.0070,  0.0367, -0.0197],\n              [-0.0481, -0.0221,  0.0503]],\n    \n             [[ 0.0029,  0.0264, -0.0107],\n              [ 0.0112, -0.0081, -0.0239],\n              [ 0.0228, -0.0258, -0.0799]],\n    \n             ...,\n    \n             [[-0.0081, -0.0022,  0.1024],\n              [ 0.1018, -0.0424, -0.0113],\n              [-0.0293,  0.0175,  0.0286]],\n    \n             [[ 0.0033,  0.0593, -0.0288],\n              [ 0.0230,  0.0914,  0.0578],\n              [-0.0083, -0.0224, -0.0048]],\n    \n             [[ 0.0206,  0.0637, -0.0416],\n              [-0.0343, -0.0241, -0.0122],\n              [-0.0191, -0.0459,  0.0374]]],\n    \n    \n            [[[ 0.0396,  0.0130, -0.0366],\n              [ 0.0864,  0.0288,  0.0017],\n              [-0.0183,  0.0095,  0.0068]],\n    \n             [[-0.0512,  0.0184,  0.0063],\n              [-0.0641, -0.0275, -0.0352],\n              [-0.0929, -0.0079, -0.0055]],\n    \n             [[-0.0120,  0.0262, -0.0928],\n              [ 0.0402, -0.0843,  0.0520],\n              [-0.0498, -0.0071,  0.0176]],\n    \n             ...,\n    \n             [[-0.0528, -0.0544,  0.0047],\n              [ 0.0691,  0.0725,  0.0821],\n              [-0.0216,  0.0192,  0.0115]],\n    \n             [[-0.0190,  0.0042,  0.0060],\n              [-0.0050, -0.0128,  0.0103],\n              [ 0.0003, -0.0117,  0.0361]],\n    \n             [[-0.0264,  0.0045, -0.0138],\n              [ 0.0196,  0.0088, -0.0140],\n              [-0.0179, -0.0327, -0.0393]]],\n    \n    \n            [[[ 0.0622,  0.0523,  0.0172],\n              [ 0.0332,  0.0088,  0.0065],\n              [-0.0505, -0.0657, -0.0513]],\n    \n             [[-0.0075,  0.0413, -0.0826],\n              [-0.0916, -0.0244, -0.0127],\n              [ 0.0192, -0.0075,  0.0614]],\n    \n             [[ 0.0446, -0.0057, -0.0179],\n              [-0.0424, -0.0085,  0.0281],\n              [ 0.0458,  0.0291,  0.0577]],\n    \n             ...,\n    \n             [[-0.0156,  0.0609, -0.0410],\n              [-0.0492, -0.0350,  0.0209],\n              [ 0.0622, -0.0449,  0.0889]],\n    \n             [[ 0.0056, -0.1113,  0.0634],\n              [-0.0184,  0.0454,  0.0228],\n              [-0.0220, -0.0301,  0.0266]],\n    \n             [[-0.0135,  0.0839,  0.0310],\n              [-0.0103,  0.0347,  0.0458],\n              [ 0.0220,  0.0154,  0.0267]]],\n    \n    \n            ...,\n    \n    \n            [[[-0.0078, -0.0462, -0.0161],\n              [ 0.0021, -0.0045,  0.0173],\n              [ 0.0205, -0.0118, -0.0095]],\n    \n             [[ 0.0239, -0.0027,  0.0639],\n              [ 0.0309,  0.0249, -0.0354],\n              [-0.0221,  0.0428,  0.0789]],\n    \n             [[-0.0301, -0.0195, -0.0692],\n              [ 0.0123,  0.0827, -0.0210],\n              [ 0.0265,  0.0423, -0.0336]],\n    \n             ...,\n    \n             [[ 0.0123, -0.0537, -0.0637],\n              [-0.0269,  0.0041,  0.0180],\n              [-0.0762, -0.0158,  0.0188]],\n    \n             [[-0.0015,  0.0406,  0.1092],\n              [-0.0734, -0.0133,  0.0136],\n              [-0.0166,  0.0125,  0.0073]],\n    \n             [[ 0.0047, -0.0786,  0.0229],\n              [ 0.0108,  0.0994, -0.0166],\n              [ 0.0565, -0.0055,  0.0201]]],\n    \n    \n            [[[-0.0304,  0.0299,  0.0265],\n              [-0.0176,  0.0101, -0.0163],\n              [-0.0201,  0.0161,  0.0459]],\n    \n             [[-0.0252,  0.0097,  0.0087],\n              [-0.0273,  0.0369,  0.0596],\n              [ 0.0017, -0.0051,  0.0220]],\n    \n             [[ 0.0535, -0.0277, -0.0246],\n              [-0.0334, -0.0068, -0.0545],\n              [ 0.0393, -0.0255, -0.0689]],\n    \n             ...,\n    \n             [[ 0.0526, -0.0041, -0.0207],\n              [-0.0118,  0.0075,  0.0579],\n              [-0.0055, -0.0106,  0.0754]],\n    \n             [[-0.0497,  0.0583,  0.0003],\n              [-0.0122, -0.0290,  0.0031],\n              [-0.0085, -0.0296, -0.0462]],\n    \n             [[ 0.0903, -0.0146,  0.0528],\n              [ 0.0015, -0.0164,  0.0669],\n              [ 0.0553, -0.0252, -0.0104]]],\n    \n    \n            [[[ 0.0152,  0.0804, -0.0369],\n              [-0.0174, -0.0044,  0.0298],\n              [-0.0309,  0.0160,  0.0801]],\n    \n             [[ 0.0680,  0.0597, -0.0083],\n              [-0.0114, -0.1010, -0.0055],\n              [ 0.0914, -0.0014,  0.0082]],\n    \n             [[-0.0095, -0.0598,  0.0139],\n              [ 0.0367,  0.0074,  0.0407],\n              [-0.0875,  0.0047, -0.0072]],\n    \n             ...,\n    \n             [[-0.0934,  0.0146, -0.0033],\n              [-0.0207,  0.0877,  0.0354],\n              [ 0.0199, -0.0098,  0.0044]],\n    \n             [[ 0.0070,  0.0238,  0.0072],\n              [-0.0090, -0.0024, -0.0194],\n              [ 0.0122, -0.0180,  0.0196]],\n    \n             [[-0.0496,  0.0103,  0.1138],\n              [-0.0145, -0.0296,  0.0259],\n              [ 0.1136,  0.0065, -0.0863]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n    Parameter containing:\n    tensor([[[[-1.9795e-02,  8.6236e-02,  3.2786e-02],\n              [ 1.1399e-02, -1.1217e-02,  2.7034e-02],\n              [ 2.9200e-02,  5.9754e-03,  3.3488e-02]],\n    \n             [[-3.8470e-02,  6.6207e-02, -3.9223e-02],\n              [-3.1481e-03,  1.4094e-02, -6.3210e-03],\n              [-4.9393e-02, -2.0365e-02,  6.2733e-02]],\n    \n             [[-3.4687e-03, -8.3386e-02,  3.1917e-02],\n              [-8.7836e-03, -1.1156e-02,  2.5680e-02],\n              [-8.2232e-02,  2.0910e-02, -6.9654e-03]],\n    \n             ...,\n    \n             [[-4.3068e-02,  1.4870e-02, -7.5049e-02],\n              [-9.3978e-03,  8.1963e-02, -5.6056e-02],\n              [ 1.5879e-02, -1.9654e-02,  4.7632e-02]],\n    \n             [[-2.9575e-02, -7.4388e-02, -1.0018e-03],\n              [-1.0920e-02,  7.5963e-03, -7.6002e-02],\n              [-5.9170e-02, -3.8366e-02, -2.4581e-02]],\n    \n             [[ 8.3438e-03,  1.6373e-02,  2.1508e-02],\n              [ 9.7358e-02, -3.7875e-03, -6.4284e-02],\n              [ 1.5496e-02,  6.8555e-02,  1.3018e-02]]],\n    \n    \n            [[[ 2.6448e-02,  3.5390e-02,  4.8139e-02],\n              [ 6.9773e-02, -5.8459e-02, -7.1701e-02],\n              [-1.6162e-02, -4.0571e-02, -7.4362e-03]],\n    \n             [[ 1.8529e-02, -4.1232e-02, -8.1856e-02],\n              [ 3.8517e-03,  1.8321e-02, -7.1975e-02],\n              [-5.7924e-03,  3.0020e-02,  2.4525e-02]],\n    \n             [[ 8.2560e-03, -2.7150e-03,  3.0579e-02],\n              [ 7.1031e-04,  3.8102e-02, -2.6051e-02],\n              [ 2.5423e-02, -1.7955e-02, -7.2632e-03]],\n    \n             ...,\n    \n             [[-2.7747e-02,  1.7845e-02, -1.5167e-02],\n              [-3.7891e-02, -3.1702e-03,  6.2226e-02],\n              [ 2.4866e-02, -3.8186e-02, -4.5680e-02]],\n    \n             [[ 5.1400e-02, -5.4232e-03,  8.8603e-02],\n              [ 2.1297e-03,  2.3279e-02, -7.6200e-02],\n              [ 2.0402e-02,  1.8357e-02,  1.3507e-02]],\n    \n             [[ 2.5296e-02,  6.1032e-03, -8.4188e-02],\n              [-7.0667e-03,  7.8166e-02,  2.1456e-02],\n              [-8.8838e-02,  3.9398e-02,  1.8263e-03]]],\n    \n    \n            [[[ 1.8486e-02, -4.7515e-02, -5.6434e-02],\n              [-1.8311e-02,  1.5836e-02, -1.3853e-02],\n              [-3.1066e-03,  2.1561e-02,  4.0413e-02]],\n    \n             [[ 3.6775e-03,  1.8682e-02, -2.7053e-03],\n              [-2.0104e-02, -6.3989e-04, -1.3324e-02],\n              [-9.2280e-03,  2.0539e-02,  4.9311e-03]],\n    \n             [[ 3.4791e-02, -1.4422e-02,  1.9109e-02],\n              [ 1.8692e-02,  9.8867e-03,  5.9864e-02],\n              [-5.3631e-02,  6.9838e-02,  2.6409e-02]],\n    \n             ...,\n    \n             [[ 3.8130e-02, -1.5386e-02,  1.3021e-01],\n              [-3.3746e-02, -2.1844e-02, -1.7694e-02],\n              [ 4.2962e-03, -4.7319e-05, -3.7877e-02]],\n    \n             [[-9.3433e-03,  3.7228e-02,  3.4970e-02],\n              [-1.6278e-02,  2.3814e-02, -7.1080e-02],\n              [ 2.6227e-02,  5.2095e-02,  4.9810e-03]],\n    \n             [[-5.9262e-02,  2.6445e-02, -2.4059e-03],\n              [-4.8066e-03,  6.8652e-02,  1.1194e-02],\n              [-8.9842e-02, -2.0678e-02,  7.5407e-02]]],\n    \n    \n            ...,\n    \n    \n            [[[ 1.3175e-02, -1.2691e-01, -9.9950e-03],\n              [-5.2586e-02,  5.8082e-02, -2.9910e-03],\n              [ 5.4190e-04,  2.1139e-02, -1.8538e-02]],\n    \n             [[ 7.8839e-02,  4.2099e-02, -4.8095e-03],\n              [ 4.3354e-02, -9.0765e-02,  8.6556e-02],\n              [-1.1215e-01, -2.0935e-02,  1.9339e-02]],\n    \n             [[ 4.4893e-02,  1.0815e-01,  2.7933e-02],\n              [ 2.5537e-02, -3.5953e-02, -8.8515e-03],\n              [-2.4916e-02,  6.4778e-02,  7.8676e-03]],\n    \n             ...,\n    \n             [[ 1.5398e-02,  4.4271e-02,  2.4398e-02],\n              [ 4.0959e-03, -1.7845e-02, -4.5118e-02],\n              [-2.8088e-02,  2.1079e-03,  1.6105e-02]],\n    \n             [[-2.2099e-02, -5.6893e-02,  2.4105e-02],\n              [ 6.0339e-02,  1.9695e-02,  1.2118e-02],\n              [ 5.4377e-03,  2.2509e-02, -1.1510e-02]],\n    \n             [[-2.8304e-02, -5.4936e-04,  5.4129e-02],\n              [ 3.4024e-02, -1.9966e-02, -3.2614e-02],\n              [-2.8606e-02,  3.2002e-02,  7.0086e-03]]],\n    \n    \n            [[[-1.2700e-02,  6.3279e-02,  9.5540e-03],\n              [ 3.4803e-02,  1.0220e-01,  4.7821e-02],\n              [ 3.4248e-02, -3.4198e-02,  1.8319e-02]],\n    \n             [[ 2.0094e-02, -5.9470e-02,  2.2722e-02],\n              [ 5.2989e-02,  8.6200e-02, -3.3265e-02],\n              [-6.1941e-02, -2.9871e-04,  1.7608e-02]],\n    \n             [[-2.0909e-02,  9.5526e-03,  4.0026e-02],\n              [-5.5778e-03, -9.7418e-03, -2.9585e-03],\n              [-2.8947e-02, -1.6154e-02,  6.3269e-02]],\n    \n             ...,\n    \n             [[ 1.1558e-02,  6.6677e-03, -3.5232e-02],\n              [ 6.1731e-02,  1.1710e-02,  9.7482e-03],\n              [-1.4049e-02,  2.0468e-02, -3.8348e-03]],\n    \n             [[ 2.0385e-02,  6.4592e-02, -1.0899e-02],\n              [-1.6283e-02,  2.9707e-03,  8.8974e-03],\n              [-1.6293e-02,  1.2398e-01, -4.0602e-02]],\n    \n             [[-2.3237e-02, -6.5583e-02, -5.4150e-02],\n              [ 1.8865e-02,  5.0608e-02,  3.0147e-02],\n              [ 2.4971e-02, -7.0116e-02,  4.3032e-03]]],\n    \n    \n            [[[-1.2088e-03,  8.8236e-03,  1.2875e-02],\n              [ 1.6202e-02,  7.4681e-02,  5.3148e-03],\n              [-5.4831e-02,  6.2036e-02, -4.4218e-02]],\n    \n             [[ 8.8418e-02, -2.1241e-02,  2.0802e-02],\n              [-5.0195e-02, -8.5369e-02, -1.1943e-03],\n              [ 8.6129e-03,  2.5737e-02, -1.2388e-02]],\n    \n             [[-7.2946e-03, -3.7079e-03, -4.1658e-02],\n              [-4.2865e-03,  4.4012e-03, -3.0869e-02],\n              [-3.0159e-02, -4.6497e-03, -1.2382e-02]],\n    \n             ...,\n    \n             [[-4.9456e-02,  5.1864e-02, -6.0865e-02],\n              [-1.0842e-01, -1.9607e-02,  9.5789e-03],\n              [ 5.7146e-03,  3.9169e-03,  2.3753e-02]],\n    \n             [[ 4.4151e-02,  2.6288e-02,  3.7924e-03],\n              [ 3.7123e-02,  8.7214e-03, -1.7471e-03],\n              [ 3.0455e-03,  1.6611e-02,  1.4854e-02]],\n    \n             [[-3.3261e-02, -3.2148e-02, -3.0145e-02],\n              [ 4.8135e-04, -1.1875e-02,  1.4395e-02],\n              [-2.9808e-02,  6.0946e-02,  4.6823e-02]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n    Parameter containing:\n    tensor([[[[-0.0028,  0.0060, -0.0232],\n              [-0.0239, -0.0190,  0.0038],\n              [ 0.0109,  0.0534,  0.0311]],\n    \n             [[ 0.0087,  0.0570, -0.0093],\n              [-0.0208,  0.0617,  0.0136],\n              [ 0.0201,  0.0152, -0.0030]],\n    \n             [[ 0.0128,  0.0302,  0.0005],\n              [-0.0094,  0.0878, -0.0348],\n              [ 0.0896,  0.0253,  0.0056]],\n    \n             ...,\n    \n             [[-0.0158,  0.0059,  0.0348],\n              [-0.0085, -0.0171,  0.0274],\n              [-0.0068, -0.0035, -0.0404]],\n    \n             [[ 0.0047, -0.0113,  0.0193],\n              [ 0.0145,  0.0198, -0.0106],\n              [-0.0233,  0.0442,  0.0044]],\n    \n             [[-0.0740, -0.0448,  0.0129],\n              [-0.0025,  0.0239, -0.0491],\n              [-0.0028, -0.0118,  0.0302]]],\n    \n    \n            [[[-0.0301, -0.0200, -0.0354],\n              [ 0.0277,  0.0207, -0.0009],\n              [ 0.0359,  0.0424,  0.0328]],\n    \n             [[-0.0139, -0.0004,  0.0180],\n              [-0.0193,  0.0548, -0.0254],\n              [-0.0201,  0.0252,  0.0250]],\n    \n             [[-0.0506, -0.0348, -0.0026],\n              [ 0.0174,  0.0013, -0.0100],\n              [ 0.0209, -0.0275,  0.0333]],\n    \n             ...,\n    \n             [[-0.0228, -0.0088, -0.0069],\n              [-0.0449,  0.0025, -0.0744],\n              [-0.0015, -0.0158,  0.0396]],\n    \n             [[ 0.0144,  0.0328, -0.0566],\n              [-0.0264,  0.0259,  0.0097],\n              [ 0.0545,  0.0332,  0.0792]],\n    \n             [[ 0.0362,  0.0160, -0.0301],\n              [ 0.0238, -0.0114, -0.0063],\n              [-0.0537, -0.0094,  0.0245]]],\n    \n    \n            [[[ 0.0178,  0.0259,  0.0396],\n              [ 0.0001,  0.0247, -0.0187],\n              [ 0.0215, -0.0423,  0.0207]],\n    \n             [[ 0.0191, -0.0127,  0.0009],\n              [-0.0009,  0.0258, -0.0358],\n              [-0.0457,  0.0018,  0.0002]],\n    \n             [[-0.0204,  0.0451, -0.0498],\n              [ 0.0107, -0.0178,  0.0245],\n              [ 0.0452, -0.0070,  0.0070]],\n    \n             ...,\n    \n             [[-0.0280,  0.0179,  0.0383],\n              [-0.0025, -0.0236,  0.0334],\n              [ 0.0232, -0.0652, -0.0040]],\n    \n             [[ 0.0214,  0.0680, -0.0482],\n              [-0.0324, -0.0339, -0.0253],\n              [-0.0285, -0.0285,  0.0117]],\n    \n             [[-0.0198,  0.0074,  0.0546],\n              [-0.0171,  0.0107,  0.0064],\n              [-0.0350,  0.0070, -0.0131]]],\n    \n    \n            ...,\n    \n    \n            [[[ 0.0451,  0.0395, -0.0167],\n              [-0.0109, -0.0255, -0.0060],\n              [-0.0231,  0.0389,  0.0189]],\n    \n             [[-0.0402, -0.0148,  0.0181],\n              [ 0.0241,  0.0273,  0.0259],\n              [ 0.0170, -0.0024,  0.0067]],\n    \n             [[ 0.0330,  0.0178, -0.0200],\n              [-0.0430, -0.0370, -0.0006],\n              [ 0.0192, -0.0568,  0.0308]],\n    \n             ...,\n    \n             [[-0.0011,  0.0583,  0.0154],\n              [ 0.0003, -0.0188, -0.0132],\n              [ 0.0661,  0.0373,  0.0036]],\n    \n             [[-0.0429,  0.0263, -0.0327],\n              [-0.0263, -0.0179, -0.0099],\n              [ 0.0230, -0.0244, -0.0165]],\n    \n             [[-0.0533, -0.0090, -0.0189],\n              [-0.0421, -0.0572, -0.0514],\n              [-0.0209, -0.0635, -0.0092]]],\n    \n    \n            [[[-0.0188, -0.0375,  0.0237],\n              [-0.0166, -0.0028,  0.0037],\n              [-0.0284,  0.0161, -0.0111]],\n    \n             [[-0.0503,  0.0176,  0.0385],\n              [ 0.0209, -0.0058, -0.0050],\n              [-0.0346, -0.0229,  0.0176]],\n    \n             [[-0.0410, -0.0146,  0.0219],\n              [ 0.0359, -0.0106,  0.0058],\n              [ 0.0131,  0.0557,  0.0034]],\n    \n             ...,\n    \n             [[-0.0201, -0.0043, -0.0235],\n              [-0.0110, -0.0119, -0.0502],\n              [ 0.0416, -0.0090, -0.0463]],\n    \n             [[-0.0209, -0.0396, -0.0016],\n              [-0.0094, -0.0250, -0.0106],\n              [-0.0181, -0.0169, -0.0470]],\n    \n             [[-0.0170, -0.0068, -0.0030],\n              [ 0.0499,  0.0138, -0.0491],\n              [ 0.0005,  0.0009, -0.0011]]],\n    \n    \n            [[[-0.0028,  0.0051,  0.0561],\n              [-0.0055, -0.0074, -0.0321],\n              [-0.0210,  0.0173,  0.0032]],\n    \n             [[ 0.0241, -0.0179,  0.0103],\n              [ 0.0141,  0.0081,  0.0201],\n              [-0.0114, -0.0129,  0.0313]],\n    \n             [[ 0.0011, -0.0168,  0.0154],\n              [-0.0078,  0.0179, -0.0172],\n              [ 0.0118, -0.0278, -0.0224]],\n    \n             ...,\n    \n             [[-0.0235,  0.0006, -0.0082],\n              [-0.0014,  0.0344,  0.0293],\n              [ 0.0253, -0.0312,  0.0003]],\n    \n             [[ 0.0598,  0.0073,  0.0187],\n              [-0.0556,  0.0285,  0.0008],\n              [-0.0105, -0.0152,  0.0092]],\n    \n             [[ 0.0338,  0.0358,  0.0176],\n              [ 0.0008, -0.0494, -0.0079],\n              [ 0.0499,  0.0213,  0.0127]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n           requires_grad=True),\n    Parameter containing:\n    tensor([[[[ 0.0079, -0.0006,  0.0364],\n              [ 0.0646, -0.0153,  0.0199],\n              [ 0.0062,  0.0135,  0.0032]],\n    \n             [[-0.0162,  0.0253,  0.0128],\n              [ 0.0005, -0.0195, -0.0464],\n              [ 0.0205, -0.0135, -0.0115]],\n    \n             [[ 0.0301,  0.0072, -0.0471],\n              [ 0.0123, -0.0074, -0.0123],\n              [-0.0191, -0.0692, -0.0161]],\n    \n             ...,\n    \n             [[-0.0226, -0.0393,  0.0142],\n              [-0.0025,  0.0293, -0.0376],\n              [-0.0028, -0.0151,  0.0133]],\n    \n             [[-0.0199,  0.0235,  0.0144],\n              [-0.0218,  0.0275,  0.0037],\n              [ 0.0088,  0.0329,  0.0470]],\n    \n             [[-0.0154,  0.0176,  0.0258],\n              [ 0.0098,  0.0201,  0.0219],\n              [-0.0297, -0.0072, -0.0092]]],\n    \n    \n            [[[ 0.0317,  0.0264,  0.0372],\n              [-0.0035, -0.0228, -0.0208],\n              [ 0.0200,  0.0099,  0.0134]],\n    \n             [[-0.0329, -0.0230,  0.0201],\n              [ 0.0320,  0.0366, -0.0102],\n              [-0.0213, -0.0054, -0.0098]],\n    \n             [[ 0.0566,  0.0187, -0.0168],\n              [-0.0004, -0.0015,  0.0541],\n              [-0.0419, -0.0291, -0.0251]],\n    \n             ...,\n    \n             [[-0.0362, -0.0080,  0.0337],\n              [-0.0103,  0.0216,  0.0261],\n              [-0.0459, -0.0381, -0.0420]],\n    \n             [[ 0.0220,  0.0065, -0.0280],\n              [ 0.0387,  0.0035, -0.0158],\n              [ 0.0025,  0.0236,  0.0441]],\n    \n             [[ 0.0105,  0.0305, -0.0357],\n              [-0.0070, -0.0077, -0.0240],\n              [ 0.0225, -0.0451,  0.0586]]],\n    \n    \n            [[[-0.0132, -0.0206, -0.0199],\n              [-0.0052,  0.0740, -0.0067],\n              [ 0.0197,  0.0073, -0.0054]],\n    \n             [[-0.0297,  0.0668,  0.0005],\n              [ 0.0058,  0.0272, -0.0102],\n              [ 0.0044, -0.0012,  0.0319]],\n    \n             [[-0.0433, -0.0261,  0.0103],\n              [-0.0423,  0.0100, -0.0035],\n              [ 0.0160,  0.0282,  0.0115]],\n    \n             ...,\n    \n             [[ 0.0076,  0.0471,  0.0234],\n              [-0.0137,  0.0161,  0.0461],\n              [ 0.0431,  0.0100, -0.0452]],\n    \n             [[-0.0331,  0.0031,  0.0267],\n              [-0.0485, -0.0210, -0.0284],\n              [ 0.0172, -0.0218, -0.0333]],\n    \n             [[ 0.0331, -0.0060, -0.0327],\n              [-0.0044, -0.0462,  0.0185],\n              [ 0.0460, -0.0382, -0.0199]]],\n    \n    \n            ...,\n    \n    \n            [[[-0.0234, -0.0357,  0.0005],\n              [ 0.0167, -0.0287, -0.0270],\n              [-0.0444, -0.0288,  0.0121]],\n    \n             [[-0.0254, -0.0248, -0.0266],\n              [ 0.0125, -0.0205, -0.0070],\n              [ 0.0401, -0.0157, -0.0397]],\n    \n             [[ 0.0062, -0.0220, -0.0095],\n              [-0.0269, -0.0627, -0.0013],\n              [-0.0086,  0.0081,  0.0192]],\n    \n             ...,\n    \n             [[-0.0140, -0.0051,  0.0249],\n              [-0.0349, -0.0212, -0.0034],\n              [ 0.0062,  0.0003, -0.0216]],\n    \n             [[-0.0305, -0.0146, -0.0164],\n              [ 0.0512, -0.0511,  0.0379],\n              [ 0.0182, -0.0265,  0.0054]],\n    \n             [[-0.0104, -0.0316, -0.0101],\n              [-0.0293,  0.0414,  0.0064],\n              [ 0.0025, -0.0040, -0.0109]]],\n    \n    \n            [[[-0.0201,  0.0471, -0.0193],\n              [ 0.0192,  0.0330,  0.0140],\n              [ 0.0094,  0.0472, -0.0131]],\n    \n             [[ 0.0365,  0.0199,  0.0268],\n              [-0.0362, -0.0047, -0.0158],\n              [ 0.0674, -0.0362,  0.0045]],\n    \n             [[-0.0023, -0.0183,  0.0057],\n              [ 0.0366,  0.0103,  0.0330],\n              [ 0.0322,  0.0261, -0.0286]],\n    \n             ...,\n    \n             [[ 0.0106,  0.0021,  0.0065],\n              [ 0.0187,  0.0136, -0.0025],\n              [-0.0207,  0.0254,  0.0068]],\n    \n             [[-0.0200,  0.0072,  0.0187],\n              [-0.0193, -0.0237,  0.0313],\n              [-0.0026, -0.0154,  0.0369]],\n    \n             [[ 0.0314, -0.0216, -0.0217],\n              [-0.0087, -0.0237, -0.0275],\n              [ 0.0330, -0.0243, -0.0042]]],\n    \n    \n            [[[ 0.0275,  0.0171,  0.0085],\n              [ 0.0612,  0.0083, -0.0032],\n              [ 0.0108,  0.0221, -0.0179]],\n    \n             [[-0.0233,  0.0754,  0.0079],\n              [ 0.0779, -0.0218,  0.0130],\n              [ 0.0072, -0.0229,  0.0452]],\n    \n             [[ 0.0314,  0.0438,  0.0037],\n              [ 0.0257,  0.0066, -0.0018],\n              [ 0.0334, -0.0341,  0.0112]],\n    \n             ...,\n    \n             [[-0.0111,  0.0289,  0.0069],\n              [-0.0181, -0.0128,  0.0151],\n              [-0.0087,  0.0389,  0.0231]],\n    \n             [[-0.0447, -0.0067,  0.0096],\n              [ 0.0230,  0.0040,  0.0060],\n              [-0.0062, -0.0294,  0.0324]],\n    \n             [[ 0.0112, -0.0007,  0.0332],\n              [-0.0049, -0.0448,  0.0381],\n              [ 0.0505,  0.0369,  0.0400]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n           requires_grad=True),\n    Parameter containing:\n    tensor([[[[-0.0726]],\n    \n             [[ 0.1562]],\n    \n             [[ 0.0774]],\n    \n             ...,\n    \n             [[ 0.1005]],\n    \n             [[-0.1083]],\n    \n             [[ 0.1209]]],\n    \n    \n            [[[ 0.0224]],\n    \n             [[ 0.0046]],\n    \n             [[ 0.1064]],\n    \n             ...,\n    \n             [[-0.2038]],\n    \n             [[ 0.0061]],\n    \n             [[-0.1298]]],\n    \n    \n            [[[-0.1401]],\n    \n             [[-0.0070]],\n    \n             [[-0.0666]],\n    \n             ...,\n    \n             [[-0.0920]],\n    \n             [[ 0.2372]],\n    \n             [[-0.1237]]],\n    \n    \n            ...,\n    \n    \n            [[[-0.0385]],\n    \n             [[-0.0211]],\n    \n             [[ 0.0647]],\n    \n             ...,\n    \n             [[-0.0304]],\n    \n             [[-0.0368]],\n    \n             [[ 0.0634]]],\n    \n    \n            [[[-0.0820]],\n    \n             [[ 0.0606]],\n    \n             [[ 0.0637]],\n    \n             ...,\n    \n             [[-0.0270]],\n    \n             [[-0.0221]],\n    \n             [[ 0.1032]]],\n    \n    \n            [[[-0.0377]],\n    \n             [[ 0.0214]],\n    \n             [[-0.0455]],\n    \n             ...,\n    \n             [[ 0.0621]],\n    \n             [[ 0.1157]],\n    \n             [[ 0.0675]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n           requires_grad=True),\n    Parameter containing:\n    tensor([[[[-0.0159, -0.0203,  0.0086],\n              [ 0.0260, -0.0700, -0.0057],\n              [-0.0249,  0.0014, -0.0261]],\n    \n             [[-0.0312,  0.0263,  0.0195],\n              [ 0.0095,  0.0045, -0.0427],\n              [-0.0252,  0.0411,  0.0252]],\n    \n             [[ 0.0591, -0.0343, -0.0022],\n              [-0.0113,  0.0404, -0.0131],\n              [ 0.0119,  0.0133, -0.0076]],\n    \n             ...,\n    \n             [[ 0.0139,  0.0217, -0.0093],\n              [ 0.0624,  0.0045,  0.0244],\n              [-0.0272, -0.0467, -0.0006]],\n    \n             [[-0.0076,  0.0249,  0.0218],\n              [-0.0119,  0.0435, -0.0154],\n              [-0.0476,  0.0009, -0.0017]],\n    \n             [[-0.0113,  0.0396, -0.0485],\n              [-0.0055,  0.0508,  0.0064],\n              [-0.0074, -0.0134, -0.0013]]],\n    \n    \n            [[[ 0.0471, -0.0218, -0.0164],\n              [-0.0010,  0.0398,  0.0306],\n              [-0.0093, -0.0180, -0.0293]],\n    \n             [[ 0.0256, -0.0202, -0.0073],\n              [-0.0020, -0.0348,  0.0230],\n              [ 0.0461, -0.0242, -0.0440]],\n    \n             [[-0.0076,  0.0393, -0.0291],\n              [ 0.0423, -0.0497, -0.0403],\n              [-0.0226, -0.0145,  0.0126]],\n    \n             ...,\n    \n             [[ 0.0179,  0.0204, -0.0351],\n              [-0.0383,  0.0152,  0.0113],\n              [-0.0205, -0.0138,  0.0193]],\n    \n             [[-0.0573, -0.0178,  0.0073],\n              [-0.0689,  0.0309,  0.0189],\n              [-0.0559,  0.0204,  0.0096]],\n    \n             [[-0.0010, -0.0110, -0.0475],\n              [ 0.0127, -0.0296, -0.0399],\n              [-0.0418,  0.0386, -0.0005]]],\n    \n    \n            [[[ 0.0339, -0.0342,  0.0065],\n              [-0.0276,  0.0280,  0.0082],\n              [-0.0336,  0.0028, -0.0102]],\n    \n             [[ 0.0127,  0.0193, -0.0215],\n              [-0.0422, -0.0154, -0.0398],\n              [-0.0185, -0.0063, -0.0231]],\n    \n             [[-0.0348, -0.0257,  0.0498],\n              [ 0.0088, -0.0146, -0.0303],\n              [ 0.0116, -0.0292, -0.0122]],\n    \n             ...,\n    \n             [[-0.0125, -0.0297, -0.0303],\n              [ 0.0267, -0.0074, -0.0350],\n              [-0.0282,  0.0049,  0.0167]],\n    \n             [[-0.0181, -0.0273, -0.0109],\n              [-0.0083,  0.0556,  0.0431],\n              [-0.0069,  0.0456,  0.0107]],\n    \n             [[ 0.0137, -0.0085, -0.0108],\n              [-0.0718, -0.0326,  0.0413],\n              [ 0.0181, -0.0401, -0.0525]]],\n    \n    \n            ...,\n    \n    \n            [[[ 0.0082,  0.0241, -0.0043],\n              [ 0.0462,  0.0333,  0.0004],\n              [ 0.0151,  0.0157,  0.0172]],\n    \n             [[-0.0530, -0.0465,  0.0006],\n              [ 0.0148, -0.0223, -0.0347],\n              [-0.0172,  0.0470,  0.0007]],\n    \n             [[ 0.0180,  0.0313, -0.0121],\n              [ 0.0332,  0.0298,  0.0031],\n              [-0.0220, -0.0220,  0.0359]],\n    \n             ...,\n    \n             [[ 0.0044, -0.0147,  0.0179],\n              [ 0.0259,  0.0084,  0.0345],\n              [ 0.0208,  0.0156,  0.0252]],\n    \n             [[ 0.0303, -0.0048, -0.0239],\n              [ 0.0251, -0.0203,  0.0055],\n              [-0.0322, -0.0436,  0.0085]],\n    \n             [[-0.0635, -0.0486, -0.0136],\n              [-0.0030,  0.0533,  0.0015],\n              [-0.0650,  0.0195, -0.0434]]],\n    \n    \n            [[[-0.0042, -0.0043, -0.0404],\n              [-0.0185,  0.0148,  0.0204],\n              [-0.0233,  0.0302,  0.0121]],\n    \n             [[ 0.0007,  0.0123,  0.0199],\n              [-0.0114, -0.0264, -0.0196],\n              [-0.0070, -0.0295,  0.0055]],\n    \n             [[ 0.0306,  0.0455,  0.0305],\n              [-0.0960,  0.0142, -0.0073],\n              [-0.0177, -0.0467, -0.0288]],\n    \n             ...,\n    \n             [[ 0.0301, -0.0091,  0.0157],\n              [-0.0051,  0.0170, -0.0208],\n              [-0.0133, -0.0515, -0.0647]],\n    \n             [[-0.0215,  0.0084, -0.0202],\n              [-0.0230,  0.0370, -0.0545],\n              [ 0.0293, -0.0170,  0.0054]],\n    \n             [[ 0.0317, -0.0180,  0.0113],\n              [ 0.0062, -0.0421,  0.0025],\n              [ 0.0076, -0.0302, -0.0155]]],\n    \n    \n            [[[ 0.0054,  0.0004,  0.0029],\n              [ 0.0255, -0.0076, -0.0326],\n              [-0.0039, -0.0012,  0.0100]],\n    \n             [[ 0.0220, -0.0344,  0.0064],\n              [ 0.0107,  0.0073, -0.0245],\n              [-0.0162,  0.0070,  0.0359]],\n    \n             [[ 0.0065,  0.0310, -0.0171],\n              [ 0.0107, -0.0172,  0.0115],\n              [ 0.0685, -0.0061,  0.0181]],\n    \n             ...,\n    \n             [[ 0.0076, -0.0482, -0.0393],\n              [-0.0196, -0.0453,  0.0139],\n              [-0.0087,  0.0012, -0.0318]],\n    \n             [[-0.0040, -0.0463,  0.0431],\n              [-0.0051,  0.0308, -0.0347],\n              [ 0.0406, -0.0398, -0.0116]],\n    \n             [[-0.0291,  0.0175, -0.0232],\n              [-0.0421,  0.0055,  0.0265],\n              [ 0.0402,  0.0074,  0.0066]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n           requires_grad=True),\n    Parameter containing:\n    tensor([[[[ 6.4623e-02,  3.1036e-03, -1.6909e-03],\n              [-5.0811e-02,  5.1321e-02, -2.0327e-02],\n              [-2.5826e-02, -7.2761e-03,  2.9174e-02]],\n    \n             [[-1.0896e-02,  1.2904e-02,  5.0955e-02],\n              [ 3.6596e-02, -1.9977e-02,  5.1504e-03],\n              [-4.9061e-02, -4.6353e-02,  2.0613e-02]],\n    \n             [[ 2.0397e-02, -1.1759e-03,  4.3367e-03],\n              [-5.8880e-03, -3.1016e-02,  2.4818e-03],\n              [-4.9666e-03,  1.9339e-02,  4.7281e-04]],\n    \n             ...,\n    \n             [[ 1.8981e-02,  3.8766e-02,  4.3244e-02],\n              [ 8.1618e-03,  1.0578e-02, -3.1153e-02],\n              [-1.7487e-02, -3.3963e-02, -1.9135e-02]],\n    \n             [[-2.5760e-02,  4.4152e-02, -4.3704e-03],\n              [-8.4423e-03,  3.4287e-02, -2.2436e-02],\n              [-9.9484e-03, -1.6508e-02,  2.1469e-02]],\n    \n             [[-8.3610e-02,  4.4534e-02, -2.6806e-02],\n              [-6.8165e-02,  3.5753e-02, -3.0407e-02],\n              [ 3.6105e-02,  6.7871e-03, -1.9667e-02]]],\n    \n    \n            [[[-9.3874e-03, -2.0559e-02, -4.2272e-02],\n              [-7.9620e-03,  1.5149e-02,  2.5254e-02],\n              [ 2.7601e-02, -1.1696e-02, -2.0228e-02]],\n    \n             [[ 1.5713e-02,  4.6134e-02, -2.4693e-02],\n              [-1.1118e-02, -4.8980e-02,  2.8118e-02],\n              [-3.0924e-02, -5.5215e-02,  5.3251e-02]],\n    \n             [[ 3.1298e-02, -4.9294e-03, -4.5002e-02],\n              [ 4.7404e-02, -1.7113e-02, -4.7077e-02],\n              [ 9.8409e-03, -4.5148e-03,  1.8805e-03]],\n    \n             ...,\n    \n             [[ 2.2019e-02,  9.7039e-03,  6.5827e-02],\n              [-1.6162e-02,  1.7261e-02, -2.0350e-02],\n              [-5.1338e-02,  2.8596e-03,  5.8139e-02]],\n    \n             [[-2.7181e-02, -2.1873e-02,  4.0215e-02],\n              [ 1.7008e-02,  5.3131e-02, -7.6819e-02],\n              [-1.1660e-03,  4.7270e-02,  4.2725e-02]],\n    \n             [[ 2.2202e-02, -1.7518e-02, -3.8808e-03],\n              [ 8.5621e-05,  1.5286e-02, -1.1990e-02],\n              [ 1.2953e-02, -8.6411e-03,  2.7188e-02]]],\n    \n    \n            [[[-3.0124e-02, -5.1366e-02,  3.0049e-02],\n              [ 4.6959e-02, -3.5266e-03, -3.2386e-02],\n              [-2.1860e-02,  2.8686e-02,  3.2822e-02]],\n    \n             [[-7.2528e-03, -3.7562e-02, -7.3361e-03],\n              [ 5.9430e-02,  2.9173e-03, -1.5595e-02],\n              [-8.1484e-02,  1.3374e-02,  6.9054e-02]],\n    \n             [[-3.7204e-03,  9.7711e-03,  2.8922e-02],\n              [-1.3917e-02, -2.9698e-02,  2.0477e-02],\n              [-1.2654e-02,  4.7804e-02, -2.5634e-02]],\n    \n             ...,\n    \n             [[-2.6448e-02,  2.6732e-02, -3.3593e-02],\n              [-1.6237e-02,  1.6803e-02, -2.8556e-03],\n              [-2.7642e-02, -3.0190e-02, -4.3648e-02]],\n    \n             [[ 1.2380e-02, -2.9831e-02,  1.2036e-03],\n              [ 8.1952e-03,  1.3613e-02,  2.5667e-02],\n              [ 2.5102e-03, -1.1859e-02,  3.3422e-02]],\n    \n             [[-2.7380e-02,  9.4078e-03,  5.9854e-02],\n              [ 3.0510e-02,  3.9179e-02, -3.3309e-02],\n              [-9.9818e-03, -6.2517e-03,  3.0292e-02]]],\n    \n    \n            ...,\n    \n    \n            [[[ 3.6045e-02, -1.7172e-02, -2.3148e-03],\n              [-1.1046e-02,  2.2515e-02, -1.7574e-02],\n              [ 1.3968e-02, -7.4495e-03,  4.3435e-02]],\n    \n             [[ 3.6326e-02, -3.4611e-02,  5.4862e-04],\n              [ 4.6926e-02, -2.3775e-02, -2.2022e-02],\n              [-1.0030e-02, -1.1926e-02,  3.9379e-02]],\n    \n             [[-1.6063e-02,  1.7260e-02,  2.8273e-02],\n              [-2.6628e-02,  1.6308e-02,  5.2297e-03],\n              [ 1.3639e-02,  7.0793e-03,  2.5756e-02]],\n    \n             ...,\n    \n             [[-5.6299e-02,  1.9608e-02,  1.6850e-02],\n              [-1.3933e-03,  2.9379e-02, -5.2708e-02],\n              [ 2.3092e-02,  1.5127e-02,  7.7539e-03]],\n    \n             [[-4.3371e-03, -3.8431e-02, -5.7067e-04],\n              [-5.7991e-02,  2.2882e-02,  2.5036e-02],\n              [-1.7140e-02, -5.2532e-02,  3.0792e-02]],\n    \n             [[ 8.6087e-03,  7.0665e-03,  2.8742e-02],\n              [ 3.0771e-03, -2.1568e-02, -1.5797e-02],\n              [ 2.3573e-02,  3.5091e-02,  4.8602e-03]]],\n    \n    \n            [[[-1.9757e-02, -4.7706e-03,  2.4818e-02],\n              [-4.8281e-02, -4.1109e-02,  3.7919e-02],\n              [ 2.9671e-02, -3.2449e-02,  6.6016e-02]],\n    \n             [[ 2.6220e-02, -3.4510e-02,  4.0938e-04],\n              [ 1.4246e-02,  2.3381e-02,  2.4728e-02],\n              [ 1.2747e-02, -1.2865e-02, -1.8130e-02]],\n    \n             [[ 4.1147e-02, -1.2304e-02, -4.1213e-02],\n              [ 1.6714e-02,  7.6532e-03, -1.2135e-02],\n              [-8.1790e-03,  2.2921e-02,  1.8061e-02]],\n    \n             ...,\n    \n             [[-3.1910e-02,  5.5848e-03,  3.2159e-02],\n              [ 1.5247e-03, -2.7293e-02, -1.6803e-02],\n              [-3.8777e-02, -1.9016e-02,  1.0707e-02]],\n    \n             [[ 1.9449e-02,  1.8234e-02, -3.6247e-03],\n              [-7.1869e-03, -2.2744e-02,  2.6477e-02],\n              [-1.9465e-02, -6.8063e-02,  1.3981e-02]],\n    \n             [[-2.4040e-02, -2.8921e-02, -6.5866e-02],\n              [-2.5238e-02,  4.8179e-02,  8.7033e-03],\n              [ 1.3897e-02,  3.5129e-02, -3.2094e-02]]],\n    \n    \n            [[[ 1.3475e-02, -3.2640e-03, -9.4418e-03],\n              [ 1.3454e-02,  1.4152e-02,  2.2621e-02],\n              [-3.8803e-02,  8.4345e-03, -3.0595e-02]],\n    \n             [[ 5.2310e-02, -5.7207e-02, -3.2508e-02],\n              [-1.6928e-02,  1.7738e-02,  1.8269e-02],\n              [ 2.9121e-02, -4.4776e-02,  2.0436e-02]],\n    \n             [[ 3.9004e-02,  9.9552e-03,  8.0466e-04],\n              [-4.3460e-02,  3.3845e-02,  4.6097e-02],\n              [-6.7961e-02, -3.9769e-02,  1.7397e-02]],\n    \n             ...,\n    \n             [[ 1.1047e-02, -1.8248e-04,  6.4758e-03],\n              [-2.7296e-02,  4.2689e-03,  2.8051e-02],\n              [-4.9129e-02, -3.9398e-02, -3.2437e-02]],\n    \n             [[ 1.2730e-02,  4.4774e-02, -4.0081e-02],\n              [-9.7942e-03, -4.7583e-02,  4.9842e-02],\n              [ 6.1779e-02, -4.2121e-02, -2.6728e-02]],\n    \n             [[-2.5418e-03,  1.2102e-02, -1.3844e-02],\n              [-1.2221e-02,  2.5199e-02, -4.5259e-02],\n              [ 1.6519e-02, -3.8014e-02,  5.3534e-02]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n           requires_grad=True),\n    Parameter containing:\n    tensor([[[[-2.7201e-03,  3.6549e-04,  9.9202e-03],\n              [ 1.5146e-02,  5.3587e-03, -1.5296e-02],\n              [-1.6143e-02, -8.3006e-03,  5.0950e-03]],\n    \n             [[ 1.2095e-02, -1.3766e-02,  1.8383e-02],\n              [ 2.2781e-02, -3.0125e-02, -7.6401e-03],\n              [ 4.0105e-02,  3.6369e-02, -1.5170e-02]],\n    \n             [[ 1.7235e-02,  2.9260e-02, -5.1398e-03],\n              [ 2.9509e-02,  1.3873e-02,  2.1065e-02],\n              [-1.7449e-02,  4.2113e-02,  3.1272e-02]],\n    \n             ...,\n    \n             [[-2.7271e-02,  4.3318e-03, -3.0083e-02],\n              [-1.5011e-02,  5.4228e-02, -6.9999e-03],\n              [-2.3438e-02,  2.9800e-02,  2.8188e-02]],\n    \n             [[-1.5413e-02,  5.3533e-03,  2.5238e-02],\n              [ 1.1738e-02,  9.6331e-03, -2.1970e-02],\n              [ 3.9721e-02,  6.5093e-03,  4.5141e-02]],\n    \n             [[ 1.2622e-03,  6.2283e-03, -1.1140e-02],\n              [-6.4714e-03,  2.7260e-02,  1.7059e-02],\n              [-7.0780e-03,  4.4869e-02,  6.8525e-03]]],\n    \n    \n            [[[ 8.9740e-03,  2.1068e-03, -3.2141e-02],\n              [ 2.5331e-02, -3.4869e-03, -4.2735e-02],\n              [ 8.4176e-03,  5.4030e-03,  4.0378e-02]],\n    \n             [[-9.7717e-03, -1.0116e-02, -1.5079e-02],\n              [-4.3595e-02,  2.2397e-03, -2.1572e-02],\n              [-2.1792e-02, -1.0809e-02,  1.7839e-02]],\n    \n             [[ 1.4064e-02, -2.4843e-03, -6.1930e-03],\n              [-1.9804e-02,  2.0904e-02,  4.6047e-02],\n              [ 4.6885e-02,  1.3519e-02,  7.9157e-03]],\n    \n             ...,\n    \n             [[ 6.3337e-04,  3.4997e-02,  7.2157e-03],\n              [-2.7212e-02, -1.2828e-03,  1.4576e-02],\n              [ 1.6487e-03, -3.8249e-03, -2.5522e-02]],\n    \n             [[-5.8435e-03, -1.5515e-03,  1.4384e-02],\n              [ 4.3319e-02, -4.9137e-04,  2.9897e-02],\n              [ 1.1249e-02,  1.5087e-02,  5.3903e-02]],\n    \n             [[-1.3245e-02, -1.7451e-02,  1.0870e-02],\n              [-6.3964e-03, -2.1529e-02, -4.6223e-03],\n              [ 1.0326e-02, -8.5297e-03,  5.1143e-03]]],\n    \n    \n            [[[-6.1009e-03, -2.2350e-02, -2.3218e-03],\n              [-1.4771e-02,  4.6936e-03, -1.0998e-02],\n              [-4.0314e-03,  1.9126e-02,  1.2942e-02]],\n    \n             [[ 3.8923e-03,  1.7996e-02, -8.9204e-03],\n              [ 3.0489e-02,  7.2350e-04,  2.7165e-02],\n              [ 1.2347e-02, -5.6527e-03,  3.7326e-03]],\n    \n             [[-1.2366e-02,  1.4802e-02, -8.4382e-03],\n              [ 3.9390e-02, -1.5324e-02, -1.1696e-03],\n              [-2.4787e-02,  1.9659e-03, -6.1518e-04]],\n    \n             ...,\n    \n             [[ 5.6563e-03, -2.0977e-02,  1.6497e-02],\n              [-3.4329e-03,  2.3948e-02, -1.7859e-02],\n              [ 6.3152e-03, -1.1963e-02, -4.8875e-03]],\n    \n             [[ 2.5085e-02,  1.3324e-02,  2.0275e-02],\n              [ 3.9323e-02, -2.4994e-02,  1.4204e-02],\n              [-2.2220e-02, -2.8901e-02, -3.0961e-02]],\n    \n             [[ 2.3243e-02, -5.8713e-03,  2.6223e-02],\n              [ 1.3720e-02, -3.7670e-02, -1.2708e-02],\n              [-2.2409e-02, -6.2068e-03, -5.2564e-02]]],\n    \n    \n            ...,\n    \n    \n            [[[ 7.5581e-03,  9.6807e-03,  1.9524e-03],\n              [-6.8240e-03,  4.1695e-02, -4.4301e-02],\n              [ 7.4793e-03,  1.0251e-03, -3.0985e-03]],\n    \n             [[ 1.0575e-02,  4.9680e-03, -9.4923e-03],\n              [ 9.6399e-03,  4.9905e-03,  2.3694e-02],\n              [ 4.5843e-02,  1.2436e-02, -2.2352e-02]],\n    \n             [[-5.7913e-03, -2.3239e-02, -2.4935e-02],\n              [ 2.0546e-02, -4.9113e-03,  6.4821e-03],\n              [-3.7841e-02,  4.3034e-02, -1.6875e-02]],\n    \n             ...,\n    \n             [[ 1.7606e-03, -2.8046e-02, -2.6849e-02],\n              [ 8.9679e-03, -1.1608e-03,  1.8552e-02],\n              [-3.6729e-02, -8.7902e-03, -2.5777e-02]],\n    \n             [[ 8.3304e-03,  1.7895e-02, -1.9896e-03],\n              [-2.7459e-03, -1.0607e-02,  1.9768e-02],\n              [ 2.8793e-02,  3.3022e-02,  2.2466e-02]],\n    \n             [[-7.7430e-03,  3.5954e-02, -3.4948e-03],\n              [-2.8555e-03,  1.3956e-02, -1.7856e-03],\n              [ 3.1080e-03,  3.0151e-03, -2.2074e-02]]],\n    \n    \n            [[[-3.5099e-03, -1.7279e-02,  2.3381e-02],\n              [-4.5143e-03,  3.7635e-02, -2.4972e-02],\n              [-2.7560e-02,  9.0284e-03, -2.6764e-03]],\n    \n             [[-3.8506e-02,  1.8940e-02,  3.3665e-02],\n              [ 3.0797e-02,  4.4894e-02, -3.6211e-02],\n              [ 1.9734e-02, -9.6193e-03, -2.5182e-02]],\n    \n             [[ 4.7038e-03,  6.9881e-03, -8.0916e-03],\n              [ 1.7744e-02,  2.9643e-02, -3.9575e-04],\n              [ 1.6029e-02,  1.4606e-03, -2.2072e-03]],\n    \n             ...,\n    \n             [[ 1.4101e-02,  1.0807e-02,  6.9191e-03],\n              [-1.5712e-02,  6.6102e-03, -2.2780e-02],\n              [-3.2513e-02,  1.6988e-02, -1.6223e-02]],\n    \n             [[-3.0598e-03, -3.0545e-03, -4.5554e-05],\n              [ 1.6595e-02,  5.5797e-02,  1.5641e-02],\n              [-3.7813e-02,  4.8110e-02, -4.1068e-02]],\n    \n             [[ 3.6013e-03, -9.7284e-03, -2.0860e-02],\n              [-2.9883e-02, -2.6987e-03,  2.5481e-02],\n              [-3.0933e-03, -1.3216e-02, -3.8352e-03]]],\n    \n    \n            [[[-4.6974e-02, -1.4302e-03,  3.9636e-02],\n              [-3.5226e-02, -1.6219e-02, -1.3099e-03],\n              [ 1.8657e-02, -2.3861e-02,  4.4932e-03]],\n    \n             [[ 1.2391e-02,  3.9086e-03,  2.2743e-02],\n              [-7.3400e-03,  1.1427e-02, -1.9263e-03],\n              [-2.4964e-02, -1.3556e-05,  1.4340e-02]],\n    \n             [[ 1.2488e-02, -2.8052e-02, -4.8402e-02],\n              [ 1.0042e-02, -5.6384e-03,  5.0890e-03],\n              [ 9.7296e-03, -2.2721e-02, -1.8964e-02]],\n    \n             ...,\n    \n             [[ 1.0270e-02, -3.2315e-03,  7.2160e-03],\n              [-2.5015e-02,  2.5506e-02, -3.6134e-02],\n              [-2.0684e-02, -1.8355e-02, -9.6217e-03]],\n    \n             [[ 1.8792e-02, -1.6453e-02, -4.9554e-02],\n              [-2.3449e-02,  1.8740e-02,  1.9927e-02],\n              [-1.2801e-02, -1.2468e-02,  2.7391e-03]],\n    \n             [[ 4.0318e-02,  1.1553e-02,  1.4076e-02],\n              [ 4.8108e-02, -5.1772e-02,  3.4008e-02],\n              [-1.5171e-02, -2.5443e-02,  8.8261e-03]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n    Parameter containing:\n    tensor([[[[-1.4024e-02, -8.1215e-03, -2.1284e-02],\n              [-1.8448e-02,  1.4248e-02, -4.9354e-02],\n              [ 4.9315e-03,  1.3149e-02, -4.0124e-03]],\n    \n             [[-1.8957e-03,  4.1597e-02, -1.2780e-02],\n              [ 3.1927e-02,  8.3222e-03, -1.0450e-04],\n              [-1.2606e-02,  2.6011e-02, -8.4924e-04]],\n    \n             [[-3.2251e-03,  3.8982e-03,  8.6754e-03],\n              [-3.2155e-02, -8.7158e-03, -1.0409e-02],\n              [-5.2620e-02,  3.9306e-02, -2.0130e-02]],\n    \n             ...,\n    \n             [[-2.5992e-02, -1.0890e-02,  1.2273e-02],\n              [-2.1310e-02, -1.4400e-02, -7.6920e-03],\n              [ 3.8477e-03, -2.1388e-02,  8.5470e-03]],\n    \n             [[-4.1957e-03,  2.7619e-02, -5.3744e-03],\n              [-1.0581e-02, -6.3883e-03, -9.8762e-03],\n              [ 1.0489e-03, -2.0728e-03, -1.0626e-02]],\n    \n             [[-4.3516e-02, -3.5632e-03, -2.6853e-02],\n              [ 2.2356e-02, -4.8946e-04, -3.2571e-02],\n              [ 8.5421e-03,  1.2478e-02, -3.1595e-03]]],\n    \n    \n            [[[-1.7403e-02, -3.8452e-02,  3.8152e-02],\n              [-2.4565e-02, -3.0964e-02, -1.9138e-02],\n              [ 3.9489e-02, -1.9281e-02,  2.0403e-02]],\n    \n             [[ 2.5185e-02, -9.9155e-03,  2.4020e-02],\n              [-2.7855e-02, -1.6057e-04, -3.8420e-02],\n              [-1.6801e-02,  1.0033e-02, -2.3181e-02]],\n    \n             [[-1.5353e-03,  2.3777e-02,  1.2098e-02],\n              [-2.2675e-02, -1.4833e-02,  2.2173e-02],\n              [ 2.3860e-02,  1.6748e-02,  3.1056e-02]],\n    \n             ...,\n    \n             [[-1.9150e-02,  1.5278e-02, -1.8297e-02],\n              [ 7.4174e-03, -1.1909e-03, -2.9946e-02],\n              [-2.1054e-02, -2.6686e-02, -2.5954e-03]],\n    \n             [[-2.5423e-02,  1.7297e-02, -1.7232e-02],\n              [-7.9541e-03,  4.4319e-02,  1.0790e-02],\n              [ 1.9040e-02,  2.5322e-02,  1.4981e-02]],\n    \n             [[-5.2588e-03,  1.0297e-02,  1.0898e-02],\n              [ 9.3869e-03, -1.9633e-02, -1.9501e-02],\n              [ 9.7995e-03,  1.0548e-02, -1.2320e-02]]],\n    \n    \n            [[[ 1.5746e-03, -3.2754e-02,  1.0371e-02],\n              [ 9.6084e-03, -2.7340e-04, -3.1158e-03],\n              [-2.3574e-02,  1.1615e-02, -2.0470e-02]],\n    \n             [[-2.6733e-02,  4.2821e-03,  1.6463e-02],\n              [ 2.0670e-02,  3.1634e-02,  3.8193e-03],\n              [ 1.1513e-02,  6.2699e-03,  2.9505e-02]],\n    \n             [[-2.0623e-02, -1.7010e-02, -7.0865e-03],\n              [-6.4661e-03,  2.6339e-02,  2.2275e-02],\n              [-2.8578e-03, -1.4107e-02,  3.9595e-03]],\n    \n             ...,\n    \n             [[-1.7973e-02,  3.0159e-02,  4.8490e-03],\n              [ 1.2373e-02, -2.2682e-03, -8.1696e-03],\n              [ 7.3892e-03, -9.8826e-03,  6.8568e-03]],\n    \n             [[-1.0285e-02,  4.8256e-03,  3.6186e-03],\n              [ 1.5287e-03,  9.7001e-03, -2.7474e-02],\n              [-1.3475e-02,  5.3503e-03, -7.5929e-03]],\n    \n             [[ 2.6737e-02,  5.5355e-02,  1.2739e-02],\n              [ 2.4482e-02, -2.2594e-02, -1.9623e-02],\n              [ 5.3584e-04, -3.3008e-03,  1.6550e-02]]],\n    \n    \n            ...,\n    \n    \n            [[[ 1.2646e-02,  1.6056e-02, -4.3820e-02],\n              [ 7.7092e-03,  2.2237e-03, -2.8682e-02],\n              [ 4.4841e-03,  1.3179e-02,  2.4575e-03]],\n    \n             [[ 4.6966e-03,  8.1230e-03,  1.4897e-02],\n              [-9.7666e-03, -2.7992e-02, -2.1170e-02],\n              [ 4.9865e-02,  1.5399e-02,  2.4057e-02]],\n    \n             [[-8.1184e-03,  3.2623e-02, -1.5193e-02],\n              [ 1.0324e-02, -1.5721e-02,  1.0766e-02],\n              [-8.3123e-03, -2.4659e-02,  3.4180e-02]],\n    \n             ...,\n    \n             [[-1.4497e-02,  2.9169e-03, -1.9871e-02],\n              [ 2.0851e-02,  2.4238e-02,  1.8242e-02],\n              [ 3.4250e-03,  7.9568e-04,  1.1452e-02]],\n    \n             [[-1.1757e-02, -9.5502e-04, -1.6147e-02],\n              [-1.9615e-02,  1.5328e-02, -2.8158e-02],\n              [ 1.3762e-02, -1.2282e-02, -3.7142e-03]],\n    \n             [[-2.0024e-02, -2.9656e-03, -1.4673e-02],\n              [-3.1923e-02,  2.2153e-02,  9.3153e-03],\n              [ 3.8173e-02,  1.4119e-02, -1.6652e-02]]],\n    \n    \n            [[[-1.0703e-02, -1.3399e-02,  1.3111e-02],\n              [ 9.8348e-04, -1.0083e-03, -1.5403e-02],\n              [ 1.9699e-02, -2.3839e-02,  1.7139e-03]],\n    \n             [[-4.1178e-03,  1.5572e-02, -2.3520e-02],\n              [-3.0667e-02,  3.4946e-02, -1.5716e-02],\n              [ 2.4502e-02, -1.3221e-03, -2.2685e-02]],\n    \n             [[-1.3873e-02, -2.6076e-02, -2.4513e-03],\n              [-5.9047e-03,  3.1610e-02,  3.8466e-02],\n              [-1.8621e-02, -8.3457e-03, -1.7440e-03]],\n    \n             ...,\n    \n             [[-3.4389e-02, -1.2358e-02,  1.6832e-02],\n              [-1.2840e-02,  9.5356e-03, -2.5016e-02],\n              [-1.8461e-02,  1.9514e-04,  4.5127e-02]],\n    \n             [[-2.1032e-02, -1.7887e-02,  9.8996e-03],\n              [-8.0429e-03, -2.3803e-02, -2.8408e-02],\n              [ 1.4625e-02, -1.2941e-02,  4.2276e-03]],\n    \n             [[-1.0633e-02,  1.2666e-02,  1.7662e-02],\n              [-1.5075e-02, -4.3497e-02, -2.4544e-03],\n              [ 7.5402e-03,  1.7709e-02,  2.7310e-02]]],\n    \n    \n            [[[-2.2355e-02,  7.1892e-03, -8.8818e-03],\n              [-2.5967e-02,  4.4696e-03, -1.6043e-02],\n              [-3.8327e-03,  3.0586e-02, -1.4844e-02]],\n    \n             [[-9.4987e-03, -1.1000e-02,  1.5638e-02],\n              [-4.3995e-02, -3.7564e-02, -9.8483e-03],\n              [ 1.7880e-02,  6.6495e-02,  1.0961e-02]],\n    \n             [[ 2.9250e-02, -1.9077e-02, -1.8478e-02],\n              [ 3.3976e-03,  1.7772e-02,  2.5940e-02],\n              [-4.7022e-02, -1.0651e-02, -3.0009e-02]],\n    \n             ...,\n    \n             [[-1.0584e-02, -1.1123e-02,  1.1891e-02],\n              [ 1.7422e-02,  2.2473e-02, -1.9112e-02],\n              [ 4.0642e-03, -2.2786e-03, -1.4051e-02]],\n    \n             [[-1.1817e-02, -1.2386e-02,  2.5777e-04],\n              [ 2.0997e-03, -2.1786e-02, -5.9378e-05],\n              [-1.0138e-04, -1.0681e-02,  1.4489e-02]],\n    \n             [[-4.6515e-03,  3.1262e-02, -2.3577e-02],\n              [ 3.1361e-02, -1.1654e-02,  8.9977e-03],\n              [ 2.5080e-02,  1.4512e-02, -8.9893e-03]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n    Parameter containing:\n    tensor([[[[ 0.1360]],\n    \n             [[ 0.0832]],\n    \n             [[ 0.0222]],\n    \n             ...,\n    \n             [[ 0.0242]],\n    \n             [[-0.0474]],\n    \n             [[-0.0653]]],\n    \n    \n            [[[-0.0916]],\n    \n             [[ 0.0135]],\n    \n             [[-0.1192]],\n    \n             ...,\n    \n             [[ 0.0664]],\n    \n             [[ 0.0046]],\n    \n             [[ 0.0382]]],\n    \n    \n            [[[ 0.0095]],\n    \n             [[-0.0914]],\n    \n             [[-0.0956]],\n    \n             ...,\n    \n             [[ 0.0613]],\n    \n             [[-0.0209]],\n    \n             [[-0.0115]]],\n    \n    \n            ...,\n    \n    \n            [[[ 0.1510]],\n    \n             [[ 0.0408]],\n    \n             [[-0.0394]],\n    \n             ...,\n    \n             [[-0.0255]],\n    \n             [[-0.0439]],\n    \n             [[-0.0423]]],\n    \n    \n            [[[ 0.0717]],\n    \n             [[-0.0070]],\n    \n             [[ 0.0669]],\n    \n             ...,\n    \n             [[ 0.1052]],\n    \n             [[-0.0623]],\n    \n             [[-0.0021]]],\n    \n    \n            [[[ 0.0591]],\n    \n             [[ 0.0183]],\n    \n             [[-0.0151]],\n    \n             ...,\n    \n             [[-0.0513]],\n    \n             [[ 0.0162]],\n    \n             [[-0.0744]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n    Parameter containing:\n    tensor([[[[ 0.0390,  0.0149, -0.0229],\n              [ 0.0453,  0.0070, -0.0489],\n              [ 0.0049,  0.0501,  0.0247]],\n    \n             [[ 0.0125,  0.0115, -0.0094],\n              [-0.0523, -0.0063,  0.0120],\n              [-0.0102,  0.0141,  0.0089]],\n    \n             [[ 0.0171,  0.0106,  0.0005],\n              [ 0.0591,  0.0104,  0.0083],\n              [ 0.0042, -0.0056, -0.0263]],\n    \n             ...,\n    \n             [[-0.0066, -0.0339,  0.0045],\n              [-0.0083, -0.0215, -0.0392],\n              [ 0.0403,  0.0352, -0.0014]],\n    \n             [[ 0.0306, -0.0236,  0.0039],\n              [ 0.0195,  0.0091,  0.0214],\n              [-0.0316,  0.0094,  0.0297]],\n    \n             [[-0.0166,  0.0137, -0.0167],\n              [ 0.0309, -0.0001,  0.0042],\n              [-0.0328,  0.0258, -0.0039]]],\n    \n    \n            [[[ 0.0264,  0.0247,  0.0305],\n              [-0.0144, -0.0101,  0.0055],\n              [-0.0157, -0.0031,  0.0012]],\n    \n             [[-0.0238,  0.0222, -0.0468],\n              [ 0.0307, -0.0467, -0.0360],\n              [-0.0421, -0.0292,  0.0177]],\n    \n             [[ 0.0080, -0.0159,  0.0245],\n              [ 0.0108, -0.0087,  0.0091],\n              [-0.0223,  0.0570, -0.0108]],\n    \n             ...,\n    \n             [[ 0.0044, -0.0222, -0.0038],\n              [ 0.0111, -0.0117,  0.0015],\n              [-0.0137, -0.0195,  0.0267]],\n    \n             [[ 0.0426, -0.0137,  0.0135],\n              [-0.0043, -0.0060,  0.0169],\n              [-0.0090, -0.0095,  0.0288]],\n    \n             [[-0.0201,  0.0056,  0.0040],\n              [-0.0216, -0.0084, -0.0024],\n              [ 0.0105,  0.0075,  0.0023]]],\n    \n    \n            [[[ 0.0449, -0.0425, -0.0088],\n              [-0.0173, -0.0131, -0.0235],\n              [-0.0329, -0.0035, -0.0128]],\n    \n             [[-0.0016, -0.0207,  0.0214],\n              [-0.0363,  0.0238, -0.0162],\n              [-0.0280,  0.0278,  0.0212]],\n    \n             [[ 0.0031, -0.0236,  0.0031],\n              [ 0.0187, -0.0259, -0.0210],\n              [-0.0114,  0.0079, -0.0339]],\n    \n             ...,\n    \n             [[ 0.0074, -0.0119,  0.0234],\n              [ 0.0101,  0.0154,  0.0284],\n              [ 0.0175, -0.0035,  0.0056]],\n    \n             [[ 0.0209, -0.0065, -0.0340],\n              [ 0.0031, -0.0026,  0.0132],\n              [ 0.0168,  0.0353, -0.0126]],\n    \n             [[-0.0127,  0.0176,  0.0197],\n              [ 0.0062,  0.0035, -0.0040],\n              [-0.0007,  0.0063,  0.0271]]],\n    \n    \n            ...,\n    \n    \n            [[[ 0.0092, -0.0316,  0.0116],\n              [ 0.0186,  0.0132, -0.0254],\n              [ 0.0225,  0.0039,  0.0042]],\n    \n             [[ 0.0061, -0.0152,  0.0097],\n              [-0.0029, -0.0115,  0.0193],\n              [-0.0078,  0.0056, -0.0051]],\n    \n             [[ 0.0105, -0.0128,  0.0002],\n              [ 0.0061,  0.0009, -0.0117],\n              [ 0.0232, -0.0202, -0.0095]],\n    \n             ...,\n    \n             [[ 0.0183,  0.0092, -0.0044],\n              [-0.0338,  0.0140, -0.0021],\n              [-0.0014,  0.0116,  0.0250]],\n    \n             [[-0.0106,  0.0206, -0.0270],\n              [ 0.0101,  0.0246,  0.0113],\n              [ 0.0180,  0.0354, -0.0095]],\n    \n             [[-0.0094, -0.0173, -0.0041],\n              [-0.0022,  0.0086, -0.0051],\n              [ 0.0147,  0.0205,  0.0083]]],\n    \n    \n            [[[-0.0101, -0.0102,  0.0070],\n              [ 0.0353, -0.0177, -0.0055],\n              [ 0.0040,  0.0359,  0.0022]],\n    \n             [[-0.0050, -0.0044,  0.0026],\n              [ 0.0162,  0.0046, -0.0060],\n              [-0.0047,  0.0345,  0.0123]],\n    \n             [[-0.0463, -0.0112, -0.0097],\n              [ 0.0065,  0.0113,  0.0285],\n              [ 0.0201, -0.0257, -0.0407]],\n    \n             ...,\n    \n             [[-0.0063, -0.0167, -0.0196],\n              [ 0.0024,  0.0577, -0.0007],\n              [ 0.0367,  0.0114, -0.0218]],\n    \n             [[-0.0042,  0.0085, -0.0169],\n              [-0.0416, -0.0343,  0.0124],\n              [ 0.0297,  0.0276,  0.0276]],\n    \n             [[ 0.0133,  0.0187, -0.0148],\n              [-0.0086,  0.0061,  0.0063],\n              [-0.0297,  0.0185, -0.0318]]],\n    \n    \n            [[[-0.0365, -0.0127, -0.0116],\n              [-0.0239, -0.0212,  0.0103],\n              [ 0.0023, -0.0017,  0.0070]],\n    \n             [[ 0.0245, -0.0083, -0.0432],\n              [ 0.0012, -0.0025, -0.0043],\n              [ 0.0022,  0.0149, -0.0212]],\n    \n             [[-0.0358, -0.0198, -0.0341],\n              [ 0.0104, -0.0223, -0.0235],\n              [-0.0177,  0.0248,  0.0002]],\n    \n             ...,\n    \n             [[ 0.0337, -0.0066,  0.0072],\n              [ 0.0315,  0.0156, -0.0044],\n              [ 0.0024,  0.0036, -0.0051]],\n    \n             [[-0.0053,  0.0314, -0.0054],\n              [ 0.0099, -0.0040,  0.0472],\n              [-0.0192,  0.0368,  0.0062]],\n    \n             [[-0.0328,  0.0053,  0.0398],\n              [ 0.0243,  0.0103, -0.0108],\n              [-0.0171, -0.0134,  0.0094]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n    Parameter containing:\n    tensor([[[[-0.0069,  0.0019,  0.0056],\n              [-0.0013, -0.0262,  0.0222],\n              [ 0.0196,  0.0241, -0.0096]],\n    \n             [[-0.0260,  0.0322, -0.0211],\n              [-0.0518, -0.0042,  0.0229],\n              [ 0.0171, -0.0162, -0.0086]],\n    \n             [[ 0.0157, -0.0194,  0.0507],\n              [ 0.0028,  0.0103, -0.0308],\n              [-0.0095, -0.0023, -0.0316]],\n    \n             ...,\n    \n             [[-0.0154, -0.0176,  0.0013],\n              [-0.0301,  0.0244, -0.0225],\n              [ 0.0230,  0.0051,  0.0333]],\n    \n             [[-0.0392,  0.0339,  0.0160],\n              [-0.0056, -0.0307,  0.0062],\n              [-0.0065,  0.0068, -0.0274]],\n    \n             [[-0.0010,  0.0228, -0.0379],\n              [-0.0026,  0.0039,  0.0151],\n              [ 0.0228,  0.0109, -0.0071]]],\n    \n    \n            [[[-0.0305, -0.0266, -0.0199],\n              [-0.0133, -0.0045,  0.0073],\n              [-0.0119, -0.0108, -0.0061]],\n    \n             [[-0.0140,  0.0258,  0.0070],\n              [-0.0188,  0.0284, -0.0056],\n              [-0.0394,  0.0019,  0.0012]],\n    \n             [[-0.0074,  0.0088,  0.0259],\n              [ 0.0176,  0.0058,  0.0276],\n              [-0.0060, -0.0190, -0.0256]],\n    \n             ...,\n    \n             [[ 0.0339,  0.0270,  0.0086],\n              [ 0.0014,  0.0184, -0.0128],\n              [ 0.0095,  0.0346,  0.0188]],\n    \n             [[ 0.0277, -0.0214,  0.0094],\n              [ 0.0084, -0.0134,  0.0217],\n              [ 0.0185,  0.0345,  0.0157]],\n    \n             [[ 0.0145, -0.0110,  0.0163],\n              [-0.0136,  0.0043, -0.0076],\n              [-0.0008,  0.0213,  0.0114]]],\n    \n    \n            [[[ 0.0118,  0.0024, -0.0209],\n              [ 0.0338,  0.0289, -0.0288],\n              [-0.0028,  0.0359, -0.0322]],\n    \n             [[-0.0083, -0.0107,  0.0137],\n              [ 0.0041, -0.0143,  0.0260],\n              [-0.0066, -0.0291,  0.0137]],\n    \n             [[-0.0556, -0.0083, -0.0230],\n              [-0.0253, -0.0376,  0.0269],\n              [ 0.0176, -0.0073,  0.0186]],\n    \n             ...,\n    \n             [[-0.0214,  0.0152,  0.0189],\n              [ 0.0591, -0.0079, -0.0148],\n              [ 0.0128,  0.0155,  0.0101]],\n    \n             [[ 0.0282, -0.0026,  0.0021],\n              [ 0.0207,  0.0148,  0.0075],\n              [-0.0146,  0.0031,  0.0105]],\n    \n             [[ 0.0242,  0.0016,  0.0078],\n              [ 0.0255, -0.0075,  0.0076],\n              [-0.0251, -0.0025, -0.0091]]],\n    \n    \n            ...,\n    \n    \n            [[[ 0.0212,  0.0059, -0.0058],\n              [-0.0296,  0.0185, -0.0022],\n              [ 0.0090,  0.0066, -0.0208]],\n    \n             [[-0.0267, -0.0019, -0.0101],\n              [-0.0345, -0.0292,  0.0176],\n              [ 0.0125, -0.0163, -0.0198]],\n    \n             [[-0.0353, -0.0058, -0.0169],\n              [-0.0110, -0.0318, -0.0199],\n              [-0.0159,  0.0087,  0.0093]],\n    \n             ...,\n    \n             [[-0.0363, -0.0068, -0.0065],\n              [-0.0200, -0.0080,  0.0255],\n              [-0.0398, -0.0013, -0.0155]],\n    \n             [[ 0.0192,  0.0129,  0.0002],\n              [ 0.0244, -0.0053, -0.0175],\n              [-0.0478,  0.0039,  0.0213]],\n    \n             [[-0.0234,  0.0368,  0.0181],\n              [ 0.0102,  0.0231,  0.0108],\n              [-0.0048,  0.0015,  0.0237]]],\n    \n    \n            [[[-0.0306, -0.0421,  0.0019],\n              [ 0.0088,  0.0046,  0.0110],\n              [ 0.0132, -0.0035,  0.0096]],\n    \n             [[-0.0160,  0.0151,  0.0317],\n              [ 0.0110, -0.0162, -0.0013],\n              [-0.0147,  0.0145,  0.0168]],\n    \n             [[-0.0298,  0.0183,  0.0219],\n              [-0.0005, -0.0064, -0.0141],\n              [-0.0157, -0.0311,  0.0234]],\n    \n             ...,\n    \n             [[-0.0197, -0.0330,  0.0239],\n              [-0.0366, -0.0094,  0.0117],\n              [-0.0270,  0.0064, -0.0212]],\n    \n             [[ 0.0255, -0.0213,  0.0168],\n              [-0.0047,  0.0363, -0.0152],\n              [ 0.0041,  0.0222, -0.0176]],\n    \n             [[-0.0020, -0.0164,  0.0270],\n              [ 0.0061, -0.0458, -0.0043],\n              [ 0.0188, -0.0213,  0.0219]]],\n    \n    \n            [[[-0.0048,  0.0279,  0.0654],\n              [ 0.0274,  0.0166, -0.0259],\n              [-0.0007, -0.0205,  0.0249]],\n    \n             [[ 0.0316, -0.0235, -0.0315],\n              [ 0.0209, -0.0190,  0.0056],\n              [ 0.0086,  0.0262, -0.0146]],\n    \n             [[-0.0162, -0.0358, -0.0091],\n              [ 0.0022, -0.0174, -0.0157],\n              [-0.0083, -0.0125,  0.0302]],\n    \n             ...,\n    \n             [[ 0.0017,  0.0020, -0.0277],\n              [ 0.0068, -0.0487,  0.0065],\n              [-0.0112,  0.0278,  0.0211]],\n    \n             [[ 0.0349, -0.0176,  0.0242],\n              [-0.0135,  0.0110,  0.0522],\n              [-0.0515,  0.0012,  0.0305]],\n    \n             [[ 0.0212,  0.0284, -0.0460],\n              [ 0.0013,  0.0340,  0.0012],\n              [ 0.0077, -0.0169, -0.0304]]]], requires_grad=True),\n    Parameter containing:\n    tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n            1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n    Parameter containing:\n    tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n    Parameter containing:\n    tensor([[ 0.0375, -0.0189,  0.0335,  ..., -0.0268,  0.0293,  0.0093],\n            [ 0.0222, -0.0155, -0.0396,  ...,  0.0311,  0.0103, -0.0356],\n            [-0.0390, -0.0320, -0.0042,  ...,  0.0323,  0.0414, -0.0423],\n            ...,\n            [-0.0418,  0.0144,  0.0077,  ..., -0.0416,  0.0397, -0.0280],\n            [ 0.0017,  0.0382,  0.0132,  ..., -0.0385, -0.0250, -0.0246],\n            [ 0.0267,  0.0295, -0.0034,  ...,  0.0104,  0.0144, -0.0143]],\n           requires_grad=True),\n    Parameter containing:\n    tensor([-1.0037e-02,  2.7371e-02,  3.5524e-02,  1.5841e-02, -1.7882e-02,\n             3.7492e-02,  2.2989e-02,  2.5000e-02, -3.4186e-02,  7.8577e-03,\n            -4.3075e-03,  1.4331e-02,  4.2627e-02,  2.9756e-02, -1.0799e-02,\n            -4.3315e-02,  2.6518e-02, -1.4011e-02,  7.7856e-03,  7.2420e-03,\n             2.1364e-02,  3.8713e-02, -2.9478e-02,  5.0666e-04, -4.4051e-02,\n            -4.1559e-02,  2.7724e-04,  3.2642e-02, -3.8921e-02,  4.1747e-02,\n            -1.2201e-03,  2.6370e-02, -3.2844e-02,  3.6575e-02,  3.7974e-02,\n            -4.1429e-02, -3.3779e-02, -2.7227e-02, -1.6922e-02,  2.8408e-02,\n            -1.3063e-02, -3.4165e-02, -9.2026e-03, -1.2430e-02,  1.5291e-02,\n             1.9308e-02,  3.1294e-02, -3.3254e-02, -3.5197e-02,  1.6438e-02,\n             9.0808e-03, -1.2974e-02,  2.0220e-02, -2.5816e-02,  1.3839e-02,\n            -2.6159e-02, -2.7019e-02,  2.4867e-02, -1.9950e-02, -4.0968e-02,\n             3.2726e-02,  1.7335e-02, -4.1836e-02,  1.4820e-02,  3.1301e-02,\n             4.4181e-02, -9.4473e-03,  7.0741e-03,  3.9094e-02, -1.2278e-03,\n             3.8430e-02,  1.1200e-02, -3.6914e-02,  1.3706e-04,  1.5920e-02,\n            -1.0453e-02,  4.2799e-03,  1.0829e-02,  4.1446e-02, -3.0405e-02,\n            -3.3043e-03, -9.7185e-03, -3.9552e-02,  3.9412e-02,  3.1309e-02,\n            -1.2636e-02,  2.4242e-02,  4.4118e-02, -3.2778e-02, -2.1132e-02,\n            -2.6850e-03,  4.0116e-02, -3.6703e-02, -1.7132e-02, -1.9969e-02,\n            -3.0824e-02, -2.1204e-02,  1.8029e-02, -3.5650e-02, -2.5151e-02,\n            -1.9332e-02,  1.9044e-02, -1.8089e-02,  1.9194e-02,  1.0873e-02,\n             2.8060e-02, -2.2733e-02, -3.0240e-02, -1.6744e-02, -2.2907e-02,\n             1.6260e-02,  3.5327e-02, -2.4749e-02, -6.5810e-03, -1.0026e-03,\n             2.3478e-02, -9.7370e-03, -6.4744e-03, -2.1012e-02, -3.9950e-02,\n            -1.4102e-03, -2.9137e-02,  3.8195e-02,  1.1902e-02, -1.6585e-03,\n             1.0841e-02, -1.8828e-02,  2.3857e-02,  3.7408e-02, -2.3951e-02,\n            -1.1774e-02, -1.9370e-02,  7.5753e-03, -2.7434e-02,  2.5654e-03,\n             2.2982e-02,  1.5158e-02,  2.8230e-02, -3.6385e-02, -2.7406e-02,\n            -2.0908e-02,  1.9547e-02,  2.6236e-02, -3.5381e-02,  1.1961e-02,\n            -1.4790e-02,  1.1226e-02, -2.1968e-02, -3.6338e-02, -8.7482e-03,\n            -3.7057e-02,  2.4641e-02, -2.9884e-02, -3.9860e-02,  1.2964e-02,\n             8.2671e-03,  2.3754e-02,  2.3235e-02, -3.1117e-02,  1.0795e-02,\n             3.4411e-02,  1.4392e-02, -2.2589e-02,  8.6564e-03, -2.5515e-02,\n             1.1987e-02, -4.0532e-02,  7.6445e-03,  9.2133e-03, -3.1804e-02,\n             1.0397e-02, -4.2144e-02, -3.2450e-02, -3.8947e-02,  6.9791e-03,\n            -1.9191e-02,  1.7517e-02, -4.9455e-03,  1.4743e-02, -2.7777e-02,\n            -2.6605e-02,  3.1398e-02, -3.6930e-02, -2.3900e-02,  4.2027e-02,\n            -1.2711e-02, -2.0327e-02, -1.5901e-02,  3.6922e-02,  7.6777e-03,\n            -7.6597e-03,  9.2744e-03,  2.6365e-03, -4.2992e-02, -8.5730e-03,\n            -3.6260e-02, -2.6200e-02, -2.2441e-03, -3.8264e-02, -6.1947e-03,\n            -1.4151e-02, -2.6643e-02,  5.6913e-03, -3.4119e-02, -2.3458e-02,\n             3.1246e-02,  2.7137e-02,  1.1934e-02,  1.1251e-02, -1.9132e-02,\n             1.6984e-02, -1.2489e-03,  4.7158e-03,  1.8212e-02, -2.1984e-02,\n             3.5425e-03, -1.7336e-02, -8.2912e-03,  3.5323e-03,  2.6926e-02,\n             4.0623e-02,  9.6084e-03, -1.0832e-02,  1.8980e-02,  4.3242e-03,\n             3.5424e-02, -2.2229e-02,  1.9003e-03,  2.3795e-02, -7.5264e-03,\n            -2.0495e-02, -2.9108e-02, -4.2669e-02,  1.4465e-02, -4.1209e-02,\n             3.2209e-02,  1.2422e-02, -2.4532e-03,  1.6501e-02,  5.9655e-03,\n             4.1261e-02,  3.5848e-02,  4.2628e-02,  1.2178e-02,  3.0875e-02,\n             1.6754e-02,  3.9868e-02,  3.1755e-02, -1.4661e-02, -9.0513e-03,\n            -2.0896e-02,  3.3799e-02,  2.0254e-03, -2.7774e-02, -2.0336e-02,\n             1.4178e-02,  8.7604e-03, -2.9604e-02, -3.2724e-02, -2.1593e-02,\n            -3.3991e-02,  1.7006e-03, -3.9996e-02,  4.9836e-03,  3.1809e-02,\n             1.2690e-02, -6.2529e-03, -8.3919e-03,  2.8835e-02, -2.4375e-02,\n             2.1531e-02, -2.9978e-02, -5.2936e-03, -2.4383e-02,  3.2039e-02,\n             6.6370e-03, -3.9236e-02,  3.0219e-02, -4.2168e-02,  3.2608e-02,\n            -1.9933e-02, -3.5686e-02, -5.7347e-03, -2.3351e-03,  3.6685e-02,\n            -1.3850e-03, -1.5920e-02, -9.7540e-03,  2.0244e-02, -3.5878e-02,\n             3.5654e-04,  9.8451e-03, -4.2745e-02,  4.1273e-02, -1.9455e-02,\n             1.4588e-02, -3.4353e-02, -5.8890e-03, -3.8284e-02, -9.0683e-03,\n            -2.8605e-02,  2.5554e-02, -2.5138e-02,  2.2193e-02, -1.6305e-02,\n             2.9267e-02,  2.5129e-02, -1.2583e-02,  3.0389e-03, -2.5377e-02,\n            -8.1919e-03, -3.3356e-03,  1.0318e-02,  4.3101e-03,  2.2053e-02,\n             3.0869e-02, -3.4911e-03, -3.6201e-02, -2.5275e-02,  1.2992e-02,\n            -4.2487e-02,  2.0152e-02, -2.0177e-02, -3.2820e-02,  1.3612e-02,\n             4.6325e-04,  2.5532e-02, -2.9429e-02, -2.1508e-02,  1.6725e-02,\n             3.7754e-02, -3.1807e-02,  6.0111e-03, -2.5083e-02, -2.7664e-02,\n             1.4708e-02,  2.4064e-02, -1.9906e-02,  1.7308e-02,  1.9597e-02,\n            -3.1852e-03,  1.5878e-02,  2.2884e-02, -9.2081e-03,  3.6589e-02,\n            -1.4200e-02, -2.4233e-02,  4.0047e-02, -4.1976e-02,  3.4310e-02,\n            -3.5920e-03, -1.2627e-02,  3.3568e-02,  6.7135e-03,  9.3191e-03,\n            -3.3240e-03, -4.6424e-03,  8.5870e-03,  3.4119e-02,  1.5150e-02,\n            -2.2598e-02, -3.5545e-02,  1.7635e-02, -3.0896e-02, -2.8331e-02,\n            -3.3050e-02, -4.3732e-02,  2.2013e-02,  2.3711e-02, -1.5971e-02,\n            -1.2601e-02,  1.1081e-02,  3.3664e-02, -4.1869e-02, -4.0967e-02,\n             2.5097e-02,  6.9856e-03,  3.3988e-02,  3.1370e-02,  3.8296e-02,\n             4.2545e-02,  4.3294e-02,  4.8493e-03, -3.5013e-02,  3.7419e-02,\n            -1.4878e-02,  4.5379e-03,  1.1384e-02, -5.9806e-03, -2.0033e-02,\n             3.8778e-02,  3.6449e-02,  1.7907e-02, -6.7933e-03, -1.3633e-02,\n             2.6464e-03, -2.8225e-02,  3.5408e-02, -1.8856e-02, -3.0792e-02,\n            -5.9619e-03,  3.4804e-02, -4.4018e-02, -3.0234e-02,  3.4512e-02,\n            -2.0739e-02, -3.9466e-02, -3.2838e-05,  3.1627e-02, -3.8376e-02,\n             2.4785e-02, -3.4149e-02, -4.2399e-02, -4.4160e-04,  2.3896e-03,\n            -3.4453e-02, -2.8522e-02,  4.0101e-04,  6.1951e-03, -1.9875e-02,\n             2.2745e-02,  1.9380e-03, -1.1687e-02, -3.6806e-02, -6.9346e-03,\n             4.3913e-02,  3.0216e-02,  4.0918e-02, -1.2515e-03,  1.0765e-02,\n             9.2893e-03, -2.8910e-02,  7.3886e-04, -2.2712e-02, -2.1124e-03,\n            -3.8250e-02, -4.0669e-03, -1.3348e-02, -1.9754e-02,  3.2496e-02,\n            -1.3390e-02, -2.6406e-02,  3.5500e-02,  2.6879e-02, -3.7972e-02,\n            -3.5547e-02,  1.3476e-02,  1.3909e-02, -2.0239e-02,  4.4060e-02,\n             1.6924e-02,  3.9457e-02,  2.0318e-03, -1.4755e-02,  1.6218e-02,\n            -3.6432e-02, -1.9676e-02,  2.4811e-03,  2.1544e-02,  3.2011e-02,\n             1.9413e-03,  2.3662e-02,  8.4416e-04, -5.8553e-03, -4.2719e-02,\n            -2.0415e-02, -3.1082e-02,  1.8803e-02,  3.1194e-02,  2.3570e-02,\n             6.8716e-03, -2.0943e-02,  1.0431e-02,  9.4175e-03,  8.4488e-03,\n            -4.1599e-02, -1.0541e-02,  2.7883e-02,  3.2225e-02,  2.9587e-02,\n            -5.3053e-03,  2.7418e-02, -2.3637e-04,  2.2571e-02, -2.8657e-03,\n             2.1979e-02, -3.8554e-02,  1.4689e-02, -3.3366e-02,  8.5076e-03,\n            -2.2934e-02,  3.4151e-02, -1.6961e-03, -2.3589e-02, -3.0298e-02,\n            -3.2971e-02, -3.3545e-02,  1.2616e-02, -1.9729e-02,  5.0031e-03,\n            -6.3496e-03, -2.3396e-02,  2.6698e-02,  4.1120e-02,  3.0997e-02,\n             2.0706e-02, -1.9945e-02,  4.2820e-02, -1.0498e-03, -1.6335e-02,\n            -3.5760e-02, -2.6236e-02,  2.4755e-02, -3.8004e-02, -9.2920e-04,\n             8.6901e-03, -1.1074e-02,  1.8680e-03, -4.0071e-02,  1.7648e-02,\n            -1.5370e-02, -1.0968e-02,  3.7210e-02,  2.1717e-02, -1.7049e-03,\n            -1.1052e-02, -2.3486e-02,  1.9283e-02,  2.7504e-02, -2.5689e-02,\n            -1.8662e-02,  1.0543e-02,  4.2308e-02, -3.6041e-02,  2.2679e-02,\n            -4.4790e-03,  5.7535e-03, -4.2122e-02,  2.3309e-02, -3.4545e-02,\n            -2.3170e-03,  3.8360e-02, -3.1565e-02,  1.5344e-02, -1.3792e-02,\n             2.4969e-02,  5.8869e-03,  1.3359e-02,  3.4247e-02,  3.7273e-02,\n            -1.5224e-02, -1.8492e-02,  1.8498e-02,  6.8026e-03, -3.0557e-02,\n            -3.2860e-02,  2.3371e-02,  4.3951e-02,  1.6527e-02, -3.1693e-02,\n            -3.9041e-02, -3.8800e-02,  1.9791e-02, -1.9812e-02,  1.8976e-02,\n            -1.7526e-02, -3.7001e-02,  3.6782e-02, -2.2668e-02, -2.1816e-02,\n            -3.7764e-02, -1.4049e-02, -2.0245e-02,  4.0379e-02,  2.2605e-02,\n             3.4348e-02,  2.6559e-02,  1.3721e-02, -1.2618e-02, -3.7508e-03,\n             2.1980e-02, -2.9989e-02, -4.0849e-02,  7.8648e-03,  3.7132e-02,\n            -3.4630e-02,  8.5812e-03, -2.3393e-02, -1.1221e-02,  3.3074e-02,\n            -3.5907e-02,  2.9664e-02, -3.7670e-02, -2.3724e-02,  1.8846e-02,\n             1.4165e-02,  2.0138e-02, -6.9223e-03,  1.9124e-02, -1.8351e-02,\n            -2.1436e-03, -2.1188e-02,  1.4442e-03,  5.8624e-03, -3.4000e-02,\n            -3.3106e-02,  7.0583e-03,  2.2031e-02,  8.2597e-03,  6.3465e-03,\n            -3.5953e-02, -3.7226e-02,  1.8144e-02,  1.4283e-03, -2.8127e-03,\n             2.5558e-02,  5.8222e-03,  1.2378e-02,  3.1728e-02,  1.6983e-02,\n            -1.1511e-02, -2.7527e-02,  3.7028e-02,  2.7166e-02, -2.9885e-02,\n             2.6335e-02, -3.1359e-02, -3.7515e-02,  2.0994e-02,  2.4387e-02,\n             9.3942e-03, -2.2619e-02,  4.3908e-02, -8.6976e-03,  3.8030e-02,\n             3.4008e-02,  1.0788e-02, -1.1364e-02,  3.2673e-02, -2.6742e-02,\n            -7.5750e-03, -2.4743e-02, -2.7584e-02, -5.6071e-03, -3.7930e-02,\n             2.0919e-03,  3.7601e-02,  2.8484e-02, -3.6405e-02, -3.5856e-02,\n             3.3822e-02, -2.8356e-03,  1.1797e-02,  1.4460e-02,  1.2815e-02,\n             1.8114e-02,  3.4112e-02,  3.4457e-02,  3.5514e-02,  1.8222e-02,\n            -9.2594e-03,  2.9844e-03,  1.4993e-02,  4.0395e-02, -3.6750e-03,\n            -1.7286e-02, -1.5558e-02,  1.1423e-02,  1.4654e-02,  3.9094e-02,\n            -1.3657e-02, -3.8395e-02,  3.9403e-02,  1.0527e-02,  1.3946e-02,\n            -3.2482e-02,  3.5442e-02,  1.8774e-02, -2.9941e-02,  1.2648e-02,\n             4.3036e-02,  3.6883e-02,  2.0428e-02, -1.4099e-02, -1.3169e-02,\n            -4.0523e-02,  2.6629e-03, -2.7031e-02,  2.2110e-02,  5.4660e-03,\n             2.5139e-02,  1.2731e-02,  4.1036e-02, -3.5602e-02,  4.3019e-02,\n             3.7402e-02, -3.0997e-02, -2.2454e-02, -2.1137e-02,  2.9775e-02,\n             2.3183e-02, -2.0120e-02,  3.4434e-02, -1.9425e-02,  2.8282e-02,\n             3.7223e-02, -7.8380e-03, -2.1503e-02, -1.6433e-03,  3.7430e-02,\n             4.2724e-02,  1.1469e-03,  3.1720e-02,  3.8345e-02, -4.3463e-02,\n             1.8668e-02, -1.9523e-02,  1.8710e-02, -2.5352e-03, -2.1458e-02,\n             2.1273e-03, -3.1931e-02, -1.5924e-02,  4.3883e-02,  3.1503e-02,\n            -4.2645e-02, -4.2372e-02, -2.6859e-02,  2.9003e-03, -3.8042e-02,\n             3.9850e-02,  1.5715e-02, -9.2691e-03, -2.3771e-02,  6.7741e-03,\n            -3.6494e-02, -3.8854e-03,  1.3265e-02, -3.3554e-02, -3.0615e-03,\n            -7.9086e-03,  1.6654e-02, -3.0614e-02, -3.0682e-02,  3.9458e-02,\n            -9.8646e-03, -4.1840e-02,  2.8502e-02,  1.4113e-02, -4.3654e-02,\n             1.2592e-02, -9.2382e-03,  3.1113e-02, -6.0717e-03,  9.6880e-03,\n            -3.2399e-02, -3.8551e-02,  3.9188e-02, -1.3057e-02, -1.9732e-02,\n            -4.6942e-03, -1.2652e-02, -2.3085e-02, -7.4521e-03,  4.3507e-02,\n             1.5740e-02,  4.1143e-02, -2.1693e-02,  3.2305e-02,  2.1853e-02,\n            -1.3555e-02,  1.3079e-02, -1.8503e-02, -7.7968e-03,  1.0451e-02,\n            -3.3060e-02,  8.2991e-03, -1.9826e-02, -2.9503e-02, -4.0642e-02,\n            -4.1785e-02,  2.8578e-02, -3.5667e-03, -2.6033e-03,  5.1315e-03,\n             3.7693e-02,  2.7769e-02, -2.1718e-02, -2.7601e-02,  4.4712e-03,\n            -4.1096e-02, -2.3090e-02, -1.6070e-02,  8.0919e-04,  1.0909e-02,\n            -1.6463e-02, -9.9463e-03,  1.7109e-02,  2.4846e-02, -2.0467e-02,\n            -3.2025e-02,  1.5403e-02, -2.2806e-02, -2.3619e-02,  7.9550e-03,\n             1.7132e-02,  2.4517e-02,  8.5273e-03,  7.5273e-03, -1.0149e-02,\n            -3.1912e-02, -4.0261e-02,  3.7607e-03, -4.0037e-02, -2.2955e-02,\n            -2.5249e-02, -3.3278e-02, -4.3435e-02,  2.6181e-02,  3.3957e-02,\n             4.3280e-02,  3.9421e-02, -3.2342e-02, -3.8205e-02,  4.3016e-02,\n            -2.5358e-02,  1.1937e-02,  1.3365e-02,  6.2741e-03,  2.7187e-03,\n             1.8379e-02, -2.4807e-02, -4.0786e-02, -3.3261e-02,  1.4854e-02,\n             4.3327e-02,  1.9616e-02,  1.7987e-02, -3.9080e-02,  1.7717e-02,\n             1.5922e-02,  2.2338e-02,  2.7991e-02, -2.7658e-02, -4.1440e-02,\n             1.1650e-03,  2.1852e-02,  6.3360e-03, -2.1139e-02, -4.2837e-02,\n            -4.2948e-02,  2.6867e-02, -4.2466e-02, -1.4020e-02, -1.6387e-02,\n             2.7525e-02, -2.7582e-02, -2.7791e-02,  8.8279e-03,  4.1535e-02,\n            -3.3549e-02, -2.2777e-03, -1.4250e-02, -3.6225e-02, -1.3965e-02,\n             2.4667e-02, -3.9074e-02, -4.2667e-02,  1.8498e-02, -1.9963e-02,\n            -8.6095e-03, -1.0216e-03, -3.3846e-02, -4.8885e-03, -3.7620e-02,\n            -3.0826e-02, -1.5987e-02, -3.9103e-02, -1.2868e-03,  4.1802e-02,\n             1.4451e-02, -4.3327e-02, -1.0998e-03, -3.1497e-04,  1.4728e-03,\n            -2.9176e-02,  2.2494e-02, -3.6503e-02, -3.2740e-02,  1.0440e-02,\n             3.1963e-02,  2.2621e-02, -2.3138e-02, -3.5982e-03, -2.3174e-02,\n             6.0174e-03,  1.6759e-02,  2.6150e-02, -1.0428e-02, -5.6445e-04,\n            -1.9584e-02,  1.5403e-02,  1.0788e-02, -3.5477e-02, -2.3808e-02,\n             5.9611e-03, -1.5524e-02,  3.4327e-02, -1.7995e-02, -1.8702e-02,\n            -2.2112e-02, -4.1638e-02, -4.2926e-02, -3.9815e-02, -1.8526e-02,\n             2.9066e-02, -6.8837e-03, -6.2073e-04, -8.5220e-03, -4.4739e-03,\n            -1.6096e-03, -4.3532e-02,  7.7528e-03,  3.5400e-02,  2.5790e-02,\n             4.7002e-03,  4.0699e-02, -1.4680e-02, -5.9465e-03,  2.4421e-02,\n             1.3869e-04,  9.1863e-03,  7.5006e-03, -1.6106e-02, -2.4460e-02,\n             1.0109e-02,  1.1212e-02,  8.4455e-04, -2.2298e-02, -1.0427e-03,\n            -2.1408e-02,  2.6756e-02, -1.6096e-02,  3.2773e-02,  1.1105e-02,\n             2.6434e-02,  3.8696e-02, -6.1676e-03,  1.6843e-02,  2.4258e-02,\n            -1.1306e-02, -4.3231e-02,  4.3923e-02,  3.6492e-02, -2.4859e-02,\n             2.4357e-02, -4.4034e-03, -2.2303e-02,  1.0517e-02,  3.8065e-02,\n            -4.2163e-02,  3.1530e-02, -1.1389e-02,  2.9319e-02,  2.9896e-02,\n             4.3169e-02,  3.4507e-03, -4.1312e-02,  1.5609e-02, -1.4632e-02,\n             4.3267e-02,  3.2513e-02, -7.1392e-03,  4.0848e-02, -1.4081e-02,\n             2.3370e-02,  4.1534e-02, -2.1286e-02,  4.2623e-02,  7.9759e-03,\n            -1.6972e-02,  4.1076e-03,  3.0129e-02, -1.8656e-03, -1.2911e-02,\n             6.1386e-03, -5.6521e-03,  4.0277e-02,  1.7416e-02, -3.4671e-02,\n            -3.2055e-02, -3.1382e-02, -3.3643e-02, -4.8112e-03, -2.0385e-02,\n             1.0149e-02, -1.8489e-02, -2.6891e-02,  1.8213e-02,  9.3293e-03],\n           requires_grad=True)],\n   'lr': 0.01,\n   'momentum': 0.9,\n   'dampening': 0,\n   'weight_decay': 0,\n   'nesterov': False,\n   'maximize': False,\n   'foreach': None,\n   'differentiable': False}],\n '_warned_capturable_if_run_uncaptured': True,\n 'lr': 1.0}"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.__dict__"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T08:34:20.553085Z",
     "start_time": "2023-08-21T08:34:20.340623Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "0.01"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.__dict__['param_groups'][0]['lr']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T08:37:12.315572Z",
     "start_time": "2023-08-21T08:37:12.299837Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.5394)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3]).to(torch.float)\n",
    "b = torch.tensor([4,5,6]).to(torch.float)\n",
    "grads = [a, b]\n",
    "\n",
    "norms = []\n",
    "norms.extend([torch.norm(g, 2.0) for g in grads])\n",
    "total_norm = torch.norm(torch.stack([norm for norm in norms]))\n",
    "print(total_norm)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T02:43:25.816318Z",
     "start_time": "2023-08-23T02:43:25.802367Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
